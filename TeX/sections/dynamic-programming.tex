\section{Dynamic programming (DP)}

Collection of algorithms that can be used to compute optimal policies.
Assumes a perfect model of the environment as a Markov decision process (MDP), but this condition is relaxed as we move on.

\subsection{Bellman optimality equations}

\begin{equation}
    \begin{aligned}
        v_*(s)
         & =
        \max_{a} \mathbb{E} \big[ R_{t+1} + \gamma v_*(S_{t+1}) \;\big|\; S_t = s, A_t = a \big]
        \\ & =
        \max_{a} \sum_{s', r} p(s', r | s, a) \Big[ r + \gamma v_*(s') \Big]
        , \qquad \forall s \in \mathcal{S}
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        q_*(s, a)
         & =
        \mathbb{E} \big[ R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') \;\big|\; S_t = s, A_t = a \big]
        \\ & =
        \sum_{s', r} p(s', r | s, a) \Big[ r + \gamma \max_{a'} q_*(s', a') \Big]
        , \qquad \forall s \in \mathcal{S}, a \in \mathcal{A}(s), s' \in \mathcal{S}^+
    \end{aligned}
\end{equation}

\subsection{Value function}

The value function \( V(s) \) of a state \( s \) is the expected return (cumulative reward) starting from state \( s \) and following a policy \( \pi \):
\begin{equation}
    V(s) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t | s_0 = s, \pi \right]
\end{equation}
where \( R_t \) is the reward received at time \( t \), and \( \gamma \) is the discount factor.

Similarly, the action-value function \( Q(s, a) \) is the expected return starting from state \( s \), taking action \( a \), and following policy \( \pi \):
\begin{equation}
    Q(s, a) = \mathbb{E} \left[ \sum_{t=0}^{\infty} \gamma^t R_t | s_0 = s, a_0 = a, \pi \right]
\end{equation}

The value function can be expressed in terms of the action-value function:
\begin{equation}
    V(s) = \sum_{a \in \mathcal{A}} \pi(a | s) Q(s, a)
\end{equation}

The action-value function can be expressed in terms of the value function:
\begin{equation}
    Q(s, a) = R(s, a) + \gamma \sum_{s' \in \mathcal{S}} P(s' | s, a) V(s')
\end{equation}

\subsection{Generalized policy iteration (GPI)}

\subsection{Value iteration}

\begin{equation}
    \begin{aligned}
        v_{k+1}(s)
         & =
        \max_{a} \mathbb{E} \big[ R_{t+1} + \gamma v_k(S_{t+1}) \;\big|\; S_t = s, A_t = a \big]
        \\ & =
        \max_{a} \sum_{s', r} p(s', r | s, a) \Big[ r + \gamma v_k(s') \Big]
        , \qquad \forall s \in \mathcal{S}
    \end{aligned}
\end{equation}

Combines one sweep of policy evaluation and one sweep of policy improvement, in each of its sweeps.

\subsection{Policy iteration}

\subsubsection{Policy evaluation}

\subsubsection{Policy improvement}

\subsection{Modified policy iteration}

\subsection{Multi-stage look ahead policy iteration}
