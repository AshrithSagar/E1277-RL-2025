\section{Function approximation}

In general, taking examples from a desired function and attempting to generalize from them to construct an approximation of the entire function.

The approximate value function \( \hat v(s; \mathbf{w}) \approx v_\pi(s) \) is represented not as a table but as a parameterized functional form with weight vector \( \mathbf{w} \in \mathbb{R}^d \), with \( d \ll \vert \mathcal{S} \vert \).

The \textit{mean square value error} \( \overline{\text{VE}} \) is defined as
\begin{equation}
    \overline{\text{VE}}(\mathbf{w}) \doteq \sum_{s \in \mathcal{S}} \mu(s) {\Big[ v_\pi(s) - \hat v(s; \mathbf{w}) \Big]}^2
\end{equation}
where \( \mu(s) \) is some state distribution, \( \mu(s) \geq 0, \ \sum_{s \in \mathcal{S}} \mu(s) = 1 \), representing how much we care about the error in each state \( s \).

\( \eta(s) \to \) Number of time steps spent on average in a state \( s \) in a single episode.
\begin{equation}
    \mu(s) = \frac{\eta(s)}{\sum_{s'} \eta(s')}
    , \quad \forall s \in \mathcal{S}
\end{equation}

\subsection{Stochastic gradient descent (SGD)}

\begin{equation}
    \begin{aligned}
        \mathbf{w}_{t+1}
         & \doteq
        \mathbf{w}_t - \frac{1}{2} \alpha \nabla {\Big[ v_\pi(S_t) - \hat v(S_t; \mathbf{w}_t) \Big]}^2
        \\ & =
        \mathbf{w}_t + \alpha \Big[ v_\pi(S_t) - \hat v(S_t; \mathbf{w}_t) \Big] \nabla \hat v(S_t; \mathbf{w}_t)
        , \quad \alpha > 0
    \end{aligned}
\end{equation}

An estimate \( U_t \) is used instead of \( v_\pi(S_t) \), giving
\begin{equation}
    \mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat v(S_t; \mathbf{w}_t) \Big] \nabla \hat v(S_t; \mathbf{w}_t)
\end{equation}
which is guarenteed to converge to the local optimum if \( U_t \) is unbiased, i.e., \( \mathbb{E}[U_t \mid S_t = s] = v_\pi(s) \), and provided that \( \alpha \) satisfies the usual stochastic approximation conditions.

The MC target \( G_t \) is an unbiased estimate of \( v_\pi(S_t) \), thereby we can use \( U_t = G_t \) in the SGD update.

Bootstrapping targets produce biased estimates, and are hence not an instance of true gradient descent, instead come under \textit{semi-gradient} methods, since they include only a part of the gradient.

Semi-gradient TD\( (0) \): Update target: \( U_t \doteq R_t + \gamma \hat v(S_{t+1}; \mathbf{w}_t) \)

\subsection{Linear function approximation}

Linear methods approximate the state-value function by the inner product between the weight vector \( \mathbf{w} \in \mathbb{R}^d \) and a real-valued feature vector \( \mathbf{x}(s) \in \mathbb{R}^d \) representing the state \( s \).
\begin{equation}
    \hat v(s; \mathbf{w}) \doteq \mathbf{w}^\top \mathbf{x}(s) \doteq \sum_{i=1}^d w_i x_i(s)
\end{equation}
This gives \( \nabla \hat v(s; \mathbf{w}) = \mathbf{x}(s) \), thereby the update rule becomes
\begin{equation}
    \mathbf{w}_{t+1} \doteq \mathbf{w}_t + \alpha \Big[ U_t - \hat v(S_t; \mathbf{w}_t) \Big] \mathbf{x}(S_t)
\end{equation}
which only has one optimum (\( \implies \) Local optimum = Global optimum).

\textit{Semi-gradient TD\( (0) \) algorithm under linear function approximation}:
\begin{equation}
    \begin{aligned}
        \mathbf{w}_{t+1}
         & \doteq
        \mathbf{w}_t + \alpha \Big[ R_{t+1} + \gamma \mathbf{w}_t^\top \mathbf{x}(S_{t+1}) - \mathbf{w}_t^\top \mathbf{x}(S_t) \Big] \mathbf{x}(S_t)
        \\ & =
        \mathbf{w}_t + \alpha \Big[ R_{t+1} \mathbf{x}(S_t) - \mathbf{x}(S_t) {\big( \mathbf{x}(S_t) - \gamma \mathbf{x}(S_{t+1}) \big)}^\top \mathbf{w}_t \Big]
    \end{aligned}
\end{equation}

Under steady-state conditions, we would have
\begin{equation}
    \begin{aligned}
        \mathbb{E} \big[ \mathbf{w}_{t+1} \mid \mathbf{w}_t \big]
         & =
        \mathbf{w}_t + \alpha \big[ \mathbf{b} - \mathbf{A} \mathbf{w}_t \big]
        =
        (\mathbf{I} - \alpha \mathbf{A}) \mathbf{w}_t + \alpha \mathbf{b}
        \\
        \text{where} \quad
        \mathbf{b}
         & \doteq
        \mathbb{E} \Big[ R_{t+1} \mathbf{x}(S_t) \Big] \in \mathbb{R}^d
        \\
        \mathbf{A}
         & \doteq
        \mathbb{E} \Big[ \mathbf{x}(S_t) {\big( \mathbf{x}(S_t) - \gamma \mathbf{x}(S_{t+1}) \big)}^\top \Big] \in \mathbb{R}^{d \times d}
    \end{aligned}
\end{equation}

The \textit{TD fixed point} is given by
\begin{equation}
    \mathbf{w}_\text{TD} = \mathbf{A}^{-1} \mathbf{b}
\end{equation}
