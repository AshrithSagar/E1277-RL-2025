\section{Finite Markov decision processes}

\subsection{Markov processes}

Markov property:
The Markov property states that the future state of a process depends only on the present state and not on the past states.
Formally, for a Markov process, the conditional probability of the future state given the present state and past states is equal to the conditional probability of the future state given only the present state.
\begin{equation}
    P(X_{t+1} \mid X_t, X_{t-1}, \ldots, X_0) = P(X_{t+1} \mid X_t)
\end{equation}

\subsection{Markov reward processes (MRP)}

A Markov reward process is a Markov process with an associated reward function.
The reward function assigns a numerical value to each state, representing the immediate reward received upon entering that state.
The expected reward at time \( t \) is given by:
\begin{equation}
    R_t = \sum_{s \in S} P(s \mid X_t) R(s)
\end{equation}
where \( R(s) \) is the reward associated with state \( s \), and \( P(s \mid X_t) \) is the probability of being in state \( s \) at time \( t \).

\subsection{Markov decision processes (MDP)}

A Markov decision process is a Markov reward process with an associated action space and transition probabilities.

The MDP framework is an abstraction of the problem of goal-directed learning from interaction.
In general, anything that cannot be controlled arbitrarily by the agent is considered to be part of its environment. Not that this refers to absolute control, not of the agent's knowledge.

The time steps need not refer to fixed intervals of real time; they can refer to arbitrary successive stages of decision making and acting.

\subsubsection{Dynamics}

The dynamics of an MDP is the function \( p: \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \to [0, 1] \) defined as
\begin{equation}
    p(s', r \mid s, a) \doteq \Pr \big[ S_{t} = s', R_{t} = r \mid S_{t-1} = s, A_{t-1} = a \big]
    , \quad \forall s, s' \in \mathcal{S}, r \in \mathcal{R}, a \in \mathcal{A}(s)
\end{equation}

Every choice of \( (s, a) \) specifies a probability distribution.
\begin{equation}
    \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \mid s, a) = 1
    , \quad \forall s \in \mathcal{S}, a \in \mathcal{A}(s)
\end{equation}

These completely characterise the environment's dynamics, for MDPs.

\subsubsection{State-transition probabilities}

The state-transition probabilities of an MDP is the function \( p: \mathcal{S} \times \mathcal{S} \times \mathcal{A} \to [0, 1] \) defined as
\begin{equation}
    p(s' \mid s, a) \doteq \Pr \big[ S_{t} = s' \mid S_{t-1} = s, A_{t-1} = a \big]
    , \quad \forall s, s' \in \mathcal{S}, a \in \mathcal{A}(s)
\end{equation}

The state-transition probabilities can be computed given the dynamics of an MDP, as
\begin{equation}
    p(s' \mid s, a) = \sum_{r \in \mathcal{R}} p(s', r \mid s, a)
    , \quad \forall s, s' \in \mathcal{S}, a \in \mathcal{A}(s)
\end{equation}

\subsubsection{Expected rewards}

\begin{equation}
    r(s, a) \doteq \mathbb{E} \big[ R_{t} \mid S_{t-1} = s, A_{t-1} = a \big]
    = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}}  p(s', r \mid s, a)
\end{equation}
\begin{equation}
    r(s, a, s') \doteq \mathbb{E} \big[ R_{t} \mid S_{t-1} = s, A_{t-1} = a, S_{t} = s' \big]
    = \sum_{r \in \mathcal{R}} r \frac{p(s', r \mid s, a)}{p(s' \mid s, a)}
\end{equation}
