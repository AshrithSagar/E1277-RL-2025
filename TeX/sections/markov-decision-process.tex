\section{Finite Markov decision processes}

\subsection{Markov processes}

Markov property:
The Markov property states that the future state of a process depends only on the present state and not on the past states.
Formally, for a Markov process, the conditional probability of the future state given the present state and past states is equal to the conditional probability of the future state given only the present state.
\begin{equation}
    P(X_{t+1} \mid X_t, X_{t-1}, \ldots, X_0) = P(X_{t+1} \mid X_t)
\end{equation}

\subsection{Markov reward processes (MRP)}

A Markov reward process is a Markov process with an associated reward function.
The reward function assigns a numerical value to each state, representing the immediate reward received upon entering that state.
The expected reward at time \( t \) is given by:
\begin{equation}
    R_t = \sum_{s \in S} P(s \mid X_t) R(s)
\end{equation}
where \( R(s) \) is the reward associated with state \( s \), and \( P(s \mid X_t) \) is the probability of being in state \( s \) at time \( t \).

\subsection{Markov decision processes (MDP)}

A Markov decision process is a Markov reward process with an associated action space and transition probabilities.

\subsection{The MDP framework}

The MDP framework~\cite{Sutton1998} is an abstraction of the problem of goal-directed learning from interaction.
In general, anything that cannot be controlled arbitrarily by the agent is considered to be part of its environment. Not that this refers to absolute control, not of the agent's knowledge.

The time steps need not refer to fixed intervals of real time; they can refer to arbitrary successive stages of decision making and acting.

\subsubsection{Dynamics}\label{sec:mdp-dynamics}

The \textit{dynamics} of an MDP is the function \( p: \mathcal{S} \times \mathcal{R} \times \mathcal{S} \times \mathcal{A} \to [0, 1] \) defined as
\begin{equation}
    p(s', r \mid s, a) \doteq \Pr \big[ S_{t} = s', R_{t} = r \mid S_{t-1} = s, A_{t-1} = a \big]
    , \quad \forall s, s' \in \mathcal{S}, r \in \mathcal{R}, a \in \mathcal{A}(s)
\end{equation}

Every choice of \( (s, a) \) specifies a probability distribution.
\begin{equation}
    \sum_{s' \in \mathcal{S}} \sum_{r \in \mathcal{R}} p(s', r \mid s, a) = 1
    , \quad \forall s \in \mathcal{S}, a \in \mathcal{A}(s)
\end{equation}

These completely characterise the environment's dynamics, for MDPs.

\subsubsection{State-transition probabilities}

The \textit{state-transition probabilities} of an MDP is the function \( p: \mathcal{S} \times \mathcal{S} \times \mathcal{A} \to [0, 1] \) defined as
\begin{equation}
    p(s' \mid s, a) \doteq \Pr \big[ S_{t} = s' \mid S_{t-1} = s, A_{t-1} = a \big]
    , \quad \forall s, s' \in \mathcal{S}, a \in \mathcal{A}(s)
\end{equation}

The state-transition probabilities can be computed given the dynamics of an MDP, as
\begin{equation}
    p(s' \mid s, a) = \sum_{r \in \mathcal{R}} p(s', r \mid s, a)
    , \quad \forall s, s' \in \mathcal{S}, a \in \mathcal{A}(s)
\end{equation}

\subsubsection{Expected rewards}

\begin{equation}
    r(s, a) \doteq \mathbb{E} \big[ R_{t} \mid S_{t-1} = s, A_{t-1} = a \big]
    = \sum_{r \in \mathcal{R}} r \sum_{s' \in \mathcal{S}}  p(s', r \mid s, a)
\end{equation}
\begin{equation}
    r(s, a, s') \doteq \mathbb{E} \big[ R_{t} \mid S_{t-1} = s, A_{t-1} = a, S_{t} = s' \big]
    = \sum_{r \in \mathcal{R}} r \frac{p(s', r \mid s, a)}{p(s' \mid s, a)}
\end{equation}

\subsubsection{Return}

The \textit{return} \( G_t \), in general, is some specific function of the reward sequence \( R_{t+1}, R_{t+2}, \ldots, R_{T} \), which are the rewards received after time step \( t \) till some \( T \).
The \textit{discounted return} is defined with a \textit{discount rate/factor} \( \gamma \in [0, 1] \) as
\begin{equation}
    G_t \doteq \sum_{k=t+1}^{T} \gamma^{k-t-1} R_k
\end{equation}
where atmost one of \( T = \infty \) or \( \gamma = 1 \) is allowed but not both.

The return \( G_t \) follows a recursive relation, as
\begin{equation}
    G_t = R_{t+1} + \gamma G_{t+1}
\end{equation}

\subsubsection{Episodes}

An \textit{episode} is a sequence of time steps that ends in a terminal state.

\subsubsection{Policy}

A \textit{policy} \( \pi: \mathcal{S} \to \mathcal{A} \) is a mapping from states to probabilities of selecting each possible action.
\begin{equation}
    \pi(a \mid s) \doteq \Pr \big[ A_t = a \mid S_t = s \big]
    , \quad \forall s \in \mathcal{S}, a \in \mathcal{A}(s)
\end{equation}
\begin{equation}
    \sum_{a \in \mathcal{A}(s)} \pi(a \mid s) = 1
    , \quad \forall s \in \mathcal{S}
\end{equation}

\textbf{Exercise 3.11~\cite{Sutton1998}}:
If the current state is \( S_t \), and actions are selected according to a stochastic policy \( \pi \), then what is the expectation of \( R_{t+1} \) in terms of \( \pi \) and the four-argument
function \( p \)?

\textbf{Solution}:
\begin{align*}
    \mathbb{E}_{\pi} \big[ R_{t+1} \mid S_t = s \big]
     & =
    \sum_{r \in \mathcal{R}} r
    \Pr \big[ R_{t+1} = r \mid S_t = s \big]
    =
    \sum_{r \in \mathcal{R}} r
    \sum_{s' \in \mathcal{S}}
    \Pr \big[ S_{t+1} = s', R_{t+1} = r \mid S_t = s \big]
    \\ & =
    \sum_{r \in \mathcal{R}} r
    \sum_{s' \in \mathcal{S}}
    \sum_{a \in \mathcal{A}(s)}
    \underbrace{ \Pr \big[ S_{t+1} = s', R_{t+1} = r \mid S_t = s, A_t = a \big] }_{p(s', r \mid s, a)}
    \underbrace{ \Pr \big[ A_t = a \mid S_t = s \big] }_{\pi(a \mid s)}
    \\ & =
    \sum_{r \in \mathcal{R}} r
    \sum_{s' \in \mathcal{S}}
    \sum_{a \in \mathcal{A}(s)}
    \pi(a \mid s) \; p(s', r \mid s, a)
    =
    \sum_{a} \pi(a \mid s)
    \sum_{s', r} r \; p(s', r \mid s, a)
\end{align*}

\subsubsection{Value function}

The \textit{state-value function} \( v_\pi(s) \) of a state \( s \) under a policy \( \pi \) is the expected return when starting from state \( s \) and following policy \( \pi \) thereafter,
\begin{equation}
    v_\pi(s) \doteq \mathbb{E}_{\pi} \big[ G_t \mid S_t = s \big]
\end{equation}

The \textit{action-value function} \( q_\pi(s, a) \) of taking an action \( a \) in state \( s \) under a policy \( \pi \) is the expected return starting from state \( s \), taking action \( a \), and following policy \( \pi \) thereafter,
\begin{equation}
    q_\pi(s, a) \doteq \mathbb{E}_{\pi} \big[ G_t \mid S_t = s, A_t = a \big]
\end{equation}

\begin{equation}
    v_\pi(s) = \sum_{a \in \mathcal{A}(s)} \pi(a \mid s) q_\pi(s, a)
    , \quad \forall s \in \mathcal{S}
\end{equation}
\begin{equation}
    \begin{aligned}
        q_\pi(s, a)
         & =
        \mathbb{E} \big[ R_{t+1} + \gamma v_\pi(S_{t+1}) \;\big|\; S_t = s, A_t = a \big]
        \\ & =
        r(s, a) + \sum_{s' \in \mathcal{S}} p(s' \mid s, a) v_\pi(s')
        , \quad \forall s \in \mathcal{S}, a \in \mathcal{A}(s)
    \end{aligned}
\end{equation}

\subsubsection{Bellman equations}

A recursive relation between the value functions.
\begin{equation}
    v_\pi(s) = \sum_{a} \pi(a \mid s) \sum_{s', r} p(s', r \mid s, a) \Big[ r + \gamma v_\pi(s') \Big]
    , \quad \forall s \in \mathcal{S}
\end{equation}
The value function \( v_\pi \) is the unique solution to its Bellman equation.

\textbf{Exercise 3.15~\cite{Sutton1998}}:
Adding a constant \( c \) to all the rewards adds a constant \( v_c \) to the values of all states and does not affect the relative values of any states under any policies.

\subsubsection{Optimal value functions}

Value functions define a partial ordering over policies.
\( \pi \geq \pi' \iff v_\pi(s) \geq v_{\pi'}(s), \ \forall s \in \mathcal{S} \).

\textit{Optimal state-value function} \( v_*(s) \)
\begin{equation}
    v_*(s) \doteq \max_{\pi} v_\pi(s)
    , \quad \forall s \in \mathcal{S}
\end{equation}

\textit{Optimal action-value function} \( q_*(s, a) \)
\begin{equation}
    q_*(s, a) \doteq \max_{\pi} q_\pi(s, a)
    , \quad \forall s \in \mathcal{S}, a \in \mathcal{A}(s)
\end{equation}

\begin{equation}
    q_*(s, a) = \mathbb{E} \big[ R_{t+1} + \gamma v_*(S_{t+1}) \;\big|\; S_t = s, A_t = a \big]
\end{equation}

\subsubsection{Bellman optimality equations}\label{sec:bellman-optimality-equations}

A self-consistency relation applying the Bellman equations to the optimal value functions, i.e., the value of a state under an optimal policy must equal the expected return for the best action from that state.

\begin{equation}
    \begin{aligned}
        v_*(s)
         & =
        \max_{a \in \mathcal{A}(s)} q_{\pi_*}(s, a)
        \\ & =
        \max_{a} \mathbb{E}_{\pi_*} \big[ R_{t+1} + \gamma v_*(S_{t+1}) \;\big|\; S_t = s, A_t = a \big]
        \\ & =
        \max_{a} \sum_{s', r} p(s', r | s, a) \Big[ r + \gamma v_*(s') \Big]
        , \qquad \forall s \in \mathcal{S}
    \end{aligned}
\end{equation}
\begin{equation}
    \begin{aligned}
        q_*(s, a)
         & =
        \mathbb{E} \big[ R_{t+1} + \gamma \max_{a'} q_*(S_{t+1}, a') \;\big|\; S_t = s, A_t = a \big]
        \\ & =
        \sum_{s', r} p(s', r | s, a) \Big[ r + \gamma \max_{a'} q_*(s', a') \Big]
        , \qquad \forall s \in \mathcal{S}, a \in \mathcal{A}(s)
    \end{aligned}
\end{equation}

\subsubsection{Optimal policies}

Any policy that assigns nonzero probability only to the actions at which the maximum is obtained in the Bellman optimality equations is an \textit{optimal policy}.

Any policy that is greedy with respect to the
optimal evaluation function \( v_* \) is an optimal policy.

\subsection{Policies}\label{sec:policies}

A \textit{stationary policy} is a policy that does not change over time.

The class of optimal policies is a subset of the class of stationary policies.

A \textit{deterministic policy} is a policy that selects a single action in a state.
\begin{equation}
    \pi(s) = a, \quad \forall s \in \mathcal{S}, a \in \mathcal{A}(s)
\end{equation}

In other words, the probability distribution can be viewed as being one-hot encoded.

A \textit{random policy} is a policy that selects a random action in a state.

A \textit{greedy policy} is a policy that selects the action with the highest value in a state.
\begin{equation}
    \pi(s) = \arg\max_a Q(s, a)
\end{equation}

An \textit{epsilon-greedy policy} is a policy that selects the action with the highest value in a state with probability \( (1 - \varepsilon) \) and selects a random action with probability \( \varepsilon \).
\begin{equation}
    \pi(s) = \begin{cases}
        \arg\max_a Q(s, a)
         &
        \text{with probability } (1 - \varepsilon)
        \\
        \text{random action}
         &
        \text{with probability } \varepsilon
    \end{cases}
    , \quad \varepsilon \in [0, 1]
\end{equation}
\( \varepsilon = 0 \) is a greedy policy and \( \varepsilon = 1 \) is a uniform random policy.

A \textit{soft policy} is a policy that has a non-zero probability of selecting all actions in a state.
\begin{equation}
    \pi(s, a) > 0, \quad \forall a \in \mathcal{A}(s), \forall s \in \mathcal{S}
\end{equation}
Gradually shifted closer to a deterministic policy while learning.

An \textit{epsilon-soft policy} is combination of a soft policy and an epsilon-greedy policy.
\begin{equation}
    \pi(a \mid s) \geq \frac{\varepsilon}{|\mathcal{A}(s)|}, \quad \forall a \in \mathcal{A}(s), \forall s \in \mathcal{S}, \varepsilon > 0
\end{equation}

\subsection{Terminology}

\subsubsection{Incremental learning}

An update rule of the form
\begin{equation}
    \text{NewEstimate} \leftarrow \text{OldEstimate} + \text{StepSize} \underbrace{ \Big[ \text{Target} - \text{OldEstimate} \Big] }_{\text{Error in the estimate}}
\end{equation}

\textbf{Notation}: Update target: \( s \mapsto u \)

Denotes an individual state updation, where \( s \) is the state updated and \( u \) is the update target that \( s \)'s estimated value is shifted towards.
\( s \mapsto \dots \) denotes that an arbitrary state is being updated, whereas \( S_t \mapsto \dots \) denotes that the state encountered in actual experience, \( S_t \), is being updated.

\subsubsection{Bootstrapping}\label{sec:bootstrapping}

The process of updating the estimate values based in part on other learned estimates, without waiting for a final outcome.

\subsubsection{Planning}

Refers to any computational process that takes a model as input and produces or improves a policy for interacting with the modeled environment.

\textbf{Types}: State-space planning, Plan-space planning.

\subsubsection{Model}

A model refers to an agent's internal representation or simulation of the environment's dynamics, i.e., \( p(s', r \mid s, a) \).
Note that this includes both the state-transition probabilities and the reward distribution.

\textbf{Types}: Distribution models, Sample models.

The estimates of the MDP's dynamics \psecref{sec:mdp-dynamics} is a distribution model.

\subsubsection{Model-based learning}

Learning from a model of the environment’s dynamics, i.e., need \( p(s', r \mid s, a) \).

Rely on \textit{planning} as their primary component.

\subsubsection{Model-free learning}\label{sec:model-free-learning}

Learning directly from raw experience without a model of the environment’s dynamics.

Rely on \textit{learning} as their primary component.

\subsubsection{Off-policy learning}

\textit{Off-policy} methods generate episodes using a different policy \( b \) (\textit{behavior policy}) and uses these episodes to improve the current policy \( \pi \) (\textit{target policy}).
Are often of greater variance and are slower to converge.

\textit{Coverage assumption}:
\begin{equation}
    \pi(a \mid s) > 0 \implies b(a \mid s) > 0, \quad \forall a \in \mathcal{A}(s), \forall s \in \mathcal{S}
\end{equation}

\subsubsection{On-policy learning}

\textit{On-policy} methods estimates the value of the current policy \( \pi \) and improves it by using the same policy to generate the episodes, i.e., the behavior policy is the same as the target policy.

\subsubsection{Model learning}

Planning agent is used to improve the model, to make it more accurately match the real environment.

\subsubsection{Direct reinforcement learning (Direct RL)}

Planning agent is used to directly improve the value function and policy.
