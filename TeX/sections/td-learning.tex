\section{Temporal difference learning (TD)}

TD learning is a combination of MC and DP ideas; are model-free \psecref{sec:model-free-learning} and they bootstrap \psecref{sec:bootstrapping}, i.e., TD methods combine the sampling of MC with the bootstrapping of DP.\@

\subsection{TD\texorpdfstring{\( (0) \)}{ (0) } algorithm}

a.k.a.~\textit{one-step TD}, the target here is \( R_{t+1} + \gamma V(S_{t+1}) \), making it a bootstrapping method \psecref{sec:bootstrapping}.
\begin{equation}
    V(S_t) \leftarrow V(S_t) + \alpha \Big[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \Big]
\end{equation}
Compare this with the MC update rule \peqref{eq:mc-update-rule-2}.

The \textit{TD error} here is given by
\begin{equation}\label{eq:td-error-TD0}
    \delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t)
\end{equation}

Note that if \( V \) does not change during an epsiode (not the case in general TD\( (0) \)), then the MC errors \peqref{eq:mc-error} can be written as a sum of TD errors \peqref{eq:td-error-TD0}.
\begin{align*}
    \underbrace{ G_t - V(S_t) }_{\delta_{t}^{\text{MC}}}
     & =
    R_t + \gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1})
    =
    \delta_{t}^{\text{TD}} + \gamma \underbrace{ (G_{t+1} - V(S_{t+1})) }_{\delta_{t+1}^{\text{MC}}}
    \\
    \implies
    \delta_{t}^{\text{MC}}
     & =
    \delta_{t}^{\text{TD}} + \gamma \delta_{t+1}^{\text{MC}}
    \implies
    \delta_{t}^{\text{MC}}
    =
    \sum_{k = t}^{T-1} \gamma^{k-t} \delta_{k}^{\text{TD}}
\end{align*}

\textbf{Exercise 6.1~\cite{Sutton1998}}:
\( \sim \) If \( V \) changes during the epsiode, then what changes need to be made above?

\textbf{Solution}:
Index \( V \) with time, i.e.\ let \( V_t \) denote the estimate of the state values \( V \) at time \( t \).
Then
\begin{align*}
    \underbrace{ G_t - V_t(S_t) }_{\delta_{t}^{\text{MC}}}
     & =
    R_t + \gamma G_{t+1} - V_t(S_t) + \gamma V_t(S_{t+1}) - \gamma V_t(S_{t+1})
    =
    \delta_{t}^{\text{TD}} + \gamma (G_{t+1} - V_t(S_{t+1}))
    \\ & =
    \delta_{t}^{\text{TD}} + \gamma \underbrace{ (G_{t+1} - V_{t+1}(S_{t+1})) }_{\delta_{t+1}^{\text{MC}}} + \gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1}))
    \\
    \implies
    \delta_{t}^{\text{MC}}
     & =
    \delta_{t}^{\text{TD}} + \gamma \delta_{t+1}^{\text{MC}} + \gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1}))
    \\
    \implies
    \delta_{t}^{\text{MC}}
     & =
    \sum_{k = t}^{T-1} \gamma^{k-t} \delta_{k}^{\text{TD}} + \boxed{ \sum_{k = t+1}^{T} \gamma^{k-t} \big[ V_{k}(S_{k}) - V_{k-1}(S_{k}) \big] }
\end{align*}

\subsection{Batch updating comparisons}

\subsection{SARSA algorithm}

An on-policy TD control algorithm.
a.k.a.~\textit{one-step SARSA}, or \textit{SARSA\( (0) \)}.

Learns an action-value function rather than a state-value function, due to the on-policy nature.

Update rule:
\begin{equation}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \Big]
\end{equation}
Uses every element of the quintuple of events \( (S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}) \) to update the action-value function from one state-action pair to the next \( \implies \) `SARSA'.

\subsection{\texorpdfstring{\( Q \)-learning}{Q-learning}}

An off-policy TD control algorithm.

\begin{equation}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \Big]
\end{equation}
The learned action-value function \( Q \) directly approximates the optimal action-value function \( q^* \) independent of the policy being followed.

In a way, this can be seen as a SARS algorithm, since it involves the tuple \( (S_t, A_t, R_{t+1}, S_{t+1}) \).

\subsection{Expected SARSA algorithm}

A generalisation of the \( Q \)-learning algorithm, and an improvement over SARSA.\@

Uses the expected value instead of the maximum value in the target.

\begin{equation}
    \begin{aligned}
        Q(S_t, A_t)
         & \leftarrow
        Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma \mathbb{E}_{\pi} \big[ Q(S_{t+1}, A_{t+1}) \;\big|\; S_{t+1} \big] - Q(S_t, A_t) \Big]
        \\ & =
        Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma \sum_{a} \pi(a \mid S_{t+1}) \; Q(S_{t+1}, a) - Q(S_t, A_t) \Big]
    \end{aligned}
\end{equation}

Given the next state \( S_{t+1} \), this algorithm moves deterministically in the same direction as SARSA, but under an expectation \( \implies \) `Expected SARSA'.

\subsection{\texorpdfstring{\( n \)}{ (n) }-step bootstrapping}

The \textit{\( n \)-step return} follows the recursive relation:
\begin{equation}
    G_{t:h} = R_{t+1} + \gamma G_{t+1:h}
    \quad \forall t < h < T
    , \qquad
    G_{h:h} \doteq V_{h-1}(S_h)
\end{equation}

\subsubsection{\texorpdfstring{\( n \)}{ (n) }-step TD prediction}

The \( n \)-step return in terms of estimated state values:
\begin{equation}
    G_{t:t+n}
    \doteq
    \begin{cases}
        R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n V_{t+n-1}(S_{t+n}),
         &
        0 \leq t < T - n
        \\
        G_t,
         &
        t \geq T - n
    \end{cases}
    , \quad \forall n \geq 1
\end{equation}
where the value function \( V_{t+n-1} \) is used to correct for the missing rewards beyond \( R_{t+n} \).

The \( n \)-step TD methods form a family of methods, with one-step TD methods (\( n = 1 \)) and MC methods (\( n = \infty \)) on the extremes.

\textit{\( n \)-step TD algorithm}:
\begin{equation}
    V_{t+n}(S_t)
    \doteq
    \begin{cases}
        V_{t+n-1}(s) + \alpha \Big[ G_{t:t+n} - V_{t+n-1}(s) \Big],
         &
        S_t = s
        \\
        V_{t+n-1}(s),
         &
        S_t \neq s
    \end{cases}
    , \quad 0 \leq t < T
\end{equation}

No estimate updates are made at all during the first \( n-1 \) steps of each episode, so an equal number of additional updates are made after the termination of each episode before the next episode begins.

\textit{Error-reduction property of \( n \)-step returns}:
\begin{equation}
    \max_{s} \Big| \mathbb{E}_{\pi} \big[ G_{t:t+n} \;\big|\; S_t = s \big] - v_\pi(s) \Big|
    \leq
    \gamma^n \max_{s} \Big| V_{t+n-1}(s) - v_\pi(s) \Big|
    , \quad \forall n \geq 1
\end{equation}

\subsubsection{\texorpdfstring{\( n \)}{ (n) }-step SARSA}

The \( n \)-step return in terms of estimated action values:
\begin{equation}
    G_{t:t+n}
    \doteq
    \begin{cases}
        R_{t+1} + \gamma R_{t+2} + \cdots + \gamma^{n-1} R_{t+n} + \gamma^n Q_{t+n-1}(S_{t+n}, A_{t+n}),
         &
        0 \leq t < T - n
        \\
        G_t,
         &
        t \geq T - n
    \end{cases}
    , \quad \forall n \geq 1
\end{equation}

\textit{\( n \)-step SARSA algorithm}:
\begin{equation}
    Q_{t+n}(S_t, A_t)
    \doteq
    \begin{cases}
        Q_{t+n-1}(s, a) + \alpha \Big[ G_{t:t+n} - Q_{t+n-1}(s, a) \Big],
         &
        S_t = s, A_t = a
        \\
        Q_{t+n-1}(s, a)
         &
        S_t \neq s, A_t \neq a
    \end{cases}
    , \quad 0 \leq t < T
\end{equation}

The \( n \)-step return of SARSA can be written exactly in terms of a novel TD error as
\begin{equation}
    G_{t:t+n} = Q_{t-1}(S_t, A_t) + \sum_{k = t}^{\min(t+n, T)-1} \gamma^{k-t} \big[ R_{k+1} + \gamma Q_{k}(S_{k+1}, A_{k+1}) - Q_{k-1}(S_k, A_k) \big]
\end{equation}

\subsection{TD\texorpdfstring{\( (\lambda) \)}{ (lambda) } algorithm}
