\section{Temporal difference learning (TD)}

TD learning is a combination of MC and DP ideas; are model-free \psecref{sec:model-free-learning} and they bootstrap \psecref{sec:bootstrapping}, i.e., TD methods combine the sampling of MC with the bootstrapping of DP.

\subsection{TD\texorpdfstring{\( (0) \)}{ (0) } algorithm}

AKA \textit{one-step TD}, the target here is \( R_{t+1} + \gamma V(S_{t+1}) \), making it a bootstrapping method \psecref{sec:bootstrapping}.
\begin{equation}
    V(S_t) \leftarrow V(S_t) + \alpha \Big[ R_{t+1} + \gamma V(S_{t+1}) - V(S_t) \Big]
\end{equation}
Compare this with the MC update rule \peqref{eq:mc-update-rule-2}.

The \textit{TD error} here is given by
\begin{equation}\label{eq:td-error-TD0}
    \delta_t \doteq R_{t+1} + \gamma V(S_{t+1}) - V(S_t)
\end{equation}

Note that if \( V \) does not change during an epsiode (not the case in general TD\( (0) \)), then the MC errors \peqref{eq:mc-error} can be written as a sum of TD errors \peqref{eq:td-error-TD0}.
\begin{align*}
    \underbrace{ G_t - V(S_t) }_{\delta_{t}^{\text{MC}}}
     & =
    R_t + \gamma G_{t+1} - V(S_t) + \gamma V(S_{t+1}) - \gamma V(S_{t+1})
    =
    \delta_{t}^{\text{TD}} + \gamma \underbrace{ (G_{t+1} - V(S_{t+1})) }_{\delta_{t+1}^{\text{MC}}}
    \\
    \implies
    \delta_{t}^{\text{MC}}
     & =
    \delta_{t}^{\text{TD}} + \gamma \delta_{t+1}^{\text{MC}}
    \implies
    \delta_{t}^{\text{MC}}
    =
    \sum_{k = t}^{T-1} \gamma^{k-t} \delta_{k}^{\text{TD}}
\end{align*}

\textbf{Exercise 6.1~\cite{Sutton1998}}:
\( \sim \) If \( V \) changes during the epsiode, then what changes need to be made above?

\textbf{Solution}:
Index \( V \) with time, i.e.\ let \( V_t \) denote the estimate of the state values \( V \) at time \( t \).
Then
\begin{align*}
    \underbrace{ G_t - V_t(S_t) }_{\delta_{t}^{\text{MC}}}
     & =
    R_t + \gamma G_{t+1} - V_t(S_t) + \gamma V_t(S_{t+1}) - \gamma V_t(S_{t+1})
    =
    \delta_{t}^{\text{TD}} + \gamma (G_{t+1} - V_t(S_{t+1}))
    \\ & =
    \delta_{t}^{\text{TD}} + \gamma \underbrace{ (G_{t+1} - V_{t+1}(S_{t+1})) }_{\delta_{t+1}^{\text{MC}}} + \gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1}))
    \\
    \implies
    \delta_{t}^{\text{MC}}
     & =
    \delta_{t}^{\text{TD}} + \gamma \delta_{t+1}^{\text{MC}} + \gamma (V_{t+1}(S_{t+1}) - V_t(S_{t+1}))
    \\
    \implies
    \delta_{t}^{\text{MC}}
     & =
    \sum_{k = t}^{T-1} \gamma^{k-t} \delta_{k}^{\text{TD}} + \boxed{ \sum_{k = t+1}^{T} \gamma^{k-t} \big[ V_{k}(S_{k}) - V_{k-1}(S_{k}) \big] }
\end{align*}

\subsection{Batch updating comparisons}

\subsection{SARSA algorithm}

An on-policy TD control algorithm.

Learns an action-value function rather than a state-value function, due to the on-policy nature.

Update rule:
\begin{equation}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t) \Big]
\end{equation}
Uses every element of the quintuple of events \( (S_t, A_t, R_{t+1}, S_{t+1}, A_{t+1}) \) to update the action-value function from one state-action pair to the next \( \implies \) `SARSA'.

\subsection{\texorpdfstring{\( Q \)-learning}{Q-learning}}

An off-policy TD control algorithm.

\begin{equation}
    Q(S_t, A_t) \leftarrow Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma \max_{a} Q(S_{t+1}, a) - Q(S_t, A_t) \Big]
\end{equation}
The learned action-value function \( Q \) directly approximates the optimal action-value function \( q^* \) independent of the policy being followed.

\subsection{Expected SARSA algorithm}

A generalisation of the \( Q \)-learning algorithm, and an improvement over SARSA.\@

Uses the expected value instead of the maximum value in the target.

\begin{equation}
    \begin{aligned}
        Q(S_t, A_t)
         & \leftarrow
        Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma \mathbb{E}_{\pi} \big[ Q(S_{t+1}, A_{t+1}) \;\big|\; S_{t+1} \big] - Q(S_t, A_t) \Big]
        \\ & =
        Q(S_t, A_t) + \alpha \Big[ R_{t+1} + \gamma \sum_{a} \pi(a \mid S_{t+1}) \; Q(S_{t+1}, a) - Q(S_t, A_t) \Big]
    \end{aligned}
\end{equation}

Given the next state \( S_{t+1} \), this algorithm moves deterministically in the same direction as SARSA, but under an expectation \( \implies \) `Expected SARSA'.

\subsection{TD\texorpdfstring{\( (\lambda) \)}{ (lambda) } algorithm}
