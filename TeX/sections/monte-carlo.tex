\section{Monte Carlo methods (MC)}

MC methods do not bootstrap \psecref{sec:bootstrapping} and the estimates for each state are independent.
As a result, these methods are also useful when one needs to only estimate the values for a subset of the state space.

Are model-free \psecref{sec:model-free-learning}, and can be either \textit{on-policy} and \textit{off-policy} \psecref{sec:policies}.

\subsection{MC prediction}

Each occurrence of state \( s \) in an episode constitutes a \textit{visit} to \( s \).

\subsubsection{First-visit MC method}

The \textit{first-visit} of a state \( s \) in an episode is the first time that \( s \) occurs in that episode.
The \textit{first-visit method} estimates \( v_\pi(s) \) by averaging the returns \( G_t \) of the first visits to \( s \) in an episode.

The estimated \( V(s) \) converges to the true value \( v_\pi(s) \) as the number of visits to \( s \) goes to infinity.
Each return is an independent and identically distributed (i.i.d.) estimate of \( v_\pi(s) \) with finite variance, and by the law of large numbers, the average converges to the expected value.
Note that the average here is an unbiased estimate of \( v_\pi(s) \), and the standard deviation of the average is \( \sigma / \sqrt{N} \), where \( N \) is the number of visits to \( s \) and \( \sigma^2 \) is the variance of the returns, \( \therefore V(s) \to v_\pi(s) \text{ and } \operatorname{Var}[V(s)] \to 0 \text{ as } N \to \infty \).

\subsubsection{Every-visit MC method}

The \textit{every-visit method} estimates \( v_\pi(s) \) by averaging the returns \( G_t \) of all visits to \( s \) in an episode.

The estimates converge quadratically to the true value \( v_\pi(s) \) as the number of visits to \( s \) goes to infinity.

\subsubsection{Constant-\texorpdfstring{\( \alpha \)}{alpha} MC}

Update target: \( S_t \mapsto G_t \)
\begin{equation}
    V(s) \leftarrow V(s) + \alpha \Big[ G_t - V(s) \Big]
\end{equation}
where \( \alpha \) is a constant step-size parameter.

The target here is the return \( G_t \), which is only known after episode termination, hence MC methods must wait until the episode terminates before updating value estimates.

Written in an other way, we have
\begin{equation}\label{eq:mc-update-rule-2}
    V(s) \leftarrow V(s) + \alpha \Big[ R_{t+1} + \gamma G_{t+1} - V(s) \Big]
\end{equation}

The MC error is given by
\begin{equation}\label{eq:mc-error}
    \delta_t \doteq G_t - V(S_t)
\end{equation}

\subsection{MC control}

Maintaining sufficient exploration is an issue in MC control methods.

Using \( \varepsilon \)-soft policies allows to drop the exploration starts requirement.
