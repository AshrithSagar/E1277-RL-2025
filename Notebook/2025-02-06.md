# RL | 2025-02-06

## Value iteration

Start from some $J$ and iterate to obtain the sequence of function $J_0, \ T J_0, \ T^2 J_0, \dots,$ and

$$
\lim_{n \to \infty} T^n J_0 = J^*
\tag{optimal value function}
$$

### Gauss-Seidel value iteration

Define an operator $F: \mathbb{R}^{\vert S \vert} \to \mathbb{R}^{\vert S \vert}$

$$
(F J)(1) = \min_{u \in A(1)} \sum_{j = 1}^{n} p_{1j}(u) \Big( g(1, u, j) + J(j) \Big)
$$

For $i = 2, 3, \dots, n$

$$
(F J)(i) = \min_{u \in A(i)} \left( \sum_{j = 1}^{n} p_{ij}(u) \ g(i, u, j) + \sum_{j = 1}^{i-1} p_{ij}(u) \ (F J)(j) + \sum_{j = i}^{n} p_{ij}(u) \ J(j) \right)
$$

**Note:** $(F J)(1) = (T J)(1)$

**Result:**

$$
\lim_{k \to \infty} F^k J = J^*, \quad \forall \ J \in \mathbb{R}^n
$$

- Implementation will be sequential, due to the dependency on other states
- Use the most recently updated values, of $(F J)(i)$

- Above is a sequence/system of equations, can solve directly if permissible
- Still falls under the rubric of finite state equations, although in practice, need to factor complexity for large $n$

## Policy iteration

1. Start with an initial policy $\mu_0$

2. **Policy evaluation:** Given a policy $\mu_k$, we compute $J^{\mu_k}(i), \ i \in S$ as the solution to

   $$
   J(i) = \sum_{j = 1}^{n} p_{ij}(\mu(i)) \Big( g(i, \mu(i), j) + J(j) \Big), \qquad i = 1, 2, \dots, n
   $$

   in unknowns $J(1), \ J(2), \dots, \ J(n)$

3. **Policy improvement:** Find a new policy $\mu_{k+1}$ s.t.

   $$
   \mu_{k+1}(i) = \arg \min_{u \in A(i)} \sum_{j = 1}^{n} p_{ij}(u) \Big( g(i, u, j) + J^{\mu_{k}}(j) \Big), \qquad \forall \ i \in 1, 2, \dots, n
   $$

   or solve

   $$
   \begin{aligned}
   T_{\mu_{k+1}} J^{\mu_k}
   & =
   T J^{\mu_k}
   \\
   \implies
   \sum_{j = 1}^{n} p_{ij}(\mu_{k+1}(i)) \Big( g(i, \mu_{k+1}(i), j) + J^{\mu_{k}}(j) \Big)
   & =
   \min_{u \in A(i)} \sum_{j = 1}^{n} p_{ij}(u) \Big( g(i, u, j) + J^{\mu_{k}}(j) \Big)
   , \quad \ i \in 1, 2, \dots, n
   \end{aligned}
   $$

- ~ The sequence of policies improve over previous ones. Since there are only a finite number of states, there are a finite number of policies, thereby the iterations converge to an optimal policy

- Can set a stopping criterion if improvement is within a threshold

- Structure is like a nested loop; (dual loop)

  - Start from a given $J_0$

  - `policy_improvement` (outer loop)

    {

      $\mu_{k+1}(i) = \arg \min_{u} \sum_{j = 1}^{n} p_{ij}(u) \Big( g(i, u, j) + J^{\mu_{k}}(j) \Big), \qquad \forall \ i \in 1, 2, \dots, n$

      `policy evaluation` (inner loop)

      {

    $$
      J_{l+1}(i) = \sum_{j = 1}^{n} p_{ij}(\mu_k(i)) \Big( g(i, \mu_k(i), j) + J_l(j) \Big), \qquad i = 1, 2, \dots, n, \quad l = 0, 1, 2, \dots
    $$

      }

      $J_l \to J^{\mu_k}$

    }

  - Repeat process if $J^{\mu_{k+1}}(i) < J^{\mu_{k}}(i)$ for atleast one $i \in S$

  - If $J^{\mu_{k+1}}(i) = J^{\mu_{k}}(i) \forall i = 1, 2, \dots, n$, then stop and output $\mu_k$ as the optimal policy

- Evaluation happens multiple times, improvement once in a while

- Modified policy iteration procedure: ~ Can decide how many steps to run the inner loop for, and terminate


### Proposition

The policy iteration algorithm generates an improving sequence of proper policies, i.e., $J^{\mu_{k+1}}(i) \leq J^{\mu_{k}}(i), \ \forall \ i, k$, and terminates with an optimal policy in a finite number of iterations.

#### Proof

Given a proper policy $\mu$, the new policy $\bar \mu$ is obtained via policy improvement as

$$
T_{\bar \mu} J^{\mu} = T J^{\mu}
$$

Then,

$$
J^{\mu} = T_{\mu} J^{\mu} \geq T J^{\mu} = T_{\bar \mu} J^{\mu}
, \qquad \implies
J^{\mu} \geq  T_{\bar \mu} J^{\mu}
$$

By monotonicity of $T_{\bar \mu}$,

$$
J^{\mu} \geq T_{\bar \mu} J^{\mu} \geq T_{\bar \mu}^2 J^{\mu} \geq T_{\bar \mu}^3 J^{\mu} \geq \dots \geq \lim_{k \to \infty} T_{\bar \mu}^k J^{\mu} = J^{\bar \mu}
, \qquad \implies
J^{\bar \mu} \leq J^{\mu}
, \quad \text{i.e., }
J^{\bar \mu}(i) \leq J^{\mu}(i), \ \forall i \in S
$$

Suppose $\bar \mu$ is improper $\implies J^{\bar \mu}(i) = \infty$ for some $i \in S$, by assumption-B

$\implies J^{\bar \mu}(i) = \infty$  for that $i \in S$, which is a contradiction since $\mu$ is proper

$\implies \bar \mu$ is also a proper policy

---

If $\mu$ is not optimal, then $J^{\bar \mu}(i) < J^{\mu}(i)$ for some $i \in S$. Otherwise,

$$
J^{\mu} = J^{\bar \mu} = T_{\bar \mu} J^{\bar \mu} = T_{\bar \mu} J^{\mu} = T J^{\mu}
, \qquad \implies
J^{\mu} = J^*
, \quad \text{since }
J^{\mu} = T J^{\mu}
$$

$\implies \mu$ is optimal. Thus, the new policy is strictly better than the current policy if the current policy is not optimal. Since the number of proper policies is finite, this procedure converges in a finite number of steps to an optimal proper policy.

## Modified policy iteration

Select a sequence of positive integers $m_0, \ m_1, \ m_2, \dots,$ and suppose $J_1, \ J_2, \dots,$ and stationary policies $\mu_0, \ \mu_1, \ \mu_2, \dots,$ are obtained as

$$
T_{\mu_k} J_k = T J_k
, \quad \text{ and } \quad
J_{k+1} = T_{\mu_k}^{m_k} J_k, \quad k = 0, 1, \dots, n
$$

We can show that this procedure terminates in an optimal policy $\mu^*$ and optimal value function $J^*$.

Consider,

1. $m_k = 1 \ \forall k$ : This is value iteration, since $J_{k+1} = T_{\mu_k} J_k = T J_k$

2. $m_k = \infty \ \forall k$ : Policy iteration

- We have a whole range of ~procedures between these two, depending on choice.

## Multi-stage look ahead policy iteration

Regular policy iteration uses a one-step look ahead and finds an optimal decision for the one-stage problem with one stage cost $g(i, u, j)$ and terminal cost $J^{\mu}(j)$ when the policy is $\mu$.

In an $m$-stage look ahead problem, we find the optimal policy for an $m$-stage DP, where we start in state $i \in S$, make $m$ subsequent decision incurring corresponding costs of $m$ stages and getting a terminal cost $J^{\mu}(j)$ where $j$ is the state after $m$ stages.

**Claim:** $m$-stage policy iteration terminates with optimal policy under same conditions as policy iteration

Let $\{ \bar\mu_0, \ \bar\mu_1, \dots, \ \bar\mu_{m-1} \}$ be an optimal policy for $m$-stage DP with terminal cost $J^\mu$. Thus,

$$
T_{\bar\mu_k} T^{m - k - 1} J^{\mu} = T^{m - k} J^{\mu}, \quad k = 0, 1, 2, \dots, m-1
$$

Suppose

$$
\begin{aligned}
k = m - 1 &: &
T_{\bar\mu_{m-1}} J^{\mu}
& =
T J^{\mu}
\\
k = m - 2 &: &
T_{\bar\mu_{m-2}} T J^{\mu}
& =
T^{2} J^{\mu}
\\ \vdots \\
k = 0 &: &
T_{\bar\mu_{0}} T^{m-1} J^{\mu}
& =
T^{m} J^{\mu}
\end{aligned}
$$

Now,

$$
T J^{\mu} \leq T_{\mu} J^{\mu} = J^{\mu}
\qquad \implies
T^{k+1} J^{\mu} \leq T^k J^{\mu} \leq J^{\mu}, \quad \forall \ k = 0, 1, 2, \dots
$$

Thus,

$$
T_{\bar\mu_k} T^{m - k - 1} J^{\mu} = T^{m - k} J^{\mu} \leq T^{m - k - 1} J^{\mu}, \quad \ \forall \ k = 0 , 1, \dots, m-1
$$

Thus, $\forall \ l \geq 1$,

$$
T_{\bar\mu_k}^l T^{m - k - 1} J^{\mu} \leq T_{\bar\mu_k} T^{m - k - 1} J^{\mu} = T^{m - k} J^{\mu}
$$

Taking limits as $l \to \infty$, we obtain

$$
J^{\bar\mu_k} = \lim_{l \to \infty} T_{\bar\mu_k}^l T^{m - k - 1} J^{\mu} \leq T^{m - k} J^{\mu} \leq J^{\mu}, \quad \ \forall \ k = 0 , 1, \dots, m-1
\label{eq:j_mu_lim_inf}
$$

Thus for a successor policy $\bar\mu$ generated by $m$-stage policy iteration, i.e., $\bar\mu = \bar\mu_0$, we have

$$
J^{\bar \mu} \leq T^m J^{\mu} \leq J^{\mu}
$$

by setting $k=0$ in $\eqref{eq:j_mu_lim_inf}$.

$\implies \bar\mu$ is an improved policy relative to $\mu_0$. If $J^{\bar \mu} = J^{\mu}$, then $J^{\mu} = T J^{\mu} \text{ and } J^{\mu} = J^*$, thereby ~optimal.

---

