# RL | 2025-03-13

## Policy evaluation with function approximation

**Given**: $\mu$

**Goal**: Find $\bar J_\mu$

$$
J_\mu(s) = \mathbb{E} \left[ \sum_{t = 0}^{+\infty} \gamma^t r(s_t, a_t) \Big| s_t = s \right]
$$

$a_t \sim \mu(\cdot \mid s_t)$

$s_0, a_0, s_1, \dots$

Eg:

$$
\mathcal{S} = \{ 1, 2 \}, \qquad \mathcal{A} = \{ L, R, U, D \} \qquad \implies P \to {(4 \times 2) \times 2}
$$

### Bellman equation

$$
J_\mu(s) = \mathbb{E}_{a, s'} \Big[ r(s, a) + \gamma J_\mu(s') \Big]
= \sum_{s', a} \mu(a \mid s) \ P(s' \mid s, a) \ \Big[ r(s, a) + \gamma J_\mu(s') \Big]
$$

$(s, a, r(s, a), s')$

$$
\mathbf{x}_{n+1} = \mathbf{x}_n + \alpha_n \Big[ r(s_n, a_n) + \gamma x_n(s_n') - x_n(s_n) \Big] \mathbf{e}_{s_n}
$$

$$
\begin{aligned}
s_n & \sim d_\mu(\cdot) \\
a_n & \sim \mu(\cdot \mid s_n) \\
s_n' & \sim P(\cdot \mid s_n, a_n)
\end{aligned}
$$

'Tabular $TD(0)$'

'Experience replay'

Stationary distribution of a Markov chain.

$\text{MDP} = (S, A, P, r, \gamma)$; Here $P \to SA \times S$

$\text{MC} = (S, P_\mu)$; Here $P_\mu \to S \times S$

$$
P_\mu(s, s') \triangleq \sum_{s} \mu(a \mid s) P(s' \mid s, a)
$$

Can see that this sum is non-negative, since it's composed of non-negative terms.

When summed over $s'$, it yields 1.

$P_\mu \to$ Probability of starting from $s$ and going to $s'$.

Policy $\mu$ induces a Markov chain.

Markov chain evolves as $s_0, s_1, s_2, \dots, s_N$, i.e., starting from $s_0$ and evolving.

In general, this evolution is random, in the sense that if we repeat the experiment, we get a different trajectory.

A **stationary distribution** is a distribution $d_\mu$ that satisfies

$$
d_\mu^\top P_\mu = d_\mu^\top, \qquad P_\mu \to S \times S
$$

If we sample $s_0$ from $d_\mu^\top$, then we can show that $s_1 \in d_\mu^\top$, hence 'stationary', since distribution doesn't change.

If we had the update rule as

$$
\mathbf{x}_{n+1} = \mathbf{x}_n + \alpha_n \Big[ r(s_n, a_n) + \gamma J_\mu(s_n') - x_n(s_n) \Big] \mathbf{e}_{s_n}
$$

it can be viewed as stochastic gradient descent. But it is harder to implement. Possible to analyse with stochastic gradient descent theory.

On the other hand

$$
\mathbf{x}_{n+1} = \mathbf{x}_n + \alpha_n \Big[ r(s_n, a_n) + \gamma x_n(s_n') - x_n(s_n) \Big] \mathbf{e}_{s_n}
$$

is easier to implement, but is disconnected from stochastic gradient descent theory / stochastic approximation theory.

It is not possible to find an objective function whose gradient is in this form. Can show that there exists none such $f$.

The design principles which stochastic approximation theory presents is very useful.

Goal is to minimise the objective function

$$
f(x) = \frac{1}{2} {\Vert J_\mu - x\Vert}_D^2
$$

and then we get to the stochastic version of this.

---

$J_\mu$ is a vector in $S$-dimensional space, which is very large.

We're interested to find a proxy/ approximate function that is in the same search space.

In function approximation, we want to reduce the search space.

We have linear function approximation.

---

Neural networks do non-linear function approximation, in the sense that the search space can be non-linear.

While Deep-RL has been shown to work, in general we don't know when it works and when it doesn't.

Current researchers also check intermediate policies, which doesn't seem right.

The theory is on very flimsy grounds.

A lot of times it fails to find the correct policy, and hits upon it later.

Not sure what's the discrepancy between theory and in practice.

---

### Linear function approximation

$$
\Phi \in \mathbb{R}^{S \times d}, \quad d << S
$$

$x = \operatorname{col}(\Phi)$. In the column space.

**New goal**: Find $\theta^*$ such that $J_\mu \approx \Phi \theta_*$

$$
f(\theta) = \frac{1}{2} {\Vert \Phi \theta - J_\mu \Vert}_{D_\mu}^2, \qquad D_\mu = \operatorname{diag} (d_\mu), \quad D_\mu \to S \times S
$$

$$
\implies
f(\theta) = \sum_{s} d_\mu(s) \frac{1}{2} {\Big( \underbrace{\Phi(s)}_{s^\text{th} \text{ row of } \Phi} \theta - J_\mu(s) \Big)}^2
$$

$$
\theta_{n+1} = \theta_n + \alpha_n [-\nabla f(\theta_n)]
$$

$$
\implies
\nabla f(\theta) = \sum_{s} d_\mu(s) \Big( \Phi(s) \theta - J_\mu(s) \Big)
$$

The new algorithm thus becomes

$$
\theta_{n+1} = \theta_n + \alpha_n \Big[ r(s_n, a_n) + \gamma \Phi^\top(s_n') \theta_n - \Phi^\top(s_n) \theta_n \Big] \Phi(s_n)
$$

$$
\begin{rcases}
\begin{aligned}
s_n & \sim d_\mu(\cdot) \\
a_n & \sim \mu(\cdot \mid s_n) \\
s_n' & \sim P(\cdot \mid s_n, a_n)
\end{aligned}
\end{rcases}
\ (s_n, a_n, s_n') \text{ is i.i.d.}
$$

'$TD(0)$ with linear function approximation'

Here, the whole operation happens in $d$-dimensional space, while previously it was $S$-dimensional.

Does this algorithm converge? If it converges, where does it converge? Does it converge to some useful point? How fast does it converge?

We will make use of stochastic approximation theory to analyse it.

Tabular $\text{TD}(0)$ is a special case of the $TD(0)$ with linear function approximation, with $\Phi$ as the identity matrix.

## Stochastic approximation theory

$$
\theta_{n+1} = \theta_n + \alpha_n \Big[ h(\theta_n) + M_{n+1} \Big]
$$

First, define a sigma-field

$$
\mathcal{F}_n = \sigma(\theta_0, s_0, a_0, r(s_0, a_0), s_0', s_1, a_1, r(s_1, a_1), s_1', \dots, s_{n-1}, a_{n-1}, r(s_{n-1}, a_{n-1}), s_{n-1}')
$$

Thoughout this discussion till now, we've assumed that initial point was given somehow. We'll deal with it later.

In some sense, the sigma-field is the information that we have till now. Given that, we can precisely know about $\theta_0, \dots, \theta_n$, i.e.,

$$
\underbrace{ \theta_0, \dots, \theta_n }_{\text{are measureable w.r.t } \mathcal{F}_n} \in \mathcal{F}_n
$$

Let

$$
\delta_n = r(s_n, a_n) + \gamma \Phi^\top(s_n') \theta_n - \Phi^\top(s_n) \theta_n
$$

$$
\theta_{n+1} = \theta_n + \alpha_n \delta_n \Phi(s_n)
$$

We know till $\theta_n$. Also, $\theta_n$ has no randomness given the information till now, i.e., $\mathcal{F}_n$.

$$
h(\theta_n) = \mathbb{E} \Big[ S_n \Phi(s_n) \Big| \mathcal{F}_n \Big]
$$

Note that expectation of a constant is the constant itself, i.e., $\mathbb{E}[c] = c$.

$$
\implies
h(\theta_n) = \mathbb{E} \Big[ r(s_n, a_n) \Phi(s_n) + \gamma \Phi(s_n) \Phi^\top(s_n') \theta_n - \Phi(s_n) \Phi^\top(s_n) \theta_n \Big| \mathcal{F}_n \Big]
$$

By linearity of he conditional expectation, we have

$$
\implies
h(\theta_n) = \mathbb{E} \Big[ r(s_n, a_n) \Phi(s_n) \Big| \mathcal{F}_n \Big] + \gamma \mathbb{E} \Big[ \Phi(s_n) \Phi^\top(s_n') \Big| \mathcal{F}_n \Big] \theta_n - \mathbb{E} \Big[\Phi(s_n) \Phi^\top(s_n) \Big| \mathcal{F}_n \Big] \theta_n
$$

$$
\implies
h(\theta_n) = \mathbb{E} \Big[ r(s_n, a_n) d(s_n) \Big] + \gamma \mathbb{E} \Big[ \Phi(s_n) \Phi^\top(s_n') \Big] \theta_n - \mathbb{E} \Big[\Phi(s_n) \Phi^\top(s_n) \Big] \theta_n
$$

since $(s_n, a_n, s_n')$ is independent of $\mathcal{F}_n$.

$$
\implies
h(\theta_n) = \sum_{s, a} d_\mu(s) \ \mu(a \mid s) \ r(s, a) \Phi(s) + \dots
$$

and similarly for other terms.

Note that $\Phi(s)$ is $d$-dimensional vector here.

In vector form, we would finally get the following equation

$$
\implies
h(\theta_n) = \Phi^\top D_\mu r_\mu + \gamma \Phi^\top D_\mu P_\mu \Phi \theta_n - \Phi^\top D_\mu \Phi \theta_n
$$

---

$$
\theta_{n+1} = \theta_n + \alpha_n \Big[ h(\theta_n) + M_{n+1} \Big]
$$

where

$$
M_{n+1} = S_n \Phi(s_n) - h(\theta_n)
$$

Note this form. Given $\mathcal{n}$, the new things are $s_n, a_n, s_n'$. Conditional expection ~makes it easy to skip over $\theta_n$.

$$
h(\theta_n) = \mathbb{E} \Big[ S_n \Phi(s_n) \Big| \mathcal{F}_n \Big]
$$

$$
\mathbb{E} \Big[ M_{n+1} \Big| \mathcal{F}_n \Big] = \mathbb{E} \Big[ S_n \Phi(s_n) \Big| \mathcal{F}_n \Big] - \underbrace{ \mathbb{E} \Big[ h(\theta_n) \Big| \mathcal{F}_n \Big] }_{h(\theta_n)}
$$

Now, we can view $M_n$ as a martingale difference sequence. $\to$ ~Zero mean.

Note observe that

$$
h(\theta_n) = b - A \theta, \quad \text{where } b = \Phi^\top D_\mu r_\mu, \quad A = \Phi^\top D_\mu (I - \gamma P_\mu) \Phi
$$

Can we find an objective function $g(\theta_n)$ whose gradient is this $h(\theta_n)$? i.e., $h(\theta_n) = \nabla g(\theta_n)$

Note that since $h(\theta_n)$ is linear in $\theta_n$, we expect $g(\theta_n)$ ot be quadratic in $\theta_n$.

But we won't be able to find such a function.

---

