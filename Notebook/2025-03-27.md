# RL | 2025-03-27

## Analysis of tabular $Q$-learning

$$
Q_{n+1} = Q_n + \alpha_n \big[ r(s_n, a_n) + \gamma \max_{a'} Q_n(s', a') - Q_n(s_n, a_n) \big] \, \mathbf{e}_{s_n, a_n}
$$

Stability, i.e., $\sup_{n \geq 0} \Vert Q_n \Vert \leq c$

$$
Q_{n+1} = Q_n + \alpha_n \Big[ D_{\pi_b} \big( \underbrace{ r + \gamma P(\Pi_{Q_n}) Q_n }_{T Q_n} - Q_n \big) M_{n+1} \Big], \qquad D_{\pi_b} = d_{\pi_b}(\lambda) \, \pi_{b}(a \mid s), \quad D_{\pi_b} \leftarrow SA \times SA
$$

$$
\implies Q_{n+1} = Q_n + \alpha_n \big[ D_{\pi_b} (T Q_n - Q_n) +M_{n+1} \big]
$$

$$
\implies \dot Q(t) = D_{\pi_b} \big( TQ(t) - Q(t) \big)
$$

Let $x(t) = Q(t) - Q_*$, again, then

$$
\dot x(t) = \dot Q(t) = D_{\pi_b} \big( T Q(t) - Q(t) \big) = D_{\pi_b} \big( T Q(t) - T Q_* + Q_* - Q(t) \big)
$$

$$
T Q = r + \gamma P \Pi_Q Q
$$

$$
T Q(s, a) = r(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \, \underbrace{ \max_{a'} }_{\Pi_{Q}} \, Q(s', a')
$$

$$
T Q_* = r + \gamma P \Pi_{Q_*} Q_* = Q_*
$$

$$
T Q_*(s, a) = r(s, a) + \gamma \sum_{s'} P(s' \mid s, a) \, \max_{a'} Q_*(s', a')
$$

$$
T Q - T Q_* = \gamma P \Pi_Q Q - \gamma P \Pi_{Q_*} Q_*
$$

$$
\begin{aligned}
\implies x(t)
& =
D_{\pi_b} \big( \gamma P \Pi_{Q} Q(t) - \gamma P \Pi_{Q*} \big)
\\ & =
D_{\pi_b} \Big( \gamma P \Pi_{Q} Q(t) x(t) + \gamma P \big( \Pi_{Q(t)} - \Pi_{Q_*} \big) - x(t) \Big)
\\ & =
D_{\pi_b} \big( \gamma P \Pi_{Q(t)} - I \big) \, x(t) + \gamma D_{\pi_b} P \big( \Pi_{Q(a)} - \Pi_{Q_*} \big) Q_*
\end{aligned}
$$

Let $\mu: \mathcal{S} \to \mathcal{A}$ be a deterministic policy. $\mu(s) C \arg\max_{a} Q(s, a)$

$$
R_\mu = \{ Q :\text{ All } \mu \text{ is greedy w.r.t. } Q \}
$$

**Claim:**

$R_\mu$ is a cone, i.e., $Q \in R_\mu \implies C Q \leq R_\mu, \ C > 0$

### Lemma 2: Vector comparison principle

Suppose $\bar f, \underline f: \mathbb{R}^d \to \mathbb{R}^d$ be a globally Lipschitz functions.

$\dots$

Let

$$
\underline f(x) = D_{\pi_b}\big( \gamma P \Pi_{x + Q_*} - I \big) \, x + \gamma D_{\pi_b} P \big( \Pi - \Pi_b \big) Q_*
$$

$$
\bar f(x) = D_{\pi_b} \big( P \Pi_* - \Pi \big) x
$$

We show that the upper bound converges to origin.

Consider $\underline f: (\Pi_{x + Q_*} - \Pi_{Q_*}) \geq 0$

$$
\Pi_{x + Q_*} \, Q_*(s) = Q_*(s, a'(s)) - \max_{a} Q_*(s, a)
$$

$$
a'(x) = \arg \max_{a} [x(s, a)] + Q_* (s, a)
$$

**Claim:**

$A_i$ is negative diagonal dominant, hence $L = I$ works

**Proof:**

$$
{[A_i]}_{(s, a) (s, a)} + \sum_{(s', a') \neq (s, a)} {\big\vert [A_i] \big\vert}_{(s, a) (s', a')} < 0
$$

$(s, a)$ through out this matrix

$$
\implies d_{\pi_b}(s) \, \pi_{b}(a \mid s) \, \begin{bmatrix} r & \dots & 1 \end{bmatrix} + d_{\pi_b} \, \pi_{b}(a \mid s) \sum_{(s', a') \neq (s, a)} \gamma^n < 0
$$

Let $\pi_\alpha = \pi_\mu$ for some $\mu$

$$
\begin{aligned}
\implies \pi_x (s, (s', a'))
& =
\begin{cases}
1, & \text{if } s' = s \text{ and } a = \mu(s) \\
0, & \text{otherwise}
\end{cases}
\\ & =
\gamma \sum_{s'} P(s' \mid s, a) \, \pi_\mu(s, a \mid s) + \gamma \sum \sum
\end{aligned}
$$

In tabular setting, we find the fixed point of Bellman operator.

In (linear) function approximation, we find the fixed point of the projected Bellman operator.

Behavorial policy $\pi_b$

---

