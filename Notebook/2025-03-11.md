# RL | 2025-03-11

## Application of SA to RL

Policy evaluation

Given a policy $H$, find $J_\mu \in \mathbb{R}^S \to$ State value function. $S = \vert \mathcal{S} \vert$

$$
J_\mu(x) = \mathbb{E}_{X_1, \dots, X_n} \left[ g_n(x_n) + \sum_{i = 0}^{n - 1} g_i(x_i, \mu(x_i), x_{i+1}) \Bigg| x_{i} = z \right]
$$

Policy $\mu: S \to \Delta(\mathcal{A})$
$$
\underbrace{s_0}_{= s}, a_0, r(s_0, a_0), s_1, a_1, r(s_1, a_1), s_2, \dots
$$

$$
s_1 \sim P(\cdot \mid s_0, a_0), \quad a_0 \sim \mu(\cdot \mid s), \quad a_1 \sim \mu(\cdot \mid s_1)
$$

~ Stochastic shortest path problem
$$
J_{\mu}(s) = \mathbb{E} \left[ \sum_{t = 0}^{+\infty} \gamma^t r(s_t, a_t) \Big| s_0 = s \right]
$$

Value function is defined as above, in this context. ~Optimisation, etc, in this sense.

- Has an expectation operator
- Discounting $\gamma$ is used
- Expected return that one would get $\to$ Value function
- The sum is over time, not over states

Environment gives the next state

$\gamma \to$ discounting factor; ~Indicates the weight given to future rewards

Eg: Getting 1 lakh today, vs 1 lakh at age 90.

Risk-sensitive RL

- Depends on risk apetite
- Going beyond expectation
- Uncertainty of taking a large value, and is that fine to allow

Bellman equation
$$
J_\mu(s) = \mathbb{E}_{a, s'} \left[ r(s, a) + \gamma J_\mu(s') \mid s_0 = s \right]
$$
The random variables here are $a, s'$

$s'$ is the next state that the environment gives
$$
J_\mu(s) = \sum_{a, s'} \mu(a \mid s) \ P(s' \mid s, a) \ \Big[ r(s, a) + \gamma J_\mu(s') \Big]
$$
This expression and the one above that sums over time are equivalent, here the sum is over $a, s'$.

Expectations can be infinite too, in general. But we assume some conditions, such as rewards are bounded to have it well-behaved.

---

Finding $J_\mu$, $J \in \mathbb{R}^S$.

Planning setup / Model-based setup

- Have access to $P$
- $S$ equations, in $S$ unknowns

Trying out policies from the environment.

- In certain cases, we're worried about intermediate policies also, due to cost associated in performing the actions
- Safe RL, takes this into account

Model-free learning

- We don't know $P$, but we know $\mu$, since user-defined
- Given $\mu$, knowledge about $P$, we want to find $J_\mu$

- $(s, a, r(s, a), s')$

- Law of large numbers (LLN)

  - For computing $\mathbb{E}(X)$

  - If we have access to i.i.d. samples $X_1, X_2, \dots, X_n$

  - Then LLN says that
    $$
    \frac{X_1 + \dots X_n}{n} \to \mathbb{E}(X)
    $$

  - We get a sequence of random variables
  - These are stochastic processes, now
    - First run: $X_1, \frac{X_1 + X_2}{2}, \dots$
    - Second run, will be different
  - Weak LLN $\to$ The convergence is with probability 1
  - Strong LLN $\to$ The convergence is almost surely

- Observe that the sum in the expectation is an infinite sum

  - How do we evaluate it?

- This makes sense, in episodic tasks, where it becomes a finite sum

  - $s_0, a_0, r(s_0, a_0), s_1, \dots, s_T$

  - Make a sequence of the rewards
    $$
    \sum_{t = 0}^{T - 1} \gamma^t r(s_t, a_t) + \gamma^T r(s_T)
    $$

    - Terminal part is written separately, since there is no action to take after reaching there
    - Note that there is no expectation here

  - Once we have this sequence of rewards, that gives one trajectory. This corresponds to one sample, and we repeat this getting multiple trajectories, and use LLN. Also, this is for one (starting) state, we have to do it for every state.

- For all $s$, collect $k$ samples of
  $$
  \sum_{t = 0}^{T - 1} \gamma^t r(s_t, a_t) + \gamma^T r(s_T)
  $$
  where $s_0 = s$.

  Say $C_1(s), \dots, C_k(s)$. Now
  $$
  J_\mu(s) \approx \frac{1}{K} \sum_{i = 1}^{K} C_i(s)
  $$
  We can choose $K$ as per convenience, and ability to get the samples.

  Can perform some analysis to determine some $K$ to get within some $\epsilon$ error. Such analysis also exists.

  This is a naive way of doing these things.

- Notion of smartness
  - Can define ~anyway, and show ~anything
  - The conventions followed have some hindsight, that is agreed upon

- Let's analyse complexity now

  - Space: Grows linearly with $n$
  - Time: Grows linearly with $n$

  Till now, this is non-incremental
  $$
  x_n = \frac{X_1 + \dots + X_n}{n}
  $$
  We can also see this as
  $$
  x_n = \frac{(n-1) x_{n-1} + X_n}{n}
  $$

  $$
  \implies x_n = \frac{n}{n} x_{n-1} - \frac{1}{n} x_{n-1} + \frac{X_n}{n} = x_{n-1} + \alpha_n [X_n - x_{n-1}]
  $$

  $$
  \alpha_n = \frac{1}{n}
  $$

  Now, this is incremental

  No need to compute everytime from the very beginning. ~If new samples come along, we just with it.

- Is such incremental versions possible to find everytime?
  - Gradient descent

  $$
  f(x) = \frac{1}{2} (x - \mathbb{E}(x))^2
  $$

  $$
  \nabla f(x) = x - \mathbb{E} X
  $$

  $$
  x_{n+1} = x_n + \alpha_n [- \nabla f(x_n)] = x_n + \alpha_n (\mathbb{E}X - x_n)
  $$
  
  - Stochastic gradient descent
    $$
    x_{n+1} = x_n + \alpha_n (X_{n+1} - x_n), \qquad \alpha_n = \frac{1}{n+1}
    $$
    Replace the expectation with a sample $X_{n+1}$
  
- Now, we can apply this techniques on $J_\mu$
  $$
  \widehat J_\mu^{n+1}(s) = \widehat J_\mu^{n}(s) + \alpha_n \left[ C_{n+1}(s) - \widehat J_\mu^{n}(s) \right], \qquad \alpha_n = \frac{1}{n+1}
  $$
  Need to repeat for all states

This is very naive, we can do better, that's where TD learning comes in.

### Temporal difference learning

Proposed by Richard Sutton

'$TD(0)$ algorithm with Markov sampling'
$$
x_{n+1} = x_n + \alpha_n \Big[ r(s_n, a_n) + \gamma r_n(s_{n+1}) - x_c(s_n) \Big] e_{s_n}
$$
Vector notation. Standard basis vector $e_s$.

Notice that we don't need to wait for anything here. ~On the fly.

Running this for a long time, and under some reasonable conditions, one can show that $x_n \to J_\mu$

Some arbitrary $s_0$, and proceed

Get hold of tuples $(s_n, a_n, r(s_n, a_n), s_{n+1})$

~Also, we know some policy $\mu$ (?)

This algorithm combines the knowledge that is got $\dots$

The $J_\mu$'s are realted by the Bellman equation.

~This paradigm/idea is what temporal difference approach is.

---

Design of algorithms

- Involves a lot guess work
- Cannot be taught

Gradient descent algorithm
$$
f(x) = \frac{1}{2} \Vert J_\mu - x \Vert_D^2 = \frac{1}{2} \sum_{s'} d(s) [J_\mu(s) - x(s')]^2
$$

$$
\nabla f(x) = - \sum_{s} d(s) [J_\mu(s) - x(s)] \ e_s
$$

$$
x_{n+1} = x_n + \alpha_n [- \nabla f(x_n)]
= x_n + \alpha_n \left[ \sum_{s'} d(s) \left( J_\mu(s) - x_n(s) \right) \right] e_s
$$

Notice that this is somewhat similar to the ~TD learning counterpart

Do we know $J_\mu$ here? We cannot replace this expectation with sample as before, since we have to deal with an infinite sum.
$$
x_{n+1} = x_n + \alpha_n \sum_{s, a, s'} d(s) \ \mu(a \mid s) \ P(s' \mid s, a) \ \Big[ r(s, a) + \gamma J_\mu(s') - x_n(s) \Big] e_s
$$
This looks similar, except that we also have the $\mu$ and $P$ factor.

Note that the $x_n(s)$ term doesn't depend on $a, s'$.

One way to go about this is to replace $J_\mu$ term (which is an expectation with an infinite sum) with some sample that we get.

If we're more lazy (Sutton's idea), we can replace this $J_\mu$ term with $x_n(s')$ instead.

But the problem now is we get disconnected from stochastic gradient theory.

We cannot see this update rule, as the gradient of any objective function.

Now, if we plug in with a sample
$$
x_{n+1} = x_n + \alpha_n \Big[  \Big]
$$
'$TD(0)$ algorithm obtained from a generative model'

Sample a state from some distribution $d$, action from some policy $\mu$, and next state is given by $P$, the transition kernel.

This involves one iteration. Sampling afresh for another iteration. This is the difference between the Markov sampling case and the generative model.

The nice part about this algorithm is that, assuming we can sample from a distribution $d$, this is model-free.

We don't even need to know $d$, all we need is the ability to sample from $d$.

If we allow a Markov chain to evolve, the MC goes to it's stationary distribution.

Markov noise is there in the Markov sampling case. Dependence of next state and previous state.

Implementing Markov sampling is easier in practice, no need to wait for anything, but analysing is difficult.

The generative model case is easier to analyse with stochastic approximation tools, but has drawbacks while implementing.

---

