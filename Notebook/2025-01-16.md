# RL | 2025-01-16

## Markov decision process (MDP)

State space $S$, action spce $A$

Problem of optimal control.

### Controlled Markov chain (CMC)

Given state $S$, let $A(s)$ be the set of feasible actions in state $s$. Then $A = \bigcup_{s \in S} A(s)$.

Let $\{ X_n \}$ be a stochastic process that depends on a control valued sequence $\{ Z_n \}$. We assume that $Z_n \in A(X_n), \ \forall \ n$. Then $\{ X_n \}$ is a controlled Markov chain if

$$
\begin{aligned}
&
P(X_{n+1} = j \mid X_n = i, Z_n = a, X_{n-1} = i_{n-1}, Z_{n-1} = a_{n-1}, \dots, X_0 = i_0, Z_0 = a_0)
\\ & \qquad =
P(X_{n+1} = j \mid X_n = i, Z_n = a), \quad \forall \ n
\\ & \qquad \triangleq
P(i, a, j)
\end{aligned}
$$

~ Similar to a Markov chain; In MC, only state evolutions happen.

Note that

$$
\begin{aligned}
P(i, a, j) & \in [0, 1], \quad \forall \ i, a, j
\\
\sum_{j} P(i, a, j) & = 1
\end{aligned}
$$

Here (in this course), we only deal with finite state space problems mainly.

### Markov decision process (MDP)

An MDP is a controlled Markov chain with a cost associated with every transition, $g(i_n, a_n, i_{n+1})$, where $X_n = i_n, Z_n = a_n, X_{n+1} = i_{n+1}$. Again, here $i_n, i_{n+1} \in S, \ a_n \in A(i_n)$.

~ Can also depend on time, i.e., as $g_n(\cdot)$

---

**Goal:** Find an "optimal" policy.

~At every single stage, at every single time.

Let

$$
J_\pi(x_0) = \operatorname{E} \left[ g_N(X_N) + \sum_{k=0}^{N-1} g_k(X_k, \mu_k(X_k), X_{k+1}) \Bigg| X_0 = x_0 \right]
$$

Here, $g_k(X_k, \mu_k(X_k), X_{k+1})$ is the single stage cost when state is $X_k$ at time $k$, action is $\mu_k(X_k)$ and next state is $X_{k+1}$ at time instant $k = 0, 1, 2, \dots, N-1$.

$g_N(X_N) \to$ Terminal cost, when terminal state is $X_N$.

---

## Principle of optimality

Let $\pi^* = \{ \mu_{0}^*, \mu_{1}^*, \dots, \mu_{N-1}^* \}$ be an optimal policy. Assume that when following $\pi^*$, state $x_i$ occurs in stage $i$ with positive probability. Consider the following subproblem starting in state $x_i$ at time $i$,

$$
J_k(x_k) = \min_{\pi^i = \{ \mu_{i}, \mu_{i+1}, \cdots, \mu_{N-1} \}} \operatorname{E} \left[ g_N(X_N) + \sum_{k=i}^{N-1} g_k(X_k, \mu_k(X_k), X_{k+1}) \Bigg| X_i = x_i \right]
$$

Then the truncated policy $\pi^{*i} = \{ \mu_{i}^*, \mu_{i+1}^*, \dots, \mu_{N-1}^* \}$ is optimal for this tail subproblem (~tail policy).

## Dynamic programming algorithm (DP)

### Proposition

For every initial state $x_0 \in S$, the optimal cost $J^*(x_0) = J_0(x_0)$ that is obtained from the last step of the following algorithm,

$$
\begin{aligned}
J_N(x_N)
& =
g_N(x_N)
\\
J_k(x_k)
& =
\min_{a_k \in A(x_k)} \operatorname{E_{X_{k+1}}} \Big[ g_k(X_k, \mu_k(X_k), X_{k+1}) + J_{k+1}(X_{k+1}) \Big]
\\ & \quad
\forall \ k = N-1, N-2, \dots, 0
\end{aligned}
$$

---

