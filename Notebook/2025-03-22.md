# RL | 2025-03-22

## Policy evaluation with function approximation ($TD(0)$)

- $Q$-learning
  - Tabular approach
  - Function approximation $\leftarrow$ D$Q$N
    - Works well when state and action space are finite

$$
\theta_{n+1} = \theta_n + \alpha_n \, \delta_n \, \phi(s_n), \qquad \text{where } \delta_n = r(s_n, a_n) + \gamma \, \phi^\top(s_n') \, \theta_n - \phi^\top(s_n) \, \theta_n
$$

$$
\theta_{n+1} = \theta_n + \alpha_n \, \big[ h(\theta_n) + M_{n+1} \big], \qquad \text{where } b = \phi^\top \, D_\mu \, r_\mu, \quad A = \phi^\top \, D_\mu \, (\mathcal{I} - \gamma P_\mu) \, \phi
$$

$$
\mathbb{E}{\Vert M_n \Vert}^2 < \infty, \quad \mathbb{E} \big[ M_{n+1} \mid \mathcal{F}_n \big] = 0
$$

Need to show that

$$
\mathbb{E} \big[ {\Vert M_{n+1} \Vert}^2 \mid \mathcal{F}_n \big] \leq k \Big[ 1 + {\Vert \theta_n \Vert}^2 \Big]
$$

**Conditional expressions:**

$\underbrace{\mathbb{E}[X \mid G]}_{\text{Random variable}} \to$ Best representation of $X$ given the knowledge of $G$

$\underbrace{X}_{\text{Random variable}} \mid \underbrace{G}_{\text{Sigma field}}$

$$
\mathcal{F}_n = \sigma \big( \theta_0, \ s_0, \ a_0, \ r(s_0, a_0), \ s_0', \, \dots \, , \ s_{n-1}, \ a_{n-1}, \ r(s_{n-1}, \ a_{n-1}), \ s_{n-1}' \big)
$$

$$
\begin{aligned}
M_{n+1}
& =
\delta_n \ \phi(s_n) - \big( b - A \theta_n \big)
\\ & =
\Big[ r(s_n, a_n) + \gamma \, \phi^\top(s_n) \, \theta_n - \phi^\top(s_n) \, \theta_n \Big] \, \phi(s_n) - \big( b - A \theta_n \big)
\\ & =
\Big[ r(s_n, a_n) \, \phi(s_n) - b \Big] + \Big[ A - \big( \phi(s_n) \, \phi^\top(s_n) - \gamma \, \phi(s_n) \, \phi^\top(s_n') \big) \Big] \, \theta_n
\\
\because \quad & \gamma \, \phi(s_n')^\top \, \theta_n \, \phi(s_n) = \gamma \, \phi(s_n) \, \phi^\top(s_n') \theta_n
\end{aligned}
$$

We have ${(a+b)}^2 \leq 2 a^2 + 2 b^2$

$$
\implies
{\Vert M_{n+1} \Vert}^2
\leq
2 {\Big\Vert \underbrace{ r(s_n, a_n) \, \phi(s_n) }_{\mathbb{E}[b]} - b \Big\Vert}^2 + 2 {\Big\Vert \underbrace{ A - \big( \phi(s_n) \, \phi^\top(s_n) - \gamma \, \phi(s_n) \, \phi^\top(s_n') \big) }_{{\Vert \theta_n \Vert}^2} \Big\Vert}^2
\leq
k \Big[ 1 + {\Vert \theta_n \Vert}^2 \Big]
$$

Monotonicity property of conditional expressions: $X \leq Y \implies \mathbb{E}[X] \leq \mathbb{E}[Y]$

$$
\implies \mathbb{E} \big[ {\Vert M_{n+1} \Vert}^2 \mid \mathcal{F}_n \big] \leq \mathbb{E} \Big[ k \big[ 1 + {\Vert \theta_n \Vert}^2 \big] \Big| \mathcal{F}_n \Big] = k \big[ 1 + {\Vert \theta_n \Vert}^2 \big]
$$

Conditional expectation of the second moment; We need noise terms to be eliminated.

Expectation of the second moment needs to be finite (?)

We now verify **(a)**; To show

$$
\mathbb{E}{\Vert M_n \Vert}^2 < \infty, \ \forall \ n \geq 1
$$
Not
$$
\sup_n \mathbb{E}{\Vert M_n \Vert}^2 < \infty
$$

$$
\begin{aligned}
\theta_{n+1} & = \theta_n + \alpha_n \, \big( b - A \theta_n + M_{n+1} \big) \\
M_{n+1} & = \delta_n \ \phi(s_n) - \big( b - A \theta_n \big)
\\ & = \Big[ r(s_n, a_n) \, \phi(s_n) - b \Big] + \Big[ A - \big( \phi(s_n) \, \phi^\top(s_n) - \gamma \, \phi(s_n) \, \phi^\top(s_n') \big) \Big] \, \theta_n
\end{aligned}
$$

$$
{\Vert M_{n+1} \Vert} \leq k_1 + k_2 \Vert \theta_n \Vert \leq k' [1 + \Vert \theta_n \Vert], \quad \text{where } k' = \max \{ k_1, k_2 \}
$$

$$
\theta_{1} = \theta_0 + \alpha_0 \, \delta_0 \, \phi(s_0) = \theta_0 + \alpha_0 \, \Big[ r(s_0, a_0) \, \phi(s_0) + \big( \gamma \, \phi^\top(s_0') \, \phi(s_0') - \phi^\top(s_0) \, \phi(s_0) \big) \, \theta_0 \Big], \qquad \Vert \theta \Vert \leq c
$$

$$
\vert X \vert \leq c \implies \mathbb{E} \big[ \vert X \vert \big] \leq c
$$

$$
h_c(\theta) = \frac{h(c \, \theta)}{c}, \quad c \geq 1, \qquad \quad \lim_{c \to \infty} h_c(\theta) = h_{\infty}(\theta), \qquad \quad \dot\theta(t) = h_{\infty}(\theta(t))
$$

Let it's ($h_{\infty}$) origin be GAS. Equilibrium $h_{\infty}(\theta) = - A \theta$

$$
h(\theta) = b - A \theta
\implies
h_c(\theta) = \frac{h(c \, \theta)}{c} = \frac{b - A (c \, \theta)}{c} = \frac{b}{c} - A \theta
$$

$$
\text{As } c \to \infty \implies h_{\infty}(\theta) = - A \theta
$$

$$
\dot\theta(t) = - A \theta(t)
$$

$$
V(\theta) = \frac{1}{2} {\Vert \theta \Vert}^2, \qquad \quad V(\theta) = 0 \ \text{ iff } \ \Vert \theta \Vert = 0 \iff \theta = 0
$$

$$
\frac{d}{dt} V(\theta(t)) = \nabla \, V^\top(\theta(t)) \, h_{\infty}(\theta(t)) < \infty
$$

$\xrightarrow{?} - \theta^\top A \theta < 0, \ \forall \ \theta \neq 0$

---

## Optimal policies

How to find the optimal policy? $\to$ A control problem

Easy for small MDPs, but what about large state-action space?

Reinforcement learning

- Value function based

  - $Q$-learning, etc

  - To find $Q_*$
    $$
    \begin{aligned}
    \pi^* \longrightarrow Q_{\pi^*} & = Q_* \\
    Q_* & = T Q_*, \quad Q_* = T_{\pi^*} Q_*
    \end{aligned}
    $$
    where $\pi^* \to$ Optimal policy

  - $\pi^*$ is greedy w.r.t. $Q_*$

- Policy based

  - Policy gradient, TRPO, etc
  - To find $\pi^*$
  - $\sigma^\top Q(\pi_0)$

- Mixed

  - Actor-critic algorithms, etc

### $Q$-learning

$Q_n(s_n', a_n'), \quad s_n' \sim P(\cdot \mid s_n, a_n), \quad a_n' \sim \mu(\cdot \mid s_n')$

$$
Q_{n+1} = Q_n + \alpha_n \big[ r(s_n, a_n) + \gamma \max_{a'} Q(s_n', a') - Q_n(s_0, a_n) \big] \mathbf{e}_{s_n, a_n}
$$

where $Q_n$ is non-linear. If not $\max_{a'}$, then $a' \sim \pi_b(\cdot \mid s_n)$

As $n \to \infty$, $Q_n \to \ ?$

Objective function $f(\theta)$

$$
f(\theta) = \frac{1}{2} {\Vert Q - Q_* \Vert}^2
$$

$$
Q_{n+1} = Q_n + \alpha \big[ Q_* - Q_n \big] = Q_n + \alpha \big[ T Q_* - Q_n \big]
$$

$$
T Q_*(s, a) = \mathbb{E} \Big[ r(s, a) + \gamma \max_{a'} Q_a(s', a') \Big], \qquad s' \sim P(\cdot \mid s, a)
$$

$$
Q_{n+1}(s, a) = Q_n(s, a) + \alpha_n \Big[ \mathbb{E} r(s, a) + \gamma \max_{a'} Q_*(s', a') - Q_n(s, a) \Big], \ \forall (s, a)
$$

$$
Q_{n+1}= Q_n + \alpha_n \Big[ r(s_n, a_n) + \gamma \max_{a'} Q_*(s_n', a') - Q_n(s_n, a_n) \Big] \mathbf{e}_{s_n, a_n}
$$

### Behaviour policy

Fixed behaviour policy $\pi_b$

$$
a_n \sim \pi_b(\cdot \mid s_n), \qquad s_n \sim \delta_{\pi_b} \to \text{stationary}, \qquad s_n' \sim P(\cdot \mid s_n, a_n)
$$

What happens if $a'$ is choosen based on $\pi_b$?

$$
Q_{n+1} = Q_n + \alpha_n \big[ b - A Q_n + M_{n+1} \big], \qquad b = \mathbb{E}[r(s_n, a_n) \, \mathbf{e}(s_n, a_n)], \qquad A^{-1} b = Q_\pi
$$

Stable, $\sup_n \Vert Q_n \Vert < \infty$ a.s.

$$
Q_{n+1}(s, a) =
\begin{cases}
Q_n(s, a), & \forall \ (s, a) \neq (s_n, a_n) \\
(1 - \alpha_n) Q_n(s, a) + \alpha_n \Big[ r(s, a) + \gamma \max_{a'} Q(s_n, a') \Big], & \forall \ (s, a) = (s_n, a_n)
\end{cases}
$$

End $s' = s_n'$

**Goal:** Finding such that ${\Vert Q_n \Vert}_{\infty} \leq c$ then ${\Vert Q_{n+1} \Vert}_{\infty} \leq c$, $\forall (s, a) = (s_n, a_n)$ and $s' = s_n'$

Triangular inequality

$$
\begin{aligned}
\Big\vert Q_{n+1}(s, a) \Big\vert & \leq (1 - \alpha_n) \, \Big\vert Q_n(s, a) \Big\vert + \alpha_n \Big[ \big\vert r(s, a) \big\vert + \gamma \max_{a'} \big\vert Q(s_n, a') \big\vert \Big]
\\ & \leq
(1 - \alpha_n) \, c + \alpha_n \, \big[ R_{\text{max}} + \gamma \, c \big] \qquad \sim \leq c
\end{aligned}
$$

$$
\alpha_n (\gamma - 1) \, c + \alpha_n \, R_{\text{max}} \leq 0 \implies R_{\text{max}} \leq (1 - \gamma) \, c
$$

$$
\implies c \geq \frac{R_{\text{max}}}{(1 - \gamma)}, \qquad {\Vert Q_0 \Vert}_{\infty} \leq c, \ \forall \ n
$$

$$
\implies c = \max \left\{ {\Vert Q_0 \Vert}_{\infty}, \ \frac{R_{\text{max}}}{(1 - \gamma)} \right\}
$$

Deadly triad

- Function approximation
- TD learning
- Off-policy learning

On-policy learning (policy iteration)

---

