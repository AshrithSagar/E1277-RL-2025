# RL | 2025-01-07

## Review

- Considering a single agent interacting with the environment

- Short-term rewards are the one-stage rewards/ ~immediate rewards
- Expected values of the rewards are taken, since rewards are probabilistic in general
- The value function is the long-term rewards, in a sense

## Policy

- A decision rule

- In a given state, it prescribes the action to be chosen

### Deterministic policy

Exactly one action is mapped to every state

### Random policy

- The policy is now a distribution

- From each state, choosing an action with some probability

## Reinforcement learning problem

An RL problem has two parts

- **Prediction problem**
  - Given a policy $\pi$, estimate the value function $v_\pi$

- Control problem
  - Find the best policy

### Markovian property

$$
P(S_{t+1} = s' \mid S_t = s, A_t = a, S_{t-1} = s_1, A_{t-1} = a_1, \dots) = P(S_{t+1} = s' \mid S_t = s, A_t = a)
$$

In the homogeneous case, ~ these values don't change with time, and we have

$$
P(S_{t+1} = s' \mid S_t = s, A_t = a) = P(S_1 = s' \mid S_0 = s, A_0 = a)
$$

The state evolution is Markovian, while the rewards are probabilistic; The reward dynamics is homogeneous.

## Multi-armed bandit problem

'Bandit' comes from gambling context.

We have a single slot machine with $k$ arms; Single state and multiple ($k$) actions to choose from.
Each time we pull an arm, we get a reward.

**Assumption:**

The rewards from various arms follows a distribution. Different distributions for different arms, in general.

These distributions are unknown; and uncorrelated, in the simplified case.

The value function,

$$
q^*(a) = \operatorname{E}[R_{t+1} \mid A_t = a], \qquad a = 1, 2, \dots, k
$$

The goal is to find the best arm, i.e., $a^* \in \arg \max_{a} q^*(a)$. We use $\in$ to account for the fact that multiple arms can be optimal, we're just interested in any one that works for our purpose.

### Strategies to find $a^*$

Let

$$
Q_n(a) = \frac{\sum_{i=1}^{n} R_{i} \ \mathbb{I}_{A_{i-1} = a}}{\sum_{i=1}^{n} \mathbb{I}_{A_{i-1} = a}}, \qquad a = 1, 2, \dots, k
$$

$Q_n(a)$ is the estimate of $q^*(a)$ at time $n$.

The indicator function $\mathbb{I}$ is

$$
\mathbb{I}_{A_{i-1} = a} =
\begin{cases}
1, & \text{if } A_{i-1} = a \\
0, & \text{otherwise}
\end{cases}
$$

$Q_n(a)$ is basically calculating the average reward for action $a$, over $n$ trials.

The denominator $\sum_{i=1}^{n} \mathbb{I}_{A_{i-1} = a}$ is the number of times action $a$ is picked in the $n$  trials.

### Greedy policy

Select action $a$ s.t.

$$
a = \underset{b \in \{1, 2, \dots, k\}}{\arg \max} Q_n(b)
$$

Not good because it doesn't allow for exploration.

### $\varepsilon$-greedy policy

Select action $a$ s.t.

$$
a =
\begin{cases}
\arg \max_{a} Q_n(a), & \text{w.p. } (1 - \varepsilon) \\
\text{random action}, & \text{w.p. } \varepsilon
\end{cases}
$$

Take $\varepsilon = 0.1, 0.01, 0.2, \dots$; $\varepsilon$ can go lower as time progresses.

Once we already have a best/ good action, it makes no sense to keep some $\varepsilon$ to select some worse action.

Can lower $\varepsilon$ over time, but ~better not to drop to $0$; Want to keep exploring.

---

