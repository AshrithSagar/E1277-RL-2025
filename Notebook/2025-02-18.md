# RL | 2025-02-18

## Policy iteration for discounted cost

### Proposition: Policy iteration

Let $\mu$ and $\bar\mu$ be two stationary policies such that $T_{\bar\mu} J^{\mu} = T J^{\mu}$, or equivalently, for $i = 1, 2, \dots, n$,

$$
g(i, \bar\mu(i)) + \alpha \sum_{j = 1}^{n} p_{ij}(\bar\mu(i)) \ J^{\mu}(j) = \min_{u \in A(i)} \left[ g(i, u) + \alpha \sum_{j = 1}^{n} p_{ij}(u) \ J^{\mu}(j) \right]
$$

Then

$$
J_{\bar\mu}(i) \leq J_{\mu}(i), \quad \forall \ i = 1, 2, \dots, n
$$

Moreover, if $\mu$ is not optimal, strict inequality holds in the above for atleast one state $i$.

This results ultimately gives the policy iteration procedure.

#### Proof

A straightforward proof

Since $J^{\mu} = T_{\mu} J^{\mu}$, and by hypothesis, $T_{\bar\mu} J^{\mu} = T J^{\mu}$, $\implies \forall i$,

$$
J^{\mu}(i)
= g(i, \mu(i)) + \alpha \sum_{j = 1}^{n} p_{ij}(\mu(i)) \ J^{\mu}(j)
\geq g(i, \bar\mu(i)) + \alpha \sum_{j = 1}^{n} p_{ij}(\bar\mu(i)) \ J^{\mu}(j) = T_{\bar\mu} J^{\mu}(i)
$$

Thus,

$$
J^{\mu} = T_{\mu} J^{\mu} \geq T J^{\mu} = T_{\bar\mu} J^{\mu}, \qquad \therefore J^{\mu} \geq T_{\bar\mu} J^{\mu}
$$

Applying $T_{\bar\mu}$ repeatedly above and using monotonicity,

$$
J^{\mu} \geq T_{\bar\mu} J^{\mu} \geq T_{\bar\mu}^2 J^{\mu} \geq \cdots \geq \lim_{k \to \infty} T_{\bar\mu}^k J^{\mu} = J^{\bar\mu}, \qquad \implies J^{\mu} \geq J^{\bar\mu}
$$

If $J^{\mu} = J^{\bar\mu}$, then since $T_{\bar\mu} J^{\mu} = T J^{\mu}$, we have

$$
J^{\mu} = J^{\bar\mu} = T_{\bar\mu} J^{\bar\mu} = T_{\bar\mu} J^{\mu} = T J^{\mu} = T J^{\bar \mu}
$$

$$
\therefore J^{\mu} = T J^{\mu}, \ \text{and} \ J^{\bar\mu} = T J^{\bar\mu}
$$

Since $T$ has a unique fixed point,

$$
J^{\mu} = J^{\bar\mu} = J^{\ast} \qquad \text{(optimal value function)}
$$

$\implies \mu, \bar\mu$ are optimal policies. 

~ The optimal value function is unique, it is a global optimum.

Thus, if $\mu$ is not optimal, then $J^{\mu}(i) > J^{\bar\mu}(i)$, for atleast one $i \in S$.

### Policy iteration algorithm

**Step-1:** Initialise a stationary policy $\mu_0$

~ The better the guess, the more likeliness of a faster convergence.

**Step-2:** **Policy evaluation**

Given a stationary policy $\mu^k$, compute corresponding cost function $J^{\mu^k}$ from the linear system of equations

$$
(I - \alpha P_{\mu^k}) J^{\mu^k} = \bar g_{\mu^k}
$$

or solve

$$
J^{\mu^k} = \bar g_{\mu^k} + \alpha P_{\mu^k} J^{\mu^k} \qquad \text{or} \quad J^{\mu^k} = T_{\mu^k} J^{\mu^k}
$$

**Step-2:** **Policy improvement**

Obtain a new stationary policy $\mu^{k+1}$ satisfying $T_{\mu^{k+1}} J^{\mu^k} = T J^{\mu^k}$.

If $J^{\mu^k} = T J^{\mu^k}$, then we stop, else we go back to Step-2 and repeat the process.

**Note:** Here

$$
\bar g_{\mu^k} = \begin{bmatrix} \bar g(1, \mu^k(1)) \\ \bar g(2, \mu^k(2)) \\ \vdots \\ \bar g(n, \mu^k(n)) \end{bmatrix} = \begin{bmatrix}
\sum_{j = 1}^{n} p(1, \mu^k(1), j) \ g(1, \mu^k(1), j) \\
\sum_{j = 1}^{n} p(1, \mu^k(2), j) \ g(2, \mu^k(2), j) \\
\vdots \\
\sum_{j = 1}^{n} p(1, \mu^k(n), j) \ g(n, \mu^k(n), j)
\end{bmatrix}
$$

Also,

$$
P_{\mu^k} = \begin{bmatrix}
p(1, \mu^k(1), 1) & p(1, \mu^k(1), 2) & \dots & p(1, \mu^k(1), n) \\
p(2, \mu^k(2), 1) & p(2, \mu^k(2), 2) & \dots & p(2, \mu^k(2), n) \\
\vdots & \vdots & \ddots & \vdots \\
p(n, \mu^k(n), 1) & p(n, \mu^k(n), 2) & \dots & p(n, \mu^k(n), n) \\
\end{bmatrix}
$$

The row sums are $1$ for $P_{\mu^k}$

---

## Topics

The story so far

- Basics of RL
  - RL
    - Environment (states)
    - Agent (actions)
    - Reward (signal)
  - Formulating problems in RL setting
- Multi-armed bandits
  - Single state with multiple actions
  - Greedy strategy
  - $\epsilon$-greedy strategy
  - UCB exploration
  - Gradient based search
- Markov decision process
  - We assume knowledge of system model
    - Transition probabilities
    - Reward(/Cost) function
  - MDP settings
    - Finite horizon problems ($N < \infty$ but deterministic)
      - Dynamic programming algorithm
    - Stochastic shortest path problems ($N < \infty$ but random), a.k.a. Episodic problems
    - Discounted cost problems ($N = \infty$)
  - Bellman equation
    - Value iteration
    - Policy iteration
  - ~ Converting DCP to SSPP
    - Similarity arises due to proper policies; termination
    - Can treat both in isolation and solve

---

Story from now on

- We shall assume no knowledge of system model
- In return, we have access to data

Data: ~Sequence of states, actions and rewards.

$S_0, A_0, R_1, S_1, A_1, R_2, \dots, S_{T-1}, A_{T-1}, R_T, S_T \mid$ $S_T \to$Terminal state, and $T \to$Terminal time

~ For DCP, it is an infinite trajectory, but we can consider a finite one practically.

Here, we consider tuples of data,

$$
(S_0, A_0, R_1, S_1), \ (S_1, A_1, R_2, S_2), \, \dots, \ (S_{T-1}, A_{T-1}, R_T, S_T)
$$

## Monte-Carlo schemes

Recall, the cost-to-go under policy $\mu$

$$
J^{\mu}(i) = \operatorname{E}_{\mu} \left[ \sum_{k = 1}^{T} r(S_k, \mu(S_k), S_{k+1}) \Bigg| S_0 = i\right]
$$

Monte-Carlo schemes largely with sample averages of collected data over trajectories.

**Example:** Consider SSPP or episodic task problem where $S = \{ 1, 2, 3, 4 \}$ and suppose $G$ is a terminal or goal state.

![Example -- Monte-Carlo schemes](./TeX/2025-02-18/1.png){ width=70% }

### First visit method

~ Calculating the estimates $\hat V$

#### Red episode

$$
\hat V_1(2) = r^1_1 + r^1_2 + r^1_3 + r^1_4 + r^1_5 + r^1_6
$$

$$
\hat V_1(1) = r^1_2 + r^1_3 + r^1_4 + r^1_5 + r^1_6
$$

$$
\hat V_1(3) = r^1_3 + r^1_4 + r^1_5 + r^1_6
$$

We skip $\hat V_2(2)$, which would've been the second visit to state 2, and move to next state.

$$
\hat V_1(4) = r^1_5 + r^1_6
$$

We skip $\hat V_2(3)$.

#### Green episode

$$
\hat V_2(3) = r^2_1 + r^2_2 + r^2_3 + r^2_4
$$

$$
\hat V_2(4) = r^2_2 + r^2_3 + r^2_4
$$

$$
\hat V_2(2) = r^2_3 + r^2_4
$$

We skip $\hat V_3(4)$.

Suppose we had $n$ estimates for a state $s$: $\hat V_1(s), \ \hat V_2(s), \dots, \ \hat V_n(s)$

$$
V(s) \approx \frac{1}{n} \sum_{k=1}^{n} \hat V_k(s)
$$

### Every visit method

Similar to first visit method, except that we do not ignore states on an episode that are not visited the first time.

~ There is some dependence between the values in every visit method. Still converges, though. Helpful in data starved cases.

Many times, we also refer to

$$
G_m \triangleq r_{m+1} + r_{m+2} + \dots + r_N
$$

as the return starting at time $m$ when state is $S_m$.

Monte-Carlo method can also be written as an update rule.

### Update rule

$$
V_n(s) = \frac{1}{n} \sum_{m = 1}^{n} G_m, \quad n \geq 1 \text{ when } S_0 = s
$$

Then,

$$
\begin{aligned}
V_{n+1}(s)
& =
\frac{1}{n+1} \sum_{m = 1}^{n+1} G_m 
=
\frac{1}{n+1} \left( \sum_{m = 1}^{n} G_m + G_{n+1} \right) 
\\ & =
\frac{n}{n+1} \left( \frac{1}{n} \sum_{m = 1}^{n} G_m \right) + \frac{1}{n+1} G_{n+1}
=
\frac{n}{n+1} V_{n}(s) + \frac{1}{n+1} G_{n+1}
\\
\text{or} \qquad
V_{n+1}(s)
& =
V_{n}(s) + \frac{1}{n+1} \Big( G_{n+1} - V_{n}(s) \Big)
\end{aligned}
$$

In general, one may let

$$
V_{n+1}(s) = V_{n}(s) + \alpha_n \Big( G_{n+1} - V_{n}(s) \Big)
$$

where $\alpha_n, n \geq 0$ are step sizes or learning rates, such that

$$
\sum_{n} \alpha_n = \infty, \qquad \sum_{n} \alpha_n^2 < \infty
$$

One can show that as $n \to \infty$,

$$
V_{n}(s) \to \operatorname{E}_{\mu} \Big[ G_{n+1} \Big| S_{n+1} = s \Big] = J^{\mu}(s)
$$

### Online version of this algorithm

$$
V_{n+1}(s_n) = V_{n}(s_n) + \alpha_n \Big( G_{n+1} - V_{n}(s_n) \Big)
$$

with $V_{n+1}(s) = V_{n}(s), \ \forall s \neq s_n$.

Another way to unify the above,

$$
V_{n+1}(s) = V_{n}(s) + \alpha_n \mathbb{I}_{\{ s = s_n \}} \Big( G_{n+1} - V_{n}(s_n) \Big)
$$

where

$$
\mathbb{I}_{\{ s = s_n \}} = \begin{cases}
1, & \text{if } s = s_n \\
0, & \text{otherwise}
\end{cases}
$$

Recall that

$$
\begin{aligned}
V_{n+1}(s_n)
& =
V_{n}(s_n) + \alpha_n \Big( G_{n+1} - V_{n}(s_n) \Big)
\\ & =
V_{n}(s_n) + \alpha_n \Big( R_{n+1} + R_{n+2} + \dots + R_{N} - V_{n}(s_n) \Big)
\\ & =
V_{n}(s_n) + \alpha_n \Big[ (R_{n+1} + V_{n}(s_{n+1}) - V_{n}(s_{n})) 
\\ & \qquad\qquad\qquad\ 
+ (R_{n+2} + V_{n}(s_{n+2}) - V_{n}(s_{n+1}))
\\ & \qquad\qquad\qquad\ 
+ (R_{N} + \underbrace{ V_{n}(s_{N}) }_{= \ 0} - V_{n}(s_{N-1})) \Big]
\end{aligned}
$$

Let

$$
\begin{aligned}
d_n
& =
R_{n+1} + V_{n}(s_{n+1}) - V_{n}(s_{n})
\\ 
d_{n+1}
& =
R_{n+2} + V_{n}(s_{n+2}) - V_{n}(s_{n+1})
\\
d_{N-1}
& =
R_{N} + V_{n}(s_{N}) - V_{n}(s_{N-1}))
\end{aligned}
$$

The quantites $d_n, \ d_{n+1}, \, \dots, \ d_{N-1}$ are reffered to as *temporal difference (TD)* terms. Then,

$$
V_{n+1}(s_n) = V_{n}(s_n) + \alpha_n \Big( d_{n+1} + d_{n+2} + \dots + d_{N-1} \Big)
$$

Another alternative,

$$
V_{n+l+1}(s_n) = V_{n+l}(s_n) + \alpha_n d_{n+l}, \qquad l = 0, 1, \dots, N-n
$$

**References:** Bertsekas and Tsitsiklis, Neuro Dynamic Programming

---

