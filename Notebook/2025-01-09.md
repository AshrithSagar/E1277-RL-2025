# RL | 2025-01-09

## Multi-armed bandits

If rewards are deterministic, then there is no problem to solve. One pass through all the arms and then pick the best one.

Rewards being random, is the challenging part.

### Strong law of large numbers

Given action $a$ is selected and the sequence of rewards obtained is i.i.d. with $\operatorname{E}[r_i^a] < \infty$ and $\operatorname{E} \left [{(r_i^a)}^2 \right] < \infty$, then

$$
\frac{1}{t} \sum_{i = 1}^{t} r_i^a \xrightarrow{\text{a.s.}} \operatorname{E} \big[ r_i^a \big] = q^*(a) = \operatorname{E} \big[ R_{t+1} \big| A_t = a \big]
$$

Sample mean converges to the true mean.

### Algorithm

$$
Q_{t+1}(a) = Q_{t}(a) + \frac{1}{t+1} \Big( R_{t+1} - Q_{t}(a) \Big)
$$

with $Q_0(a) = 0$

We call $\cfrac{1}{t+1} \triangleq \alpha_t$, as the *step size* or *learning rate*.

---

### Stochastic approximation algorithms (SA)

SA: H. Robbins and S. Munro, Annals of Statistics, 1951.

#### Root finding problem

Consider a function $f: \mathbb{R}^d \to \mathbb{R}^d$ for which we want to find $x \in \mathbb{R}^d$ such that $f(x) = 0$.

The problem is that $f$ is not known. We however have access to noisy function evaluations of the function $f$.

### Upper confidence bound (UCB) based action selection

$$
A_t = \arg \max_{a} \Bigg[ \ \underbrace{ Q_t(a) }_{\text{Expoitation term}} + c \underbrace{ \sqrt{\frac{\ln (t + \epsilon)}{N_t(a)}} }_{\text{Exporation term}} \ \Bigg]
$$

~$\epsilon$ optional

$N_t(a) \to$ Number of times arm $a$ is pulled until time $t$

$c > 0 \to$ Exploration parameter

### Gradient bandit algorithms

Let's say that $H_t(a)$ is a preference for action $a$ at time $t$

Gibb's or Boltzmann policy

$$
P(A_t = a) \triangleq \pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b = 1}^{k} e^{H_t(b)}}
$$

Gradient ascent, actually

$$
H_{t+1}(a) = H_t(a) + \alpha \frac{\partial \operatorname{E}[R_t]}{\partial H_t(a)}
$$

where

$$
\operatorname{E}[R_t] = \sum_{x} \pi_t(x) \ q^*(x), \qquad q^*(x) = \operatorname{E}[R_{t+1} \mid A_t = x]
$$

$$
\frac{\partial \operatorname{E}[R_t]}{\partial H_t} = \frac{\partial}{\partial H_t(a)} \left[ \sum_{x} \pi_t(x) \ q^*(x) \right] = \sum_{x} q^*(x) \left( \frac{\partial \pi_t(x)}{\partial H_t(a)} \right) = \sum_{x} \left( q^*(x) - B_t \right) \left( \frac{\partial \pi_t(x)}{\partial H_t(a)} \right)
$$

$$
\sum_{x} B_t \frac{\partial \pi_t(x)}{\partial H_t(a)} = B_t \frac{\partial}{\partial H_t(a)} \underbrace{ \sum_{x} \pi_t(x) }_{=1} = 0
$$

Judicious choice of $B_t$ can bring down the variance

Now, recall

$$
\pi_t(x) = \frac{e^{H_t(x)}}{\sum_{y} e^{H_t(y)}}
$$

$$
\implies \frac{\partial \pi_t(x)}{\partial H_t(a)} = \frac{\left( \sum_{y} e^{H_t(y)} \right) \frac{\partial}{\partial H_t(a)} e^{H_t(x)} - e^{H_t(x)} e^{H_t(a)} }{{\left( \sum_{y} e^{H_t(y)} \right)}^2}
$$

Then

$$
\implies \sim\pi_t(x) = \frac{\mathbb{I}_{\{ x = a \}} \ e^{H_t(a)}}{\sum_{y} e^{H_t(y)}} - \pi_t(x) \pi_t(a)
$$

$$
\because \frac{\partial}{\partial H_t(a)} \sum_{y} e^{H_t(y)} = e^{H_t(a)}
$$

Also

$$
\frac{\partial}{\partial H_t(a)} e^{H_t(x)} =
\begin{cases}
0, & \text{if } x \neq a \\
e^{H_t(a)}, & \text{if } x = a
\end{cases}
$$

$$
\frac{\partial \pi_t(x)}{\partial H_t(a)} = \pi_t(x) \Big( \mathbb{I}_{\{ x = a \}} - \pi_t(a) \Big)
$$

Then

$$
\frac{\partial \operatorname{E}[R_t]}{\partial H_t(a)} = \sum_{x} \Big( q^*(x) - \bar R_t(x) \Big) \pi_t(x) \Big( \mathbb{I}_{\{ x = a \}} - \pi_t(a) \Big)
$$

Then

$$
H_{t+1}(a) = H_t(a) + \alpha \sum_{x} \Big( q^*(x) - \bar R_t(x) \Big) \pi_t(x) \Big( \mathbb{I}_{\{ x = a \}} - \pi_t(a) \Big)
$$

$$
H_{t+1}(a) = H_t(a) + \alpha \Big[ (R_t - \bar R_t) \big( \mathbb{I}_{\{ A_t = a \}} - \pi_t(a) \big) \Big]
$$

References

- Finite horizon problems, Bertsekas, Volume-1, Chapter-1

---

