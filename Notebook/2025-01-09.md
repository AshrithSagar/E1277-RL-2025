# RL | 2025-01-09

## Multi-armed bandits

If rewards are deterministic, then there is no problem to solve. One pass through all the arms and then pick the best one.

Rewards being random, is the challenging part.

### Strong law of large numbers

Given action $a$ is selected and the sequence of rewards obtained is i.i.d. with $\operatorname{E}[r_i^a] < \infty$ and $\operatorname{E} \left [{(r_i^a)}^2 \right] < \infty$, then
$$
\frac{1}{t} \sum_{i = 1}^{t} r_i^a \xrightarrow{\text{a.s.}} \operatorname{E} \big[ r_i^a \big] = q^*(a) = \operatorname{E} \big[ R_{t+1} \big| A_t = a \big]
$$
Sample mean converges to the true mean.

### Algorithm

$$
Q_{t+1}(a) = Q_{t}(a) + \frac{1}{t+1} \Big( R_{t+1} - Q_{t}(a) \Big)
$$

with $Q_0(a) = 0$

We call $\cfrac{1}{t+1} \triangleq \alpha_t$, as the *step size* or *learning rate*.

---

### Stochastic approximation algorithms (SA)

SA: H. Robbins and S. Munro, Annals of Statistics, 1951.

#### Root finding problem

Consider a function $f: \mathbb{R}^d \to \mathbb{R}^d$ for which we want to find $x \in \mathbb{R}^d$ such that $f(x) = 0$.

The problem is that $f$ is not known. We however have access to noisy function evaluations of the function $f$.

### Upper confidence bound (UCB) based action selection

$$
A_t = \arg \max_{a} \Bigg[ \ \underbrace{ Q_t(a) }_{\text{Expoitation term}} + c \underbrace{ \sqrt{\frac{\ln (t + \epsilon)}{N_t(a)}} }_{\text{Exporation term}} \ \Bigg]
$$

~$\epsilon$ optional

$N_t(a) \to$ Number of times arm $a$ is pulled until time $t$

$c > 0 \to$ Exploration parameter

### Gradient bandit algorithms

Gibb's or Boltzmann policy
$$
P(A_t = a) \triangleq \pi_t(a) = \frac{e^{H_t(a)}}{\sum_{b = 1}^{k} e^{H_t(b)}}
$$
Gradient ascent, actualy

---

