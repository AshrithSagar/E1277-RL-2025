# RL | 2025-03-06

## Reinforcement learning

Field of RL yet to be explored

**Tabular RL**

- Known $\forall (s, a) \in S \times A$
- $\operatorname{MDP}(S, A, P, r, \gamma)$
- When this is very large, we have RL with approximation $\to$ Field of **Deep RL**
- An RL algorithm with guarenteed results is non-existent

**Model-based learning**

- $P, r$ are known
- Finding $P(s, a, s') \in \mathbb{R}^{{\vert S \vert}^2 \vert A \vert}$ is difficult

**Model-free learning**

- $P, r$ are not known
- $Q_* \in \mathbb{R}^{\vert S \vert \vert A \vert}$

RL ~frameworks

- DQN-2013: By Deepmind, Deep $Q$-Neural networks
- TRPO-2014
  - Advantage function $A_\pi(s, a) = Q_\pi(s, a) - V_\pi(s, a)$
  - Assumed that it is initially known for policy $\pi$ that we start with
- PPO

Need to check if the RL algorithm is good enough; Is the policy $\pi$ good enough? How do we do this?

- Deep RL by Peter (YouTube)

  - Foundations of Deep RL -- 6-lecture series by Pieter Abbeel

    <https://www.youtube.com/playlist?list=PLwRJQ4m4UJjNymuBM9RdmB3Z9N5-0IlY0>

  - Pieter Abbeel: Deep Reinforcement Learning | Lex Fridman Podcast #10

    <https://www.youtube.com/watch?v=l-mYLq6eZPY&ab_channel=LexFridman>

Syllabus: Deep RL with function approximation

## Stochastic approximation

Is a superset $(\supseteq)$ of Stochastic gradient descent (SGD)

Policy evaluation

- Given $\pi \to$ What is $V_\pi, Q_\pi, A_\pi$?
- $V_\pi \to$ Value function
- $Q_\pi \to$ State-action value function
- $A_\pi \to$ Advantage function

In the context of function approximation (TD learning)

Any algorithm of the form

$$
x_{n+1} = \underbrace{ x_n }_{\text{Current estimate}} + \underbrace{ \alpha_n }_{\text{Step size}} \overbrace{ \Big[ \underbrace{ h(x_n) }_{\text{True driving function}} + M_{n+1} \Big] }^{\text{Noisy estimate of the true driving function}}
$$

$$
\begin{aligned}
x_{n+1} - x_{n0} &
= \sum_{j = n_0}^{n} (x_{j+1} - x_j)
= \sum_{j = n_0}^{n} \alpha_j \big[ h(x_j) + M_{j+1} \big]
\\ &
= \sum_{j = n_0}^{n} \alpha_j \ h(x_j) + \sum_{j = n_0}^{n} \alpha_j \ h(x_j) \ M_{j+1}
\end{aligned}
$$

- $\sum_{j = n_0}^{n} \alpha_j \ h(x_j) \longrightarrow$ True driving direction; We chose this;

- $\sum_{j = n_0}^{n} \alpha_j \ h(x_j) \ M_{j+1} \longrightarrow$ Will be very large if it represents some other direction

### Theorem: Benaim'gg, Chapter-2 of BarKae

BarKae, Stochastic approximation theory: A dynamical system perspective

Suppose

**(A1).** $h: \mathbb{R}^d \to \mathbb{R}^d$ is Lipschitz continuous, i.e., $\exist L \geq 0$ such that

$$
\Vert h(x) - h(y) \Vert \leq \Vert x - y \Vert, \quad \forall \ x, y \in \mathbb{R}^d
$$

**(A2).**

$$
\sum_{n = 0}^{\infty} \alpha_n = +\infty, \qquad \sum_{n = 0}^{\infty} \alpha_n^2 < \infty
$$

- $\sum_{n = 0}^{\infty} \alpha_n = +\infty \to$ $\alpha$ becomes $0$ slowly. Eg:

  $$
  \sum_{n = 0}^{\infty} \frac{1}{n+1} = +\infty
  $$

- $\sum_{n = 0}^{\infty} \alpha_n^2 < \infty \to$ $\alpha$ becomes $0$ eventually. Eg:

  $$
  \alpha_n = \frac{1}{{(n+1)}^\alpha}, \quad \frac{1}{2} < \alpha \leq 1
  $$

**(A3).** The sequence $\{ M_n \}_{n \geq 0}$ needs to be a square-integrable "martingale difference sequence", i.e.,

$$
\exist \ \mathcal{F}_n \text{ of } \sigma\text{-fields}, \quad \mathcal{F}_n \subseteq \mathcal{F}_{n+1}, \quad \mathcal{F}_n = \sigma(x_0, M_1, \dots, M_n) \quad \text{such that } \operatorname{E}[M_{n+1} \mid \mathcal{F}_n] = 0, \ \forall \ n
$$

- Square-integrable: $\operatorname{E}{\Vert M_n \Vert}^2 < +\infty, \ \forall \ n \geq 1$

- Information available at time $n$: $\mathcal{F}_n = \sigma(x_0, M_1, \dots, M_n)$

- $\sigma$-fields: A collection of events

- A sequence $Y_n$ of i.i.d. zero mean random variables

  $$
  \mathcal{F}_n = \sigma(Y_0, \dots, Y_n), \qquad \operatorname{E}[Y_{n+1} \mid Y_n] = \operatorname{E}[Y_{n+1}] = 0
  $$

**(A4).** Let $h_c(x) = \cfrac{h(cx)}{c}$ for $c \geq 1$. Suppose there exists a constant function $h_\infty: \mathbb{R}^d \to \mathbb{R}^d$ such that

$$
h_c(x) \to h_\infty(x) \text{ as } c \to \infty \text{ uniformly on compact set}
$$

Furthermore, the (deterministic) ODE $\dot x(t) = h_\infty(x(t))$ has the origin as its globally asymptotically stable equilibrium point.

Then, $x_n$ generated by (stochastic) $x_{n+1} = x_n + \alpha_n \big[ h(x_n) + M_{n+1} \big]$ asymptotically converges to a compact connected invariant set of the ODE $\dot x(t) = h(x(t))$.

$$
\text{Eg:} \quad A \subseteq \mathbb{R}^d, \quad x_0 \in A, \quad \cfrac{d}{dt} x(t, 0, x_0) = h \big( x(t, 0, x_0) \big)
$$

Positive and negative invariant set

---

