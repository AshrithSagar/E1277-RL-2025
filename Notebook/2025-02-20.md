# RL | 2025-02-20

## Review

- Model-free methods
  - Do not require knowledge of the model
- Monte-Carlo schemes
  - Broadly trajectory-based mthods
- Data
  - In the form of tuples
  - Until termination
  - Accumulate return on an episode
  - First visit method
  - Every visit method

## Temporal difference learning

An famously popular technique. TD learning has started out with Richard Sutton's PhD thesis, 1984.

Recall Monte-Carlo scheme tries to solve for

$$
v_\pi(s) = \operatorname{E}_\pi [G_n \mid S_n = s]
$$

$$
G_n = R_{n+1} + R_{n+2} + \dots + R_N
$$

where $N$ is the terminal instant od episode.

Monte-Carlo scheme works with sample average data collected over trajectories.

~ As $N \to \infty$, convergence. A very basic approach, still need to wait for the trajectories to span out.

Suppose the return on $i$-th trajectory,

$$
G^{(i)}_n = R^{(i)}_{n+1} + R^{(i)}_{n+2} + \dots + R^{(i)}_{N}
$$

Suppose we collect $M$ trajectories,

$$
v_\pi(s) \approx \frac{1}{M} \sum_{i = 1}^{M} G^{(i)}_n
$$

~ Can think of $N$ as a random variable. Collecting all this returns is a tedious process.

An alterative way of updating in the Monte-Carlo scheme

$$
V_{m+1}(s) = V_{m}(s) + \alpha(s) \mathbb{I}_{\{ s = s_n \}} (G^m_n - V_m(s_n))
$$

These are what are reffered to as *Stochastic approximation algorithms*. Involve an incremental update.

Need to wait for the entire sequence of rewards until termination. Need to wait for the entire trajectory to play out, to update once.

~ Doesn't matter where it starts from, what matters is if starting from a particular state.

**Key idea in Temporal difference learning, (~TD$\lambda$)**

Instead of looking at $v_\pi(s) = \operatorname{E}_\pi [G_n \mid S_n = s]$, we look at the Bellman equation.
$$
\begin{aligned}
&
v_\pi(s) = \operatorname{E}_\pi [R_{n+1} + v_\pi(s_{n+1}) \mid S_n = s]
\\
\text{or} \quad &
\operatorname{E}_\pi [R_{n+1} + v_\pi(s_{n+1}) - v_\pi(s_n) \mid S_n = s] = 0
\end{aligned}
$$

### TD recursion

$$
V_{n+1}(s_n) = V_{n}(s_n) + \alpha_n (R_{n+1} + V_n(s_{n+1}) - V_n(s_n))
$$

$$
\text{for } n \geq 0 \text{ with } V_{n+1}(s) = V_n(s), \quad \forall s \neq s_n
$$

Alternatively,
$$
V_{n+1}(s) = V_{n}(s) + \alpha \mathbb{I}_{\{ s = s_n \}} (R_{n+1} + V_n(s_{n+1}) - V_n(s_n))
$$

$$
\text{where }
\mathbb{I}_{\{ s = s_n \}} =
\begin{cases}
1, & \text{if } s = s_n \\
0, & \text{otherwise}
\end{cases}
$$

$m$ is the iteration index.

Suppose the Markov chain $\{ S_n \}$ under policy $\pi$ is ergodic, i.e., irreducible, aperiodic, and positive recurrent.

Then, starting from an initial distribution, $\{ S_n \}$ will settle into a steady state or stationary distribution that will be unique $V$.

Corresponding ODE: $\dot V(s)$

Form a sequence of time points $\{ t(n) \}$ as follows

$$
t(0) = 0, \quad t(1) = \alpha_1, \quad t(2) = \alpha_0 + \alpha_1, \ \dots
$$

Conditions on $\{ \alpha_n \}$:

$$
\begin{aligned}
& \alpha_n > 0 \ \forall n \\
& \sum_{n} \alpha_n = \infty \implies t(n) \to \infty \text{ as } n \to \infty \\
& \sum_{n} \alpha_n^2 < \infty \\
\implies & \alpha_n \to \infty \text{ as } n \to \infty
\end{aligned}
$$

One can show that

$$
\lim_{n \to \infty} \sup_{t \in [T_n, T_{n+1}]} \Vert \bar V(t) - V^{T_n}(t) \Vert = 0, \text{ w.p. } 1
$$

in the asymptotic case.

Here, $\bar V(t), t \geq 0$ is the algorithm's trajectory (continuously interpolated).

$V^{T_n}(t), t \in [T_n, T_{n+1}]$ is the ODE trajectory, where $V^{T_n}(T_n) = \bar V(T_n)$.

Suppose the ODE has $v^\ast$ as a globally asymptotic stable equilibrium. Then the algorithm will satisfy $V_n \to v^\ast$ a.s. as $n \to \infty$ (under same conditions).

ODE: $\dot V(t) = D \bar V$, where

$$
D = \begin{bmatrix}
v(1) & 0 & \cdots & 0 \\
0 & v(2) & \cdots & 0 \\
\vdots & \vdots & \ddots & \vdots \\
0 & 0 & \cdots & v(n)
\end{bmatrix}_{n \times n}
$$

where

$$
V(t) = \begin{bmatrix}
\sum_{j = 1}^{n} p_{1j}(\pi(1)) \ ( R_\pi(1, j) + V_\pi(j) - V_\pi(1)) \\
\sum_{j = 1}^{n} p_{2j}(\pi(2)) \ ( R_\pi(2, j) + V_\pi(j) - V_\pi(2)) \\
\vdots \\
\sum_{j = 1}^{n} p_{nj}(\pi(n)) \ ( R_\pi(n, j) + V_\pi(j) - V_\pi(n))
\end{bmatrix}
$$

$V(t) = 0$ will satisfy the Bellman equation.

Then, it can be shown that

$$
V_{n} \to V_\pi,
\quad \text{where }
V_\pi = \begin{bmatrix}
V_\pi(1) \\ V_\pi(2) \\ \vdots \\ V_\pi(n)
\end{bmatrix}
$$

~ $D$ comes in becuase of asynchronous updates. Only those components will be updated ~which are visited.

---

A good reference for ODE approach to stochastic approximation:

- V. Borkar, Stochastic approximation: A dynamical systems viewpoint, 2022
  - Hindustan book agency, 2022
  - Springer, 2023
- Look at Chapters 1 and 2 of this book

As an aside, usual SA algorithm
$$
\begin{aligned}
\text{Algorithm:} \qquad
x_{n+1} & = x_n + a(n) \Big( h(x_n) + M_{n+1} \Big) \\
\text{ODE:} \qquad
\dot x(t) & = h(x(t))
\end{aligned}
$$
~ Proving convergence, and bounds

Here, *point-to-point* maps: given $x_n$, give back $h(x_n)$. Alternately, thinking of $h(x_n)$ as a *point-to-set* map instead.

Actor-critic algorithms in reinforcement learning.

---

Since we have an episodic task,

~ This is the manner in which the episodic task is evolving. Reinitialise, and start again from a new state. The whole thing becomes like an ergodic Markov chain.

### TD($\lambda$) algorithm

Consider the $(l+1)$ step Bellman equation
$$
v_\pi(i_k) = \operatorname{E}_\pi \left[ \sum_{m = 0}^{l} r(i_{k+m}, i_{k+m+1}) + v_\pi(i_{k+l+1}) \right]
$$
~ The usual Bellman equation is one-step.

Since value of $l$ is arbitrary, we can form a weighted average of all such Bellman equations, in the following manner.

Let $0 \leq \lambda < 1$.

Since
$$
\sum_{l = 0}^{\infty} (1 - \lambda) \lambda^l = 1
$$

$$
\begin{aligned}
v_\pi(i_k)
& =
(1 - \lambda) \operatorname{E}_\pi \left[ \sum_{l = 0}^{\infty} \lambda^l \left( \sum_{m = 0}^{l} r(i_{k+m}, i_{k+m+1}) + v_\pi(i_{k+l+1}) \right) \right]
\\ & =
(1 - \lambda) \operatorname{E}_\pi \left[ \sum_{l = 0}^{\infty} \lambda^l \sum_{m = 0}^{l} r(i_{k+m}, i_{k+m+1}) \right]
+ (1 - \lambda) \operatorname{E}_\pi \left[ \sum_{l = 0}^{\infty} \lambda^l \ v_\pi(i_{k+l+1}) \right]
\\ & =
(1 - \lambda) \operatorname{E}_\pi \left[ \sum_{m = 0}^{\infty} \sum_{l = m}^{\infty} \lambda^l \ r(i_{k+m}, i_{k+m+1}) \right]
+ \operatorname{E}_\pi \left[ \sum_{l = 0}^{\infty} (\lambda^l - \lambda^{l+1}) \ v_\pi(i_{k+l+1}) \right]
\end{aligned}
$$

$$
\begin{aligned}
(1 - \lambda) \operatorname{E}_\pi \left[ \sum_{m = 0}^{\infty} \sum_{l = m}^{\infty} \lambda^l \ r(i_{k+m}, i_{k+m+1}) \right]
& =
(1 - \lambda) \operatorname{E}_\pi \left[ \sum_{m = 0}^{\infty} r(i_{k+m}, i_{k+m+1}) \sum_{l = m}^{\infty} \lambda^l \right]
\\ & =
(1 - \lambda) \operatorname{E}_\pi \left[ \sum_{m = 0}^{\infty} r(i_{k+m}, i_{k+m+1}) \frac{\lambda^m}{(1 - \lambda)} \right]
\end{aligned}
$$

$$
\begin{aligned}
\operatorname{E}_\pi \left[ \sum_{l = 0}^{\infty} (\lambda^l - \lambda^{l+1}) \ v_\pi(i_{k+l+1}) \right]
& =
\end{aligned}
$$

Stochastic approximation version
$$
v(i_k) := v(i_k) + \alpha \sum_{m = k}^{\infty} \lambda^{m - k} \bar d_m
$$
where
$$
\bar d_m = r()
$$


As number of iterates $\to \infty$,
$$
v(i_k) \to v()
$$


#### Case-1: $\lambda = 0, \ \operatorname{TD}(0)$ algorithm

#### Case-2: $\lambda = 1, \ \operatorname{TD}(1)$ algorithm

$$
v(i_k) := v(i_k) + \alpha \sum_{m} \bar d_m
$$

We have seen earlier that the $\text{sum of TD terms} = \text{sum of rewards until termination}$.

---

