# RL | 2025-02-13

## Corollary: BE for a given policy $\mu$

For every stationary policy $\mu$, the associated cost function satisfies,

$$
J_{\mu}(i) = \sum_{j \in S} p_{ij}(\mu(i)) \Big( g(i, \mu(i), j) + \alpha J(j) \Big), \quad \forall \ i \in S
$$

Moreover, $J_\mu$ is the unique solution to the equation within the class of bounded functions.

## Necessary and sufficient condition for optimality

### Proposition

A stationary policy $\mu$ is optimal if and only if $\mu(i)$ attains the minimum in the Bellman equation $\forall \ i \in S$, i.e.,

$$
T J^* = T_\mu J^*
$$

#### Proof

Suppose $T J^* = T_\mu J^*$. Then

$$
\begin{aligned}
J^* & = T J^* = T_\mu J^*
,\quad \text{(by BE)}
\\
\implies
J^* & = T_\mu J^*,
\quad \text{or} \quad
J^* = J^\mu
\end{aligned}
$$

Conversely, suppose $\mu$ is optimal. Then $J^* = J^\mu$. Then

$$
\begin{aligned}
J^* = T_\mu J^* & = T J^*
\\
T J^* & = T_\mu J^*
\end{aligned}
$$

---

Define now max norm ${\Vert \cdot \Vert}_{\infty}$ on $\mathbb{R}^n$ by ${\Vert J \Vert}_{\infty} = \max_{i \in S} \vert J(i) \vert$

### Proposition

For any two bounded functions $J: S \to \mathbb{R}$ and $J': S \to \mathbb{R}$ and for all $k = 0, 1, 2, \dots$

**(a).**

$$
{\Vert T^k J - T^k J' \Vert}_{\infty} \leq \alpha^k {\Vert J - J' \Vert}_{\infty}
$$

**(b).**

$$
{\Vert T_\mu^k J - T_\mu^k J' \Vert}_{\infty} \leq \alpha^k {\Vert J - J' \Vert}_{\infty}
$$

#### Proof

**(a).** Let

$$
c = {\Vert J - J' \Vert}_{\infty} = \max_{i \in S} \vert J(i) - J'(i) \vert
$$

$$
\implies J(i) - c \leq J'(i) \leq J(i) + c, \quad \forall \ i \in S
$$

$$
\implies (T^k J)(i) - \alpha^k c \leq (T^k J')(i) \leq \implies (T^k J)(i) + \alpha^k c
$$

$$
\therefore \quad \left\vert (T^k J)(i) - (T^k J')(i) \right\vert \leq \alpha^k c, \quad \forall \ i \in S
$$

$$
\implies \max_{i \in S} \left\vert (T^k J)(i) - (T^k J')(i) \right\vert \leq \alpha^k {\Vert J - J' \Vert}_{\infty}
$$

$$
\implies {\Vert T^k J - T^k J' \Vert}_{\infty} \leq \alpha^k {\Vert J - J' \Vert}_{\infty}
$$

**(b).** Follows similarly

### Corollary: Rate of convergence of VI

For any bounded function $J: S \to \mathbb{R}$, we have

1.

$$
\max_{i \in S} \left\vert (T^k J)(i) - J^*(i) \right\vert \leq \alpha^k \max_{i \in S} \vert J(i) - J^*(i) \vert
$$

2.

$$
\max_{i \in S} \left\vert (T_\mu^k J)(i) - J_\mu(i) \right\vert \leq \alpha^k \max_{i \in S} \vert J(i) - J_\mu(i) \vert, \quad \forall \ i \in S, \quad k = 0, 1, 2, \dots
$$

## Example: Machine replacement

A machine can be in one of $n$ states $1, 2, \dots, n$ where $1$ is the best state and $n$ is the worst state.

Suppose transition probabilities $p_{ij}$ are known.

Cost of operating machine for one period $= g(i)$ when state of machine is $i$.

Actions:

- Select "$\mathcal{O}$": Operate machine
- Select "$\mathcal{C}$": (Change)/Replace by a new machine

~Let us assume that the cost incurred when $\mathcal{C}$ is chosen is $R$.

Once replaced the new machine is guarenteed to stay in state $1$ for one period.

~In subsequent periods it can detiorate, as previously.

Suppose $\alpha \in (0, 1)$ is a given discount function.

What is the optimal policy? When to operate and when to replace?

**Bellman equation**

$$
J^*(i) = \min \left\{ R + g(1) + \alpha J^*(i), \quad g(i) + \alpha \sum_{j=1}^{n} p_{ij} \, J^*(i) \right\}
$$

Optimal policy is to use action $\mathcal{C}$ if

$$
R + g(1) + \alpha J^*(i) < g(i) + \alpha \sum_{j=1}^{n} p_{ij} \, J^*(i)
$$

else selection action $\mathcal{O}$.

**Note:** Assume

1.

$$
g(1) \leq g(2) \leq \dots \leq g(n)
$$

- **Exercise:** (Can come in the midterm also :))

  $$
  \implies J^*(1) \leq J^*(2) \leq \dots \leq J^*(n)
  $$
  
  - State $1$ is best, lowest ~cost-to-go (?)

2.

$$
p_{ij} = 0, \quad \text{ if } j < i
$$

3.

$$
\sum_{j} p_{ij} \, J^*(j) \leq \sum_{j} p_{(i+1)j} \, J^*(j)
$$

[~From $p_{ij} \leq p_{(i+1)j}, \quad \text{if } i < j$ (?)]

$$
\implies g(i) + \alpha \sum_{j=1}^{n} p_{ij} \, J^*(i) \leq g(k) + \alpha \sum_{j=1}^{n} p_{kj} \, J^*(i), \quad i < k
$$

Let

$$
S_R = \left\{ i \in S \Bigg| R + g(1) + \alpha J^*(1) \leq g(i) + \alpha \sum_{j=1}^{n} p_{ij} \, J^*(j) \right\}
$$

Let

$$
i^* =
\begin{cases}
\text{smallest state in } S_R, & \text{if } S_R \text{ is non empty} \\
(n+1), & \text{if } S_R \text{ is empty}
\end{cases}
$$

**Optimal policy**: Replace machine if and only if $i \geq i^*$.

---

Recall that

$$
\begin{aligned}
(T_\mu J)(i)
& =
\sum_{j=1}^{n} p_{ij}(\mu(i)) \Big( g(i, \mu(i), j) + \alpha J(j) \Big), \quad i \in S
\\ & =
\sum_{j=1}^{n} p_{ij}(\mu(i)) \ g(i, \mu(i), j) + \alpha \sum_{j=1}^{n} p_{ij}(\mu(i)) \ J(j), \quad i \in S
\\ & =
\bar g(i, \mu(i), j) + \alpha \sum_{j=1}^{n} p_{ij}(\mu(i)) \ J(j), \quad i \in S
\end{aligned}
$$

Let

$$
\bar g_\mu = \begin{bmatrix} \bar g(1, \mu(1)) \\ \bar g(2, \mu(2)) \\ \vdots \\ \bar g(n, \mu(n)) \end{bmatrix}
$$

Let

$$
P_\mu =
\begin{bmatrix}
p_{11}(\mu(1)) & p_{12}(\mu(1)) & \dots & p_{1n}(\mu(1)) \\
p_{21}(\mu(2)) & p_{22}(\mu(2)) & \dots & p_{2n}(\mu(2)) \\
\vdots & \vdots & \ddots & \vdots \\
p_{n1}(\mu(n)) & p_{n2}(\mu(n)) & \dots & p_{nn}(\mu(n)) \\
\end{bmatrix}
$$

Then

$$
T_\mu J = \bar g_\mu + \alpha P_\mu J
$$

$J^\mu$ is the unique fixed point of this equation. Thus

$$
J^\mu = \bar g_\mu + \alpha P_\mu J
$$

$$
\implies (I - \alpha P_\mu) J^\mu = \bar g_\mu
$$

$$
\text{or } \quad J^\mu = (I - \alpha P_\mu)^{-1} \bar g_\mu
$$

This matrix is always invertible (?)

~For large state spaces, value iteration is better than inverting this matrix.

## Value iteration and error bounds

We have shown that starting from any $J \in \mathbb{R}^n$,

$$
\lim_{k \to \infty} (T^k J)(i) = J^*(i), \quad i \in S
$$

Also,

$$
\left\vert (T^k J)(i) - J^*(i) \right\vert \leq \alpha^k \vert J(i) - J^*(i) \vert, \quad \forall \ i \in S
$$

Recall

$$
\begin{aligned}
J^\mu(i)
& =
\mathbb{E} \left[ \sum_{k = 0}^{\infty} \alpha^k g(i_k, \mu(i_k), i_{k+1}) \Bigg| i_0 = i \right]
\\ & =
\bar g(i, \mu(i)) + \sum_{k = 1}^{\infty} \alpha^k \mathbb{E} \Big[ g(i_k, \mu(i_k), i_{k+1}) \Bigg| i_0 = i \Big]
\end{aligned}
$$

Letting

$$
\begin{aligned}
\underline\beta & = \min_{i} \bar g(i, \mu(i)) \\
\bar\beta & = \max_{i} \bar g(i, \mu(i))
\end{aligned}
$$

$$
\implies \underline\beta \leq \bar g(i, \mu(i)) \leq \bar\beta, \quad \forall \ i
$$

In vector notation,

$$
\bar g_\mu + \left( \frac{\alpha \underline\beta}{1 - \alpha} \right) e \leq J_\mu \leq \bar g_\mu + \left( \frac{\alpha \bar\beta}{1 - \alpha} \right) e
$$

Since $\underline\beta \leq \bar g \leq \bar\beta$, we have

$$
\left( \frac{\underline\beta}{1 - \alpha} \right) e \leq \bar g_\mu + \left( \frac{\alpha \underline\beta}{1 - \alpha} \right) e \leq J_\mu \leq \bar g_\mu + \left( \frac{\alpha \bar\beta}{1 - \alpha} \right) e \leq \left( \frac{\bar\beta}{1 - \alpha} \right) e
$$

Given a vector $J$, we know that

$$
T_\mu J = \bar g_\mu + \alpha P_\mu J
$$

Subtracting the above from

$$
J^\mu = \bar g_\mu + \alpha P_\mu J^\mu
$$

$$
\begin{aligned}
\implies
J^\mu - T^\mu J
& =
\alpha P_\mu (J^\mu - J)
\\
\text{or } \quad
(J^\mu - J)
& =
(T^\mu J - J) + \alpha P_\mu (J^\mu - J)
\end{aligned}
$$

~Seems similar to ~$J = T J$

Thus, if cost-per-stage vector $= T J - J$, then $J^\mu - J$ is the cost-to-go vector. Then,

$$
\left( \frac{\underline\gamma}{1 - \alpha} \right) e \leq T_\mu J - J + \left( \frac{\alpha \underline\gamma}{1 - \alpha} \right) e \leq J^\mu - J \leq T_\mu J - J + \left( \frac{\alpha \bar\gamma}{1 - \alpha} \right) e \leq \left( \frac{\bar\gamma}{1 - \alpha} \right) e
$$

where

$$
\begin{aligned}
\underline\gamma & = \min_{i} \Big[ (T_\mu J)(i) - J(i) \Big] \\
\bar\gamma & = \max_{i} \Big[ (T_\mu J)(i) - J(i) \Big]
\end{aligned}
$$

Adding $J$ on all sides, we get

$$
J + \left( \frac{\underline\gamma}{1 - \alpha} \right) e \leq T_\mu J + \left( \frac{\alpha \underline\gamma}{1 - \alpha} \right) e \leq J^\mu \leq T_\mu J + \left( \frac{\alpha \bar\gamma}{1 - \alpha} \right) e \leq J + \left( \frac{\bar\gamma}{1 - \alpha} \right) e
$$

or

$$
J + \frac{\underline{c}}{\alpha} e \leq T_\mu J + \underline{c} e \leq J^\mu \leq T_\mu J + \bar{c} e \leq J + \bar{c} e
$$

where

$$
\begin{aligned}
\underline{c} & = \left( \frac{\alpha \underline\gamma}{1 - \alpha} \right) \\
\bar{c} & = \left( \frac{\alpha \bar\gamma}{1 - \alpha} \right)
\end{aligned}
$$

### Proposition

For every function $J: S \to \mathbb{R}$, state $i$ and $k \geq 0$,

$$
(T^k J)(i) + \underline{c}_k \leq (T^{k+1} J)(i) + \underline{c}_{k+1} \leq J^*(i) \leq (T^{k+1} J)(i) + \bar{c}_{k+1} \leq (T^k J)(i) + \bar{c}_k
$$

where

$$
\begin{aligned}
\underline{c}_k & = \left( \frac{\alpha}{1 - \alpha} \right) \min_{i = 1, 2, \dots, n} \Big[ (T^k J)(i) - (T^{k-1} J)(i) \Big] \\
\bar{c}_k & = \left( \frac{\alpha}{1 - \alpha} \right) \max_{i = 1, 2, \dots, n} \Big[ (T^k J)(i) - (T^{k-1} J)(i) \Big]
\end{aligned}
$$

---

