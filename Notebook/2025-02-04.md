# RL | 2025-02-04

## Review

1. Operator $T: \mathbb{R}^{\vert S \vert} \to \mathbb{R}^{\vert S \vert}$

$$
(TJ)(i) = \min_{u \in A(i)} \sum_{j \in S} p_{ij}(u) \Big( g(i, u, j) + J(j) \Big), \quad \forall i \in S
$$

2. Operator $T_\mu: \mathbb{R}^{\vert S \vert} \to \mathbb{R}^{\vert S \vert}$

$$
(T_\mu J)(i) = \sum_{j \in S} p_{ij}(\mu(i)) \Big( g(i, \mu(i), j) + J(j) \Big), \quad \forall i \in S
$$

We shall show that $T$ and $T_\mu$ are contraction maps in a certain norm $\Vert \cdot \Vert_{\xi}$

We show that $\exists \ \beta \in (0, 1)$ s.t. $\Vert T J - T \bar J \Vert_{\xi} \leq \beta \Vert J - \bar J \Vert_{\xi} \ \forall \ J, \bar J \in \mathbb{R}^{\vert S \vert}$

Recall that $S = \{ 1, 2, \dots, n \}$ is the set of non-terminal states

$0$ is the terminal state

$S^+ = S \cup \{ 0 \}$ is the set of all states

## Brower's fixed point theorem

**Theorem:** Suppose $S$ is a complete separable space w.r.t. metric $\rho$. Suppose $T$ is a contraction w.r.t. $\rho \implies$ There exists a fixed point $x^*$, i.e., $x^* = T x^*$. Moreover, $x^*$ is unique.

- Consider an interval $(0, 1]$ and consider the sequence $x_n = \frac{1}{n}, \quad \rho = \vert \cdot \vert \implies x_n \to 0$

- ~ Cauchy sequences are complete.

We will show that there is vector $\xi = \Big( \xi(1), \ \xi(2), \dots, \ \xi(n) \Big)$ s.t. $\xi(i) > 0 \ \forall \ i$ and a scalar $0 \leq \beta < 1$ s.t.

$$
\Vert TJ - T \bar J \Vert_{\xi} \leq \beta \Vert J - \bar J \Vert_{\xi}, \quad \forall \ J, \bar J \in \mathbb{R}^n
$$

where

$$
{\Vert J \Vert}_{\xi} \triangleq \max_{i = 1, 2, \dots, n} \frac{\vert J(i) \vert}{\xi(i)}
$$

## Proposition-3

Assume all stationary policies are proper. Then there exists a vector $\xi = \Big( \xi(1), \ \xi(2), \dots, \ \xi(n) \Big)$ with $\xi(i) > 0 \ \forall \ i = 1, 2, \dots, n$ s.t. the mappings $T$ and $T_\mu \ \forall$ stationary $\mu$ are contractions w.r.t.$\Vert \cdot \Vert_{\xi}$. In particular, $\exist \beta \in (0, 1)$ s.t.

$$
\sum_{j = 1}^{n} p_{ij}(u) \xi(j) \leq \beta \ \xi(i) \quad \forall i, \ u \in A(i)
$$

### Proof

Consider a new SSPP where transition probabilities are same as before but transition costs are all equal to $-1$, except the transition state where $g(0, u, 0) = 0, \forall \ u \in A(0)$

Let $\widehat J (i) \to$ Optimal cost to go from state $i$ in the new problem

$$
\begin{aligned}
\widehat J (i)
& =
-1 + \min_{u \in A(i)} \sum_{j \in S} p_{ij}(u) \ \widehat J (j), \quad \forall \ i \in S
\\ & \leq
-1 + \sum_{j \in S} p_{ij}(\mu(j)) \ \widehat J (j), \quad \forall \ i \in S, \quad \text{for any given } \mu
\end{aligned}
$$

Let $\xi(i) = - \widehat J (i)$. Then $\xi(i) \geq 1 \ \forall \ i$

$$
\begin{aligned}
\implies
- \widehat J (i)
& \geq
1 + \sum_{j \in S} p_{ij}(\mu(j)) \ \Big( - \widehat J (j) \Big), \quad \forall \ i \in S
\\
\xi(i)
& \geq
1 + \sum_{j \in S} p_{ij}(\mu(j)) \ \xi(j), \quad \forall \ i \in S
\end{aligned}
$$

$$
\sum_{j \in S} p_{ij}(\mu(i)) \ \xi(j) \leq \xi(i) - 1 \leq \beta \xi(i), \quad \text{where } \beta = \max_{i = 1, 2, \dots, n} \left( \frac{\xi(i) - 1}{\xi(i)} \right) < 1
$$

Now for a stationary policy $\mu$, state $i$ and vectors $J, \bar J \in \mathbb{R}^n$,

$$
\begin{aligned}
\left\vert (T_\mu J)(i) - (T_\mu \bar J)(i) \right\vert
& =
\left\vert \sum_{j = 1}^{n} p_{ij}(\mu(i)) \ \Big( J(j) - \bar J(j) \Big) \right\vert
\\ & \leq
\left( \sum_{j = 1}^{n} p_{ij}(\mu(i)) \ \xi(j) \right) \left( \max_{j = 1, 2, \dots, n} \frac{\vert J(j) - \bar J(j) \vert}{\xi(j)} \right)
\\ & \leq
\beta \ \xi(i) \ {\Vert J - \bar J \Vert}_{\xi}
\end{aligned}
$$

Recall that

$$
{\Vert J - \bar J \Vert}_{\xi} = \max_{i = 1, 2, \dots, n} \frac{\vert J(i) - \bar J(i) \vert}{\xi(i)}
$$

$$
\implies \max_{i = 1, 2, \dots, n} \frac{\vert T_\mu J(i) - T_\mu \bar J(i) \vert}{\xi(j)} \leq \beta {\Vert J - \bar J \Vert}_{\xi}
$$

$$
\implies {\Vert T_\mu J - T_\mu \bar J \Vert}_{\xi} \leq \beta {\Vert J - \bar J \Vert}_{\xi}
$$

Recall that

$$
\vert T_\mu J(i) - T_\mu \bar J(i) \vert \leq \beta \ \xi(i) \ {\Vert J - \bar J \Vert}_{\xi}
$$

$$
\implies (T_\mu J)(i) \leq (T_\mu \bar J)(i) + \beta \ \xi(i) \ {\Vert J - \bar J \Vert}_{\xi}
$$

Taking minimum over $\mu$ on either side,

$$
\implies (T J)(i) \leq (T \bar J)(i) + \beta \ \xi(i) \ {\Vert J - \bar J \Vert}_{\xi}
$$

We also get

$$
\implies (T \bar J)(i) \leq (T J)(i) + \beta \ \xi(i) \ {\Vert J - \bar J \Vert}_{\xi}
$$

Combining the two inequalities, we obtain

$$
\implies {\Vert T J - T \bar J \Vert}_{\xi} \leq \beta {\Vert J - \bar J \Vert}_{\xi}, \quad \ J, \bar J \in \mathbb{R}^n
$$

## Numerical schemes for solving MDPs

### Value iteration

- Critically depends on proposition-1

Recall proposition-1,

$$
\begin{aligned}
\lim_{k \to \infty} T_\mu^k J & = J_\mu, \quad \text{for any } J \in \mathbb{R}^n
\\
\lim_{k \to \infty} T^k J & = J^*, \quad \text{for any } J \in \mathbb{R}^n
\end{aligned}
$$

Consider the optimal control problem,

1. Choose some $J \in \mathbb{R}^n$
2. Recursively iterate $J \longleftarrow T^k J, \quad k = 1, 2, \dots$

We know that $T^k J \to J^*$ as $k \to \infty$

Suppose $V_0, \ V_1, \ V_2, \dots$ be the sequence of functions obtained when $T$ is applied

$$
V_{m+1} (i) = \min_{u \in A(i)} \sum_{j \in S} p_{ij}(u) \Big( g(i, u, j) + V_m(j) \Big), \quad \forall \ i \in S, \quad \forall \ m \geq 0
$$

starting with some $V_0 \in \mathbb{R}^n$.

By proposition-1, $V_m \to V^*$ as $n \to \infty$, where $V^* = T V^*$

---

## Grid world problem

- Sutton & Barto, Chapter 4

---

States are the cells in the grid

$14$ non-terminal states and $2$ terminal states

Rewards $\longrightarrow -1$ until termination

Consider the equiprobable policy

$$
\pi (\text{up} \mid s) = \pi (\text{down} \mid s) = \pi (\text{right} \mid s) = \pi (\text{left} \mid s) = \frac{1}{4}
$$

where $S$ is a non-terminal state

**Aim:** Apply value iteration for the equiprobable policy

- ~ (Optimal) value function is unique, ~policies can vary

### Value iteration

- Operates over values, upon convergence can get the optimal value function, and in turn the optimal policy

#### $k=0$

At $k = 0$, suppose $V_0 = 0$

#### $k=1$

$$
V (1) \longleftarrow \sum_{j \in S} p_{1j}(\mu(1)) \Big( g(1, \mu(1), j) + V(j) \Big) = \frac{1}{4} \Big( -1 -1 -1 -1 \Big) + \frac{1}{4} (0) = -1
$$

#### $k=2$

$$
V (1) \longleftarrow \frac{1}{4} \Big( -1 -1 -1 -1 \Big) + \frac{1}{4} \Big( -1 -1 -1 +0 \Big) = -1 -\frac{3}{4} = -\frac{7}{4}
$$

#### $k=3$

$$
V (1) \longleftarrow \frac{1}{4} \Big( -1 -1 -1 -1 \Big) + \frac{1}{4} \Big( -\frac{7}{4} -2 -2 +0 \Big) = -1 -\frac{23}{16} = -\frac{39}{16}
$$

$$
V (2) \longleftarrow -1 + \frac{1}{4} \Big( -2 -2 -2 -\frac{7}{4} \Big) = -1 -\frac{31}{16}= -\frac{47}{16}
$$

### Optimal policy

- Operates over policies

#### $k=0$

- Is a reward problem $\implies \max$

$$
V_{m+1} (i) = \max_{u \in A(i)} \sum_{j \in S} p_{ij}(u) \Big( g(i, u, j) + V_m(j) \Big), \quad \forall \ i \in S, \quad \forall \ m \geq 0
$$

#### $k=1$

$$
V(1) \longleftarrow \max \{ -1 + 0, \ -1 + 0, \ -1 + 0, \ -1 + 0 \} = -1
$$

#### $k = 2$

$$
V(1) \longleftarrow \max \{ -2, \ -2, \ -2, \ -1 \} = -1
$$

### Policy iteration

- Iterates over policies

---

