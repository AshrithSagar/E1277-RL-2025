# RL | 2025-03-20

## Policy evaluation with function approximation

Given $\mu, \Phi \in \mathbb{R}^{S\times d}$. We have $D_\mu = \operatorname{diag}(d_\mu)$. Want to do $J_\mu \approx \Phi \theta_\ast$.

$$
\frac{1}{2} \underbrace{\theta^\top \Phi^\top D_\mu \Phi \theta}_{d \times d}
$$

$$
f(\theta) = \frac{1}{2} {\Vert J_\mu - \Phi \theta \Vert}^2_{D_\mu} = \frac{1}{2} \sum_{s} d_\mu(s) {\Big [J_\mu(s) - \phi^\top(s) \theta \Big]}^2
$$

Can replace the $J_\mu$ with $J$ here

$$
\implies \nabla f'(\theta) = - \sum_{x} d_\mu(s) \Big[ J_\mu(s) - \phi^\top(s) \theta \Big] \phi(s) \implies 0
$$

$$
\sum_{s} d_\mu(s) J_\mu(s) \phi(s) = \sum_{s} d_\mu(s) \phi^\top(s) \theta \phi(s)
$$

$$
\begin{aligned}
\text{LHS} & = \underbrace{\Phi^\top}_{d \times S} \overbrace{D_\mu}^{S \times S} \underbrace{J_\mu}_{S \times 1} \\
\text{RHS} & = \sum_{s} d_\mu(s) \phi(s) \phi^\top(s) \theta = \Phi^\top D_\mu \Phi \theta
\end{aligned}
$$

$$
\begin{aligned}
\implies & \Phi^\top D_\mu J_\mu = \Phi^\top D_\mu \Phi \theta \\
\implies & \theta_\ast = {(\Phi^\top D_\mu \Phi)}^{-1} \Phi^\top D_\mu J_\mu \\
\implies & \Phi \theta = \Phi {(\Phi^\top D_\mu \Phi)}^{-1} \Phi^\top D_\mu J_\mu \\
\end{aligned}
$$

$\Phi \theta$ is closest approximation to $J$ in $\operatorname{col}(\Phi)$

$$
T_\mu = s + \gamma P_\mu J \implies T_\mu J_\mu = J_\mu
$$

$$
\Pi T_\mu \Phi \theta_\ast' = \Phi \theta'
$$

LLMs will eventually ~take over. It is our understanding which is important, the grand overview. Both should complement each other and work hand in hand.

**Question:** How does ${\Vert J_\mu - \Phi \theta_\ast' \Vert}_{D_\mu}$ compare with ${\Vert J_\mu - \Phi \theta_\ast \Vert}_{D_\mu}$?

**Claim:**

$$
{\Vert J_\mu - \Phi \theta_\ast \Vert}_{D_\mu} \leq {\Vert J_\mu - \Phi \theta_\ast' \Vert}_{D_\mu}
$$

$$
\begin{aligned}
{\Vert J_\mu - \Phi \theta_\ast' \Vert}_{D_\mu}
& \leq
{\Vert J_\mu - \Phi \theta_\ast \Vert}_{D_\mu}
+ {\Vert \Pi J_\mu - \Phi \theta_\ast' \Vert}_{D_\mu}
\\ & \leq
{\Vert J_\mu - \Phi \theta_\ast \Vert}_{D_\mu}
+ {\Vert \Pi J_\mu - \Pi T_\mu \Phi \theta_\ast' \Vert}_{D_\mu}
\end{aligned}
$$

Projected Bellman operator $\Pi$

**Claim:**
$$
{\Vert \Pi V - \Pi V' \Vert}_{D_\mu} \leq {\Vert V - V' \Vert}_{D_\mu}
$$

$$
\begin{aligned}
\implies
{\Vert J_\mu - \Phi \theta_\ast' \Vert}_{D_\mu}
& \leq
{\Vert J_\mu - \Phi \theta_\ast \Vert}_{D_\mu}
+ {\Vert J_\mu - T_\mu \Phi \theta_\ast' \Vert}_{D_\mu}
\\ & \leq
{\Vert J_\mu - \Phi \theta_\ast \Vert}_{D_\mu}
+ {\Vert T_\mu J_\mu - T_\mu \Phi \theta_\ast' \Vert}_{D_\mu}
\qquad (\because T_\mu J_\mu = J_\mu)
\\ & \leq
{\Vert J_\mu - \Phi \theta_\ast \Vert}_{D_\mu}
+ \gamma {\Vert J_\mu - \Phi \theta_\ast' \Vert}_{D_\mu}
\quad (T_\mu \text{ is a } \gamma-\text{contraction w.r.t. } {\Vert \cdot \Vert}_{D_\mu})
\end{aligned}
$$

$$
\implies {\Vert J_\mu - \Phi \theta_\ast' \Vert}_{D_\mu} \leq \frac{1}{1 - \gamma} {\Vert J_\mu - \Phi \theta_\ast \Vert}_{D_\mu}
$$

Consider playing a game, and analysing. Playing games is for kids, for RL practitioners, coming up with strategy is what matters :)

Choosing between $\mu_1$ and $\mu_2$, and comparing. Prone to making errors if choosing $\gamma$ close to 1.

The $\mu$ in $D_\mu$ plays an important role here.

### Summary

We have $\mu, J_\mu, \Phi$, and we want to find a good proxy for ~$J_\mu$

$$
\theta_{n+1} = \theta_n + \alpha_n \Big[ r(s_n, a_n) + \gamma \Phi^\top(s_n') \theta_n - \Phi^\top(s_n) \theta_n \Big] \Phi(s_n)
$$

~Dependencies $(s_n, a_n, s_n')$

Looking at a stochastic algorithm now

$$
\theta_{n+1} = \theta_n + \alpha_n \Big[ b - A \theta_n + M_{n+1} \Big]
$$

this is connected to the deterministic ODE

$$
\dot \theta(t) = b - A \theta(t)
$$

Analysing noiseless version first, to check if it's any good.

By Lyapunov, we can see that any solution trajectory would converge to $\theta_\ast' = A^{-1} b$.

This is the idealised algorithm, the question is, does the stochastic version also converge here to this point?

This, in some sense is a principled way to approach this. A guide to approching and designing algorithms.

Starting with some stochastic version, and check if it agrees with the idealistic version.

~Running simulations simply and seeing that empirically is not the right way.

Strating with a $\theta(t, t_0, \theta_0)$, it converges to $\theta_\ast'$, where $t_0$ is the initial time and $\theta_0$ is the inital point, i.e., $\theta(t, t_0, \theta_0) \to \theta_\ast'$

**Claim:**

${(\theta_n')}_{n \geq 0}$ generated using (the stochastic algorithm) converges almost surely to $\theta_\ast'$, i.e., $\theta_n \xrightarrow{a.s.} \theta_\ast'$.

~Almost surely here means that it converges, except for some measure $0$.

In some sense, this is a variant of the law of large numbers, in stochastic approximation, or rather inspired/ generalised from.

~LLN says that sample mean converges to true mean as number of samples goes to infinity, provided i.i.d., etc.

At a higher level, this is also saying something similar.

**Proof:**

We verify the four assumptions of the convergence result proved by Michel Benaim in 1996.

Can be found in chapter-2 of Borkar.

**A1.** $h$ is Lipschitz continuous

$$
\Vert h(x) - h(y) \Vert \leq L \Vert x - y \Vert
$$

The difference in a contraction mapping in contrast, is that we would have had $0 \leq L < 1$.

$$
\begin{aligned}
& h(x) = b - A x, \qquad h(y) = b - A y \\
& \Vert h(x) - h(y) \Vert = \Vert A(x - y) \Vert \leq \Vert A \Vert \, \Vert x - y \Vert, \qquad \Vert A \Vert = \sup_{x \neq 0} \frac{\Vert A x \Vert}{\Vert x \Vert}
\end{aligned}
$$

**A2.**

$$
\sum_{n \geq 0} \alpha_n = +\infty, \qquad \sum_{n \geq 0} \alpha_n^2 < +\infty
$$

Some step sizes that satisfy,

$$
\begin{aligned}
\alpha_n & = \frac{1}{n+1} \\
\alpha_n & = \frac{1}{n^\sigma}, \quad \sigma \in \Big( \frac{1}{2}, 1 \Big] \\
\end{aligned}
$$

In practice, keep step sizes constant for some time, and decrease after some time, and repeat.

The problem with this is that, while working with constant step sizes, it even multiples with the noise term, so we will keep hovering around.

Small step sizes would mean it will take a long time to converge.

Choosing step sizes properly, is domain specific, and an important aspect.

**A3.**

${(M_n)}_{n \geq 1}$ be a square-integrable Martingale difference sequence, i.e.,

$$
\begin{aligned}
& {\small(a).} \quad \operatorname{E} {\Vert M_n \Vert}^2 < +\infty, \ \forall n \\
& {\small(b).} \quad \operatorname{E} \Big[ M_{n+1} \Big| \mathcal{F}_n \Big] = 0
\end{aligned}
$$

Further, for some $K \geq \tau$, we have

$$
{\small(c).} \quad \operatorname{E} \Big[ {\Vert M_{n+1} \Vert}^2 \Big| \mathcal{F}_n \Big] \leq K \Big[ 1 + {\Vert \theta_n \Vert}^2 \Big]
$$

Martingale differences, in some way, are the next best thing to i.i.d. zero mean random variables. Most results that can be shown with i.i.d. zero mean random variables can probably be done with martingale difference sequences too. ~They capture some dependency between the variables.

**References:** Probability with Martingales with David Williams. Useful at an introductory level.

$$
\theta_{n+1} = \theta_n + \alpha_n \Big[ \overbrace{r(s_n, a_n) + \gamma \Phi^\top(s_n') \theta_n - \Phi^\top(s_n) \theta_n}^{\delta_n} \Big] \Phi(s_n)
$$

$$
M_{n+1} = \delta_n \phi(s_n) - (b - A \theta_n)
$$

$$
\Vert r(s_n, a_n) \Phi(s_n) \Vert = \vert r(s_n, a_n) \vert \ \Vert \Phi(s_n) \Vert \leq R_{\max} \cdot 1
$$

In RL, the value of the reward doesn't matter exactly, what matters is the ordering.

$$
b = \operatorname{E} \Big[ r(s_n, a_n) \Phi(s_n) \Big| \mathcal{F}_n \Big]
$$

$$
A = \operatorname{E} \Big[ \gamma \underbrace{\Phi(s_n) \Phi^\top(s_n')}_{d \times d} - \underbrace{\Phi(s_n) \Phi^\top(s_n)}_{d \times d} \Big| \mathcal{F}_n \Big]
$$

~ Can/Cannot(?) apply Cauchy-Schwarz inequality
$$
\Vert \gamma \Phi(s_n) \Phi^\top(s_n') \Vert = \gamma \Vert \Phi(s_n) \Phi^\top(s_n') \Vert \leq \gamma \ \Vert \Phi(s_n) \Vert \ \Vert \Phi^\top(s_n') \Vert \leq \gamma
$$

In finite-dimensional settings, all norms are similar. Upto some constant ~factor, this will hold. In the sense that, if points are closer based on one norm, they're also closed based on any other norm.

Inner products, and outer products.

$$
\Vert u v^\top \Vert = \sup_{x \neq 0} \frac{\Vert u v^\top x \Vert}{\Vert x \Vert} = \frac{\vert v^\top x \vert \ \Vert u \Vert}{\Vert x \Vert} \leq \frac{\Vert v \Vert \cdot \cancel{\Vert x \Vert} \ \Vert x \Vert}{\cancel{\Vert x \Vert}}
$$

$$
\begin{aligned}
\implies
M_{n+1} & = \delta_n \Phi(s_n) + (b - A \theta_n) \\
{\Vert M_{n+1} \Vert}^2 & \leq \zeta \Big[ {\Vert \delta_n \phi(s_n) \Vert}^2 + {\Vert b \Vert}^2 + {\Vert A \theta_n \Vert}^2 \Big]
\end{aligned}
$$

$\sim \quad +R_{\max}^2 + {\Vert A \Vert}^2 {\Vert \theta \Vert}^2 \implies +R_{\max}^2 + 2[1 + r^2] {\Vert \theta \Vert}^2 \implies \cdots$

In the end, we will get $\leq 6 [R_{\max}^2 + 2[1 + r^2] {\Vert \theta \Vert}^2]$

---

