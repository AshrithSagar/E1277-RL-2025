# RL | 2025-01-02

## About course

- Teams code: 17jggiq

- Extra class slot: Saturday, 9:30 - 11 AM
  - For missed classes, tutorials, etc

- References
  - R Sutton and A Barto, Reinforcement Learning, 2nd Edition, 2018

    <http://incompleteideas.net/book/RLbook2020.pdf>

    <https://www.andrew.cmu.edu/course/10-703/textbook/BartoSutton.pdf>

  - D. Balsekas and J. Tsitsiklis, Neuro Dynamic Programming, 1996
  - D. Balsekas, Optimal Control and Dynamic Programming, Vol 1 & 2, 2013/17
  - Dmitri Balsekas, Reinforcement Learning and Optimal Control, 2019

- Instructors
  - Shalabh Bhatnagar

    <https://www.csa.iisc.ac.in/~shalabh/>

  - Gugan Thoppe

    <https://sites.google.com/site/gugancth/home>

- Grading
  - Sessionals: 50 marks
    - SB
      - 25 marks
      - Quiz, 5 marks, ~End of January
      - Midterm, 20 marks, ~Feb 15

    - GT
      - 25 marks
  - Finals: 50 marks
    - Course project, 20 marks
    - Final exam, 30 marks

- TAs (PhD students)
  - Kaustubh
  - Prashansa
  - Naman
  - Ankur

- Pre-requisites
  - Random Processes, STOMA, Basic probability, etc

---

## Learning theory

- Learning theory
  - Supervised learning
  - Unsupervised learning
  - Reinforcement learning

### Reinforcement learning

- States of the environment

- ~The environment gives a response in terms of a reward or penalty, which triggers an action by the agent.

- **Goal** of an agent**:** To select a sequence of actions depending on the states of the environment that maximise a *long term* reward.

- Homogeneous transitions

  - Stationary dynamics $\to$ The dynamics is stationary

- *Probabilistic transition* between states
  $$
  P(S_{t+1} = s', r_{t+1} = r \mid S_t = s, A_t = a)
  $$
  
  - Non-stationary probabilities
  - Deterministic $\to$ is a special case of probabilistic
  - Random rewards, in general
  - Probabilistic rewards

## Class of problems

$N \to$ Number of states of decision making

![Timeline of decisions](./TeX/2025-01-02/1.png){ width=60% }

1. $N < \infty$

   1a. $N < \infty$ and $N$ is a deterministic number $\longrightarrow$ *Finite horizon problem*

   1b. $N < \infty$ with probability $1 \longrightarrow$ *Episodic* or *Stochastic shortest path problem*

   - Long term revival
     $$
     \operatorname{E} \left[ \sum_{i = 1}^{N} r_i \Bigg| S_0 = s \right]
     $$

2. $N = \infty$

   2a. *Discounted rewards*

   - Comes in Economic theory

     $$
     \lim_{N \to \infty} \operatorname{E} \left[ \sum_{i = 0}^{N} \gamma^i r_{i+1} \Bigg| S_0 = s \right]
     $$

     where $0 < \gamma < 1$ is the discounted factor

   - Perishable discovery items
     - Value goes down over time

   2b. *Long run average reward*

   $$
   \lim_{N \to \infty} \frac{1}{N} \operatorname{E} \left[ \sum_{i = 0}^{N} r_i \Bigg| S_0 = s \right]
   $$

   - Steady state performance
   - Long term
   - Long horizon

$$
\lim_{N \to \infty} (\gamma - 1) \Big( \text{Discounted revival, or Average revival} \Big) \to 0
$$

Key ingredients to an RL problem

- State (environment)
- Action (agent)

- Reward

Eg: Communication network (routing)

### Algorithms

- Monte-Carlo schemes
  - Are trajectory based
- Temporal difference methods
  - No need to wait till end of trajectory

## Exploration vs. Exploitation

- Comes up in learning theory
- Trade-offs between exploration and exploitation
  - Usually a combination of exploration and exploitation is preferred
- The way around is to
  - With high probability, select the action learnt so far as best action
  - With remaining probability, select a random action, ~against actions selected so far

## Tic Tac Toe game

- Player-1: RL agent
- Player-2: Professional player

---

