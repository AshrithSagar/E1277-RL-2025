# RL | 2025-02-25

## $Q$-learning

Consider SSPP

Recall Bellman equation

$$
J^*(i) = \min_{u \in A(i)} \left( \sum_{j = 1}^{n} p_{ij}(u) \Big( g(i, u, j) + J^*(j) \Big) \right), \quad \forall \ i \in S
$$

Let

$$
Q^*(i, u) = \sum_{j = 1}^{n} p_{ij}(u) \Big( g(i, u, j) + J^*(j) \Big), \quad \forall \ i \in S, \ u \in A(i)
$$

Then

$$
J^*(i) = \min_{u \in A(i)} Q^*(i, u)
$$

### $Q$-Bellman equation

$$
Q^*(i, u) = \sum_{j = 1}^{n} p_{ij}(u) \Big( g(i, u, j) + \min_{v \in A(j)} Q^*(j, v) \Big), \quad \forall \ i \in S, \ u \in A(i)
$$

A numerical scheme for it's solution

$$
Q_{m+1}(i, u) = \sum_{j = 1}^{n} p_{ij}(u) \Big( g(i, u, j) + \min_{v \in A(j)} Q_m(j, v) \Big), \quad \forall \ i \in S, \ u \in A(i), \ m \geq 0
$$

~ Similar to a value iteration scheme

It can be shown that $Q_{m}(i, u) \longrightarrow Q^*(i, u)$ as $n \to \infty, \ \forall \ i \in S, \ u \in A(i)$

Suppose now that we do not have access to $p_{ij}(u) \ \forall \ i, j \in S, \ u \in A(i)$

But suppose we have access to states $j \sim p_{i, \cdot} (u), \ \forall \ i \in S, \ u \in A(i)$

### Learning algorithm ($Q$- learning)

$$
Q_{m+1}(i, u) = Q_{m}(i, u) + \gamma_m \Big( g(i, u, j) + \min_{v \in A(j)} Q_m(j, v) - Q_m(j, v) \Big), \quad \forall \ i , u
$$

Here, $j \sim p_{i, \cdot}(u)$

~ $\gamma_m \to$ step size parameter/ learning rate

$\gamma_m$ should be selected such that

$$
\sum_{m} \gamma_m = \infty, \qquad \sum_{m} \gamma_m^2 < \infty
$$

### Proposition

~ General proposition on convergence. ~ $t \to$ index; ~ discrete time

Consider the following algorithm,

$$
r_{t + 1}(i) = \big(1 - \gamma_t(i) \big) \ r_t(i) + r_t(i) \big( (H r_t)(i) + w_t(i) \big)
$$

where

**(a).**

$$
\sum_{t} r_t(i) = \infty, \qquad \sum_{t} r_t(i) < \infty
$$

**(b). (i).**

$$
\forall \ i, t, \quad \mathbb{E} \left[ w_t(i) \mid \mathcal{F}_t \right] = 0, \quad \text{where } \mathcal{F}_t = \sigma(r_s, s \leq t, w_s, s < t)
$$

~ $\sigma$-algebra

**(b). (ii).**

$$
\exists A, B > 0, \text{ such that} \quad \mathbb{E} \left[ w_t^2(i) \mid \mathcal{F}_t \right] \leq A + B {\Vert r_t \Vert}^2, \quad \forall \ i, \ \forall t
$$

**(c).**

$H: \mathbb{R}^n \to \mathbb{R}^n$ is a weighted max norm pseudo contraction, i.e., $\exists r^* \in \mathbb{R}^n$, a positive vector $\xi = \Big( \xi(1), \ \xi(2), \ \dots, \ \xi(n) \Big)$ and a constant $\beta \in [0, 1)$ such that

$$
{\Vert H r - r^*\Vert}_{\xi} \leq \beta {\Vert r - r^* \Vert}_{\xi}
$$

where

$$
{\Vert x \Vert}_{\xi} = \max_{i = 1, 2, \dots n} \frac{\vert x(i) \vert}{\xi(i)}
$$

for any $x = {\Big( x(1), \ x(2), \ \dots, \ x(n) \Big)}^\top$. Then, $r_t \to r^*$ as $t \to \infty \text{ w.p. } 1$.

### Proposition: $Q$-learning convergence

Recall the $Q$-learning algorithm

$$
Q_{t + 1}(i) = \big(1 - \gamma_t(i) \big) \ Q_t(i, u) + \gamma_t(i, u) \Big( g(i, u, \bar i) + \min_{v \in A(\bar i)} Q_t(\bar i, v) \Big), \quad \forall \ i \in S, \ u \in A(i)
$$

where $\bar i = p_{i, \cdot}(u)$.

Let $Q_t(0, u) = 0 \ \forall \ u \in A(0)$.

Let $T^{i, u} \triangleq$ Set of times $t$ at which $Q(i, u)$ is updated.

Let $\gamma_t(i, u) = 0, \ \forall t \notin T^{i, u}$ and $\sum_{t} \gamma_t = \infty$, $\sum_{t} \gamma_t^2 < \infty$.

Then $Q_t(i, u) \longrightarrow Q^*(i, u) \text{ w.p. } 1 \ \forall \ i, u$ in both the following cases,

**(i).** All policies are proper, OR

**(ii).** Assumptions (A) and (B) holds.

#### Proof

Define the mappings $H$ as follows

$$
(HQ)(i, u) = \sum_{j = 1}^{n} p_{ij}(u) \left( g(i, u, j) + \min_{v \in A(j)} Q(j, v) \right), \quad \forall \ i \neq 0, \ u \in A(i)
$$

The $Q$-learning algorithm can then be rewritten as

$$
Q_{t + 1}(i) = \big(1 - \gamma_t(i, u) \big) \ Q_t(i, u) + \gamma_t(i, u) \Big( (H Q_t)(i, u) + q_t(i, u) \Big)
$$

Here,

$$
w_t(i, u) = g(i, u, \bar i) + \min_{v \in A(j)} Q_t(j, v) = \sum_{j = 1}^{n} p_{ij}(u) \left( g(i, u, j) + \min_{v \in A(j)} Q_t(j, v) \right)
$$

**Note:**

$$
\mathbb{E} \Big[ w_t(i, u) \Big| \mathcal{F}_t \Big] = 0
$$

$$
\mathbb{E} \Big[ w_t^2(i, u) \Big| \mathcal{F}_t \Big] \leq k \left( 1 + \max_{j, v} Q_t^2(j, v) \right)
$$

Then assumption (B) holds.

Suppose now that all policies are proper. Then we have seen that

$$
\begin{aligned}
& \exists \ \xi(i) > 0, \ \forall \ i \neq 0 \text{ and } \beta \in [0, 1] \text{ such that}
\\
& \sum_{j = 1}^{n} p_{ij}(u) \ \xi(i) \leq \beta \, \xi(i), \quad \forall \ i \neq 0, \ u \in A(i)
\end{aligned}
$$

Let $Q = {\Big( Q(i, u), \ \forall i \in S, \ u \in A(i) \Big)}^\top$. Let

$$
{\Vert Q \Vert}_{\xi} = \max_{i \in S, u \in A(i)} \frac{\vert Q(i, u) \vert}{\xi(i)}
$$

Consider two vectors $Q$ and $\bar Q$. Then

$$
\begin{aligned}
\left\vert (H Q)(i, u) - (H \bar Q)(i, u) \right\vert & \leq \sum_{j = 1}^{n} p_{ij}(u) \ \left\vert \min_{v \in A(j)} Q(j, v) - \min_{v \in A(j)} \bar Q(j, v) \right\vert
\\ & \leq
\sum_{j = 1}^{n} p_{ij}(u) \ \max_{v \in A(j)} \left\vert Q(j, v) - \bar Q(j, v) \right\vert \qquad (\text{TBP})
\\ & \leq
\sum_{j = 1}^{n} p_{ij}(u) \max_{v \in A(j)} \left( \frac{\left\vert Q(j, v) - \bar Q(j, v) \right\vert}{\xi(j)} \right) \xi(j)
\\ & \leq
\sum_{j = 1}^{n} p_{ij}(u) \ {\Vert Q - \bar Q\Vert}_{\xi} \ \xi(j)
\\ & \leq
\beta \ {\Vert Q - \bar Q\Vert}_{\xi} \ \xi(j)
\end{aligned}
$$

Since

$$
\sum_{j = 1}^{n} p_{ij}(u) \ \xi(j) \leq \beta \ \xi(i)
$$

$$
\implies
\begin{aligned}
\frac{\left\vert (H Q)(i, u) - (H \bar Q)(i, u) \right\vert}{\xi(i)}
& \leq
\beta \ {\Vert Q - \bar Q\Vert}_{\xi}, \quad \forall \ i \in S, \ u \in A(i)
\\
\implies
{\Vert H Q - \bar H Q\Vert}_{\xi}
& \leq
\beta \ {\Vert Q - \bar Q\Vert}_{\xi}
\end{aligned}
$$

By the general proposition on convergence, $Q$-learning algorithm converges.

---

Note that if $A \subset B$, then

$$
\inf_{x \in A} f(x) \geq \inf_{x \in B} f(x)
$$

$$
\inf_{x \in A} \left( f(x) + g(x) \right)
= \inf_{x, y \in A, \ x = y} \left( f(x) + g(y) \right)
\geq \inf_{x, y \in A} \left( f(x) + g(y) \right)
$$

Thus,

$$
\inf_{x \in A} \left( f(x) + g(x) \right) \geq  \inf_{x, y \in A} \left( f(x) + g(y) \right)
$$

$$
\inf_{x \in A} \left( (f \cdot g)(x) + g(x) \right) \geq \inf_{x \in A} (f \cdot g)(x) + \inf_{x \in A} g(x)
$$

or

$$
\inf_{x \in A} f(x) \geq \inf_{x \in A} \left( f(x) - g(x) \right) + \inf_{x \in A} g(x)
$$

$$
\inf_{x \in A} \left( f(x) - g(x) \right) \leq \inf_{x \in A} f(x) - \inf_{x \in A} g(x)
$$

Let $h(x) = - g(x), \ \forall x$. Then,

$$
\sup_{x \in A} h(x) = \sup_{x \in A} (- g(x)) = - \inf_{x \in A} g(x)
$$

$$
\therefore \quad \inf_{x \in A} (f(x) + h(x)) \leq \inf_{x \in A} f(x) + \sup_{x \in A} h(x)
$$

$$
\therefore \quad \inf_{x \in A} (f(x) + h(x)) - \inf_{x \in A} f(x) \leq \sup_{x \in A} h(x)
$$

Let $h(x) = g(x) - f(x)$.

$$
\implies
\inf_{x \in A} g(x) - \inf_{x \in A} f(x)
\leq \sup_{x \in A} \left( g(x) - f(x) \right)
\leq \sup_{x \in A} \left\vert g(x) - f(x) \right\vert
$$

Similarly,

$$
\inf_{x \in A} f(x) - \inf_{x \in A} g(x) \leq \sup_{x \in A} \vert g(x) - f(x) \vert
$$

$$
\implies
\vert \inf_{x \in A} f(x) - \inf_{x \in A} g(x) \vert \leq \sup_{x \in A} \vert g(x) - f(x) \vert
$$

Thus,

$$
\left\vert \min_{v \in A(j)} Q(j, v) - \min_{v \in A(j)} \bar Q(j, v) \right\vert \leq \max_{v \in A(j)} \left\vert Q(j, v) - \bar Q(j, v) \right\vert
$$

Suppose that state $S_t$ is visited at time $t$.

$Q$-learning algorithm (in the online setting) (~on-the-fly),

$$
Q_{t+1}(S_t, A_t) = Q_t(S_t, A_t) + \gamma_t(S_t, A_t) \left( g(S_t, A_t, S_{t+1}) + \min_{v \in A(S_{t+1})} Q_t(S_{t+1}, v) - Q_t(S_t, A_t) \right)
$$

with

$$
Q_{t+1}(s, a) = Q_t(s, a), \quad \forall \ s \neq S_t \text{ or } a \neq A_t
$$

---

**(Q).** How do we select $A_t$ in the update rule?

**(A).** $A_t$ is selected randomly from the set $A(S_t)$.

An alternative way of rewriting the above is

$$
Q_{t+1}(S_t, A_t) = Q_t(S_t, A_t) + \gamma_t(S_t, A_t) \Big( g(S_t, A_t, S_{t+1}) + Q_t(S_{t+1}, A_{t+1}) - Q_t(S_t, A_t) \Big)
\label{eq:dagger}
$$

### SARSA

Use $\eqref{eq:dagger}$ but with

$$
A_t = \begin{cases}
\arg \min_{u \in A(S_t)} Q_t(s, u) & \text{w.p. } 1-\varepsilon \\
\text{random action} & \text{w.p. } \varepsilon
\end{cases}
$$

$$
A_{t+1} = \begin{cases}
\arg \min_{v \in A(S_{t+1})} Q_t(S_{t+1}, v) & \text{w.p. } 1-\varepsilon \\
\text{random action} & \text{w.p. } \varepsilon
\end{cases}
$$

SARSA is an on-policy algorithm.

~ In contrast to $Q$-learning, which is an off-policy algorithm.

**One possibility:**

$$
A_t = \begin{cases}
\arg \min_{u \in A(S_t)} Q_t(S_t, u) & \text{w.p. } 1-\varepsilon \\
\text{random action} & \text{w.p. } \varepsilon
\end{cases}
$$

$$
A_{t+1} = \begin{cases}
\arg \min_{v \in A(S_{t+1})} Q_t(S_{t+1}, v) & \text{w.p. } 1-\varepsilon \\
\text{random action} & \text{w.p. } \varepsilon
\end{cases}
$$

Off-policy algorithm.

Double $Q$-learning, Expected SARSA.

---

