# RL | 2025-01-21

## Topics

- Basics of RL
- Multi-armed Bandits
  - Sutton & Barto, Reinforcement learning, Chapter-1,2

- Finite horizon Markov Decision Process
  - Bertsekas, Dynamic programming and optimal control, Volume-1, Chapter-1
- Stochastic shortest path problems
  - Bertsekas & Tsitsiklis, Neuro-dynamic programming
  - Bertsekas, Reinforcement learning & optimal control, 2019
  - Bertsekas, Dynamic programming and optimal control, Volume-2, Chapter-2

---

- Dropping the Markov assumption, we use a technique known as ~state aggregation

  $$
  \begin{aligned}
  S_{n+1}
  & =
  f(S_n, S_{n-1}, \dots, S_{n-N}, a_n)
  \\
  \phi_n
  & =
  (S_n, S_{n-1}, \dots, S_{n-N})
  \end{aligned}
  $$

## Stochastic shortest path problems

- a.k.a Episodic problems
- Characterised by the existance of a goal/terminal state
- Assumed that there is cost-free terminal state $0$

$$
p_{00}(u) = 1, \qquad g(0, u, 0) = 0, \quad \forall u \in A(0)
$$

- Non-terminal states: $1, 2, \dots, n$
- ~ Will stay in terminal state (forever) once reaching there
- No discounting, i.e., $\gamma = 1$
  - ~Alternatively, posing as an infinite horizon MDP

### Problem

How to to reach the terminal state with a minimum expected cost. Let

$$
J_\mu (i) \triangleq \lim_{N\to \infty} \mathbb{E}_\mu \left[ \sum_{m=0}^{N-1} g(S_m, \mu(S_m), S_{m+1}) \Big| S_0 = i \right]
$$

**Note:**

We consider stationary policies of the form $\pi = \{ \mu, \mu, \mu, \dots \}$. Many times, it is convenient to simply call $\mu$ as our policy.

It is sufficient to restrict the search for an optimal policy within the set of all stationary policies. ~An optimal policy, if it exists, will also be stationary.

### Definitions

1. A stationary policy is said to be proper, if

$$
\rho_\mu \triangleq  \max _{\underbrace{ i = 1, 2, \dots, n }_{\text{over non-terminal states}}} P(S_n \neq 0 \mid S_0 = i, \mu) < 1
$$

Implications: Starting from any initial state, after $n$ steps, there is a positive probability that you terminate.

Equivalently, can also be defined using $\min$, in which case the inequality becomes $> 0$.

$\rho_\mu$ eventually acts like a discounting factor, even if $\gamma = 1$, ~in cases where we're unable to use it.

$\rho_\mu = 0 \implies$ converged already, which is a special case.

State spaces:

- Non-terminal states ($\text{NT}$): $\{ 1, 2, \dots, n \}$
- Terminal states ($\text{T}$): $\{ 0 \}$

$$
S = \text{NT} \cup \text{T} = \{ 0, 1, 2, \dots, n \}
$$

2. A stationary policy that is not proper is called improper.

**Note:** $\mu$ is proper $\implies$ in the Markov chain corresponding to $\mu$, there is a path of positive probability from each state to the terminal state.

---

Aside, consider an MDP $\{ X_n \}$ governed by the stationary policy $\mu$,

$$
\begin{aligned}
&
P(X_{n+1} = j \mid X_n = i, Z_n = a, X_{n-1} = i_{n-1}, Z_{n-1} = a_{n-1}, \dots, X_0 = i_0, Z_0 = a_0)
\\ & \quad =
P(X_{n+1} = j \mid X_n = i, Z_n = a)
\end{aligned}
$$

When governed by policy $\mu$,

$$
\begin{aligned}
&
P(X_{n+1} = j \mid X_n = i, Z_n = \mu(i), X_{n-1} = i_{n-1}, Z_{n-1} = \mu(i_{n-1}), \dots, X_0 = i_0, Z_0 = \mu(i_0))
\\ & \quad =
P(X_{n+1} = j \mid X_n = i, Z_n = \mu(i))
\end{aligned}
$$

Let

$$
P(X_{n+1} = j \mid X_n = i, Z_n = \mu(i)) \triangleq P_\mu (i, j)
$$

Then

$$
\begin{aligned}
P_\mu (i, j) & \geq 0, \quad \forall \ j \in S
\\
\sum_{j \in S} P_\mu (i, j) & = 1, \quad i \in \text{NT}
\end{aligned}
$$

~For non-homogeneous Markov chains,

$$
P(X_{n+1} = j \mid X_n = i, Z_n = \mu_n(i)) \triangleq {}^n P_\mu (i, j)
$$

~Homogeneous $\implies$ State transition probabilities are time independent

Assuming $\mu$ is proper, consider

$$
\begin{aligned}
P(S_{2n} \neq 0 \mid S_0 = i, \mu)
& =
P(S_{2n} \neq 0 \mid S_0 = i, S_n \neq 0, \mu) \ P(S_n \neq 0 \mid S_0 = i, \mu)
\\ & \quad +
\underbrace{ P(S_{2n} \neq 0 \mid S_0 = i, S_n = 0, \mu) }_{0} \ P(S_n = 0 \mid S_0 = i, \mu)
\end{aligned}
$$

By definition,

$$
P(S_n \neq 0 \mid S_0 = i, \mu) \leq \rho_\mu
$$

By Markov property,

$$
P(S_{2n} \neq 0 \mid S_0 = i, S_n \neq 0, \mu) \leq \rho_\mu
$$

$$
\implies P(S_{2n} \neq 0 \mid S_0 = i, \mu) = \rho_\mu^2
$$

More generally,

$$
P(S_k \neq 0 \mid S_0 = i, \mu) \leq {\rho_\mu}^{\lfloor k/n \rfloor}, \quad i = 1, 2, \dots, n
$$

Eg: Consider $n < k < 2n$,

$$
P(S_k \neq 0 \mid S_0 = i, \mu) = \underbrace{ P(S_k \neq 0 \mid S_0 = i, S_n \neq 0, \mu) }_{< 1} \underbrace{ P(S_n \neq 0 \mid S_0 = i, \mu) }_{\leq \rho_\mu}
$$

From the foregoing,

$$
\lim_{k \to \infty} P(S_k \neq 0 \mid S_0 = i, \mu) = 0
$$

Recall that

$$
J_\mu (i) = \lim_{N \to \infty} \mathbb{E}_\mu \left[ \sum_{m=0}^{N-1} g(S_m, \mu(S_m), S_{m+1}) \Big| S_0 = i \right]
$$

Assuming $\mu$ is a proper policy,

$$
\begin{aligned}
J_\mu (i)
& =
\mathbb{E}_\mu \left[ \sum_{m=0}^{\infty} g(S_m, \mu(S_m), S_{m+1}) \Big| S_0 = i \right]
\\ & =
\sum_{m=0}^{\infty} \mathbb{E}_\mu \Big[ g(S_m, \mu(S_m), S_{m+1}) \Big| S_0 = i \Big]
\end{aligned}
$$

We'll assume that

$$
\vert g(i, a, j) \vert \leq K, \quad \forall i, \ \forall a \in A(i), \ \forall j
$$

i.e., the costs are bounded. Then

$$
\begin{aligned}
\implies
\vert J_\mu (i) \vert
& \leq
\sum_{m=0}^{\infty} \mathbb{E}_\mu \Big[ \vert g(S_m, \mu(S_m), S_{m+1}) \vert \Big| S_0 = i \Big]
\\ & =
\sum_{m=0}^{\infty} \sum_{j} \sum_{k} p_{ij}^{m}(\mu) \ p_{ijk}(\mu(j)) \ \vert g(j, \mu(j), k) \vert
\end{aligned}
$$

Let

$$
\sum_{k} p_{ij}(\mu(j)) \ \vert g(j, \mu(j), k) \vert \triangleq \widehat g_{\mu}(j)
$$

Then

$$
\implies \vert J_\mu (i) \vert \leq \sum_{m=0}^{\infty} \sum_{j} p_{ij}^{m}(\mu) \ \widehat g_{\mu}(j)
$$

When $j = 0 \implies \widehat g_u(j) = 0$. Then

$$
\vert J_\mu (i) \vert \leq \sum_{m=0}^{\infty} \sum_{j=1}^{n} p_{ij}^{m}(\mu) \underbrace{ \left( \max_{j = 1, 2, \dots, n} \widehat g_{\mu}(j) \right) }_{\leq K}
$$

Now,

$$
\sum_{j = 1}^{n} p_{ij}^m(\mu) = P(S_n \neq 0 \mid S_0 = i, \mu) \leq \rho_\mu^{\lfloor m/n \rfloor}
$$

$$
\implies\vert J_\mu (i) \vert \leq \sum_{m=0}^{\infty} \rho_\mu^{\lfloor m/n \rfloor} K < \infty, \quad \because \rho_\mu < 1
$$

Let

$$
\bar g(i, u) \triangleq \sum_{j=0}^{n} p_{ij}(u) \ g(i, u, j)
$$

denote the expected single stage cost in the non-terminal state $i \in \{ 1, 2, \dots, n \}$ when action $u$ is chosen.

We define now, mappings $T$ and $T_\mu$ in functions $J = (J(1), \ J(2), \ \dots, \ J(n)), \quad J: \text{NT} \to \mathbb{R}$,

$$
(TJ)(i) = \min_{u \in A(i)} \left[ \bar g(i, u) + \sum_{j=1}^{n} p_{ij}(u) \ J(j) \right], \quad i = 1, 2, \dots, n
$$

$$
(T_\mu J)(i) = \bar g_\mu(i) + \sum_{j=1}^{n} p_{ij}(\mu(i)) \ J(j)
$$

where

$$
\bar g_\mu(i) \triangleq \bar g(i, \mu(i))
$$

Eventually, $T$ and $T_\mu$ will play a crucial role in Bellman equations; Monotonicity, fixed points.

Let

$$
P_\mu =
\begin{bmatrix}
p_{11}(\mu(1)) & p_{12}(\mu(1)) & \dots & p_{1n}(\mu(1)) \\
p_{21}(\mu(2)) & p_{22}(\mu(2)) & \dots & p_{2n}(\mu(2)) \\
\vdots & \vdots & \ddots & \vdots \\
p_{n1}(\mu(n)) & p_{n2}(\mu(n)) & \dots & p_{nn}(\mu(n)) \\
\end{bmatrix}
$$

Observe that, the row sum

$$
\sum_{j = 1}^{n} p_{ij}(\mu(i)) \leq 1
$$

since this matrix is only over the non-terminal states.

---

