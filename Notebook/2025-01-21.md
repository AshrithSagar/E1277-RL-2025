# RL | 2025-01-21

## Topics

- Basics of RL
- Multi-armed Bandits
  - Sutton & Barto, Reinforcement learning, Chapter-1,2

- Finite horizon Markov Decision Process
  - Bertsekas, Dynamic programming and optimal control, Volume-1, Chapter-1
- Stochastic shortest path problems
  - Bertsekas & Tsitsiklis, Neuro-dynamic programming
  - Bertsekas, Reinforcement learning & optimal control, 2019
  - Bertsekas, Dynamic programming and optimal control, Volume-2, Chapter-2


---

- Dropping the Markov assumption, we use a technique known as ~state aggregation
  $$
  S_{n+1} = f(S_n, S_{n-1}, \dots, S_{n-N}, a_n)
  \\
  \phi_n = (S_n, S_{n-1}, \dots, S_{n-N})
  $$

- 

## Stochastic shortest path problems

- a.k.a Episodic problems
- Characterised by the existance of a goal/terminal state
- Assumed that there is cost-free terminal state $0$

$$
p_{00}(u) = 1, \qquad g(0, u, 0) = 0, \quad \forall u \in A(0)
$$

- Non-terminal states: $1, 2, \dots, n$
- ~ Will stay in terminal state (forever) once reaching there
- No discounting, i.e., $\gamma = 1$
  - ~Alternatively, posing as an infinite horizon MDP


### Problem

How to to reach the terminal state with a minimum expected cost. Let
$$
J_\mu (i) \triangleq \lim_{N\to \infty} \operatorname{E}_\mu \left[ \sum_{m=0}^{N-1} g(S_m, \mu(S_m), S_{m+1}) \Big| S_0 = i \right]
$$
**Note:**

We consider stationary policies of the form $\pi = \{ \mu, \mu, \mu, \dots \}$. Many times, it is convenient to simply call $\mu$ as our policy.

It is sufficient to restrict the search for an optimal policy within the set of all stationary policies. ~An optimal policy, if it exists, will also be stationary.

### Definitions

1. A stationary policy is said to be proper, if

$$
\rho_\mu \triangleq  \max _{\underbrace{ i = 1, 2, \dots, n }_{\text{over non-terminal states}}} P(S_n \neq 0 \mid S_0 = i, \mu) < 1
$$

Implications: Starting from any initial state, after $n$ steps, there is a positive probability that you terminate.

Equivalently, can also be defined using $\min$, in which case the inequality becomes $> 0$.

$\rho_\mu$ eventually acts like a discounting factor, even if $\gamma = 1$, ~in cases where we're unable to use it.

$\rho_\mu = 0 \implies$ converged already, which is a special case.

State spaces:

- Non-terminal states ($\text{NT}$): $\{ 1, 2, \dots, n \}$
- Terminal states ($\text{T}$): $\{ 0 \}$

$$
S = \text{NT} \cup \text{T} = \{ 0, 1, 2, \dots, n \}
$$

2. A stationary policy that is not proper is called improper.

**Note:** $\mu$ is proper $\implies$ in the Markov chain corresponding to $\mu$, there is a path of positive probability from each state to the terminal state.

---

