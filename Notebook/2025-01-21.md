# RL | 2025-01-21

## Topics

- Basics of RL
- Multi-armed Bandits
  - Sutton & Barto, Reinforcement learning, Chapter-1,2

- Finite horizon Markov Decision Process
  - Bertsekas, Dynamic programming and optimal control, Volume-1, Chapter-1
- Stochastic shortest path problems
  - Bertsekas & Tsitsiklis, Neuro-dynamic programming
  - Bertsekas, Reinforcement learning & optimal control, 2019
  - Bertsekas, Dynamic programming and optimal control, Volume-2, Chapter-2

---

- Dropping the Markov assumption, we use a technique known as ~state aggregation

  $$
  S_{n+1} = f(S_n, S_{n-1}, \dots, S_{n-N}, a_n)
  \\
  \phi_n = (S_n, S_{n-1}, \dots, S_{n-N})
  $$

## Stochastic shortest path problems

- a.k.a Episodic problems
- Characterised by the existance of a goal/terminal state
- Assumed that there is cost-free terminal state $0$

$$
p_{00}(u) = 1, \qquad g(0, u, 0) = 0, \quad \forall u \in A(0)
$$

- Non-terminal states: $1, 2, \dots, n$
- ~ Will stay in terminal state (forever) once reaching there
- No discounting, i.e., $\gamma = 1$
  - ~Alternatively, posing as an infinite horizon MDP


### Problem

How to to reach the terminal state with a minimum expected cost. Let

$$
J_\mu (i) \triangleq \lim_{N\to \infty} \operatorname{E}_\mu \left[ \sum_{m=0}^{N-1} g(S_m, \mu(S_m), S_{m+1}) \Big| S_0 = i \right]
$$

**Note:**

We consider stationary policies of the form $\pi = \{ \mu, \mu, \mu, \dots \}$. Many times, it is convenient to simply call $\mu$ as our policy.

It is sufficient to restrict the search for an optimal policy within the set of all stationary policies. ~An optimal policy, if it exists, will also be stationary.

### Definitions

1. A stationary policy is said to be proper, if

$$
\rho_\mu \triangleq  \max _{\underbrace{ i = 1, 2, \dots, n }_{\text{over non-terminal states}}} P(S_n \neq 0 \mid S_0 = i, \mu) < 1
$$

Implications: Starting from any initial state, after $n$ steps, there is a positive probability that you terminate.

Equivalently, can also be defined using $\min$, in which case the inequality becomes $> 0$.

$\rho_\mu$ eventually acts like a discounting factor, even if $\gamma = 1$, ~in cases where we're unable to use it.

$\rho_\mu = 0 \implies$ converged already, which is a special case.

State spaces:

- Non-terminal states ($\text{NT}$): $\{ 1, 2, \dots, n \}$
- Terminal states ($\text{T}$): $\{ 0 \}$

$$
S = \text{NT} \cup \text{T} = \{ 0, 1, 2, \dots, n \}
$$

2. A stationary policy that is not proper is called improper.

**Note:** $\mu$ is proper $\implies$ in the Markov chain corresponding to $\mu$, there is a path of positive probability from each state to the terminal state.

---

Aside, consider an MDP $\{ X_n \}$ governed by the stationary policy $\mu$,

$$
P(X_{n+1} = j \mid X_n = i, Z_n = a, X_{n-1} = i_{n-1}, Z_{n-1} = a_{n-1}, \dots, X_0 = i_0, Z_0 = a_0)
\\ =
P(X_{n+1} = j \mid X_n = i, Z_n = a)
$$

When governed by policy $\mu$,

$$
P(X_{n+1} = j \mid X_n = i, Z_n = \mu(i), X_{n-1} = i_{n-1}, Z_{n-1} = \mu(i_{n-1}), \dots, X_0 = i_0, Z_0 = \mu(i_0))
\\ =
P(X_{n+1} = j \mid X_n = i, Z_n = \mu(i))
$$

Let

$$
P(X_{n+1} = j \mid X_n = i, Z_n = \mu(i)) \triangleq P_\mu (i, j)
$$

Then

$$
P_\mu (i, j) \geq 0, \quad \forall \ j \in S
\\
\sum_{j \in S} P_\mu (i, j) = 1, \quad i \in \text{NT}
$$

~For non-homogeneous Markov chains,

$$
P(X_{n+1} = j \mid X_n = i, Z_n = \mu_n(i)) \triangleq {}^n P_\mu (i, j)
$$

~Homogeneous $\implies$ State transition probabilities are time independent

Assuming $\mu$ is proper, consider

$$
\begin{aligned}
P(S_{2n} \neq 0 \mid S_0 = i, \mu)
& =
P(S_{2n} \neq 0 \mid S_0 = i, S_n \neq 0, \mu) \ P(S_n \neq 0 \mid S_0 = i, \mu)
\\ & \quad +
\underbrace{ P(S_{2n} \neq 0 \mid S_0 = i, S_n = 0, \mu) }_{0} \ P(S_n = 0 \mid S_0 = i, \mu)
\end{aligned}
$$

By definition,

$$
P(S_n \neq 0 \mid S_0 = i, \mu) \leq \rho_\mu
$$

By Markov property,

$$
P(S_{2n} \neq 0 \mid S_0 = i, S_n \neq 0, \mu) \leq \rho_\mu
$$

$$
\implies P(S_{2n} \neq 0 \mid S_0 = i, \mu) = \rho_\mu^2
$$

More generally,

$$
P(S_k \neq 0 \mid S_0 = i, \mu) \leq {\rho_\mu}^{\lfloor k/n \rfloor}, \quad i = 1, 2, \dots, n
$$

Eg: Consider $n < k < 2n$,

$$
P(S_k \neq 0 \mid S_0 = i, \mu) = \underbrace{ P(S_k \neq 0 \mid S_0 = i, S_n \neq 0, \mu) }_{< 1} \underbrace{ P(S_n \neq 0 \mid S_0 = i, \mu) }_{\leq \rho_\mu}
$$

From the foregoing,

$$
\lim_{k \to \infty} P(S_k \neq 0 \mid S_0 = i, \mu) = 0
$$

---

