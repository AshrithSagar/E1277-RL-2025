\section*{Question 1}

Consider the MDP \( \mathcal{M} \equiv(\mathcal{S}, \mathcal{A}, P, r, \gamma) \) with \( \vert \mathcal{S} \vert = S \) and \( \vert \mathcal{A} \vert = A \).
Suppose \( \mu \) is a stochastic policy and \( \Phi \in \mathbb{R}^{S \times d} \) a feature matrix for some \( d \geq 1 \).
Let \( P_{\mu} \) be the \( S \times S \) matrix given by
\[
    P_{\mu}\left(s^{\prime} \mid s\right)=\sum_{a} \mu(a \mid s) P\left(s^{\prime} \mid s, a\right)
\]
This matrix represents the transition matrix of the Markov chain \( \left(\mathcal{S}, P_{\mu}\right) \) induced by \( \mu \).
Suppose this Markov chain is ergodic so that it has a unique stationary distribution, which we denote by \( d_{\mu} \).
Let \( D_{\mu} \) be the \( S \times S \) diagonal matrix whose diagonal is \( d_{\mu} \), and let
\[
    A=\Phi^{\top} D_{\mu}\left(\mathbb{I}-\gamma P_{\mu}\right) \Phi \quad \text { and } b=\Phi^{\top} D_{\mu} r_{\mu},
\]
where \( r_{\mu}(s)=\sum_{a} \mu(a \mid s) r(s, a) \).
Let \( \Pi: \mathbb{R}^{S} \rightarrow \mathbb{R}^{S} \) be given by \( \Pi J=\Phi{\left(\Phi^{\top} D_{\mu} \Phi\right)}^{-1} \Phi^{\top} D_{\mu} J \).

Show that \( \theta_\ast:=A^{-1} b \) is the fixed point of the projected Bellman operator, i.e., \( \Pi T_{\mu} \Phi \theta_\ast=\Phi \theta_\ast \), where \( T_{\mu}: \mathbb{R}^{S} \rightarrow \mathbb{R}^{S} \) is the Bellman operator satisfying \( T_{\mu} J=r_{\mu}+\gamma P_{\mu} J \).

\subsection*{Solution}

Given an MDP \( \mathcal{M} \equiv(\mathcal{S}, \mathcal{A}, P, r, \gamma) \) with \( \vert \mathcal{S} \vert = S \) and \( \vert \mathcal{A} \vert = A \), we have the Bellman operator defined as
\[
    T_{\mu} J(s) = r_{\mu}(s) + \gamma \sum_{s^{\prime}} P_{\mu}(s^{\prime} \mid s) J(s^{\prime}),
\]
where \( J \in \mathbb{R}^{S} \) is the value function.

In (linear) stochastic function approximation, we are interested in finding a \( \theta_\ast \in \mathbb{R}^{d} \) such that \( J_\mu \approx \Phi \theta_\ast \), where \( J_\mu \) is the optimal value function for the policy \( \mu \), and \( \Phi \in \mathbb{R}^{S \times d} \) is a feature matrix.
The reason we want to do this is because the dimension of \( J_\mu \) is \( S \), which can be very large, in general, and we want to approximate it with a smaller dimensional representation \( \Phi \theta_\ast \).
From general MDP theory, we know that the optimal value function \( J_\mu \), for a given policy \( \mu \), satisfies the Bellman equation
\[
    J_\mu(s) = r_{\mu}(s) + \gamma \sum_{s^{\prime}} P_{\mu}(s^{\prime} \mid s) J_\mu(s^{\prime}),
    \implies
    J_\mu = r_{\mu} + \gamma P_{\mu} J_\mu.
    \implies
    \left(\mathbb{I}-\gamma P_{\mu}\right) J_\mu = r_{\mu}.
\]
Given that the underlying Markov chain \( \left(\mathcal{S}, P_{\mu}\right) \) induced by \( \mu \) is ergodic, it converges to a unique stationary distribution \( d_{\mu}: \mathcal{S} \rightarrow \mathbb{R} \) such that
\[
    d_{\mu}(s) = \sum_{s^{\prime}} P_{\mu}(s^{\prime} \mid s) d_{\mu}(s^{\prime}),
    \implies
    d_{\mu} = P_{\mu}^{\top} d_{\mu}.
\]

Since we want a \( \theta_\ast \) such that \( J_\mu \approx \Phi \theta_\ast \), we start by posing this as an optimization problem, i.e., we want to find some \( \theta \) such that
\[
    f(\theta) = \frac{1}{2} \Vert J_\mu - \Phi \theta \Vert_{D_{\mu}}^2
    = \frac{1}{2} \sum_{s} d_{\mu}(s) {\left[ J_\mu(s) - \phi^\top(s) \theta \right]}^2
\]
is minimized.
This is how we define the optimality for how we want to approximate the value function \( J_\mu \) with a smaller dimensional representation \( \Phi \theta \).
For such a function \( f(\theta) \), we can find the optimal \( \theta_\ast \) by setting the gradient of \( f(\theta) \) to zero, i.e.,
\[
    \nabla f(\theta_\ast) = - \sum_{s} d_{\mu}(s) \left[ J_\mu(s) - \phi^\top(s) \theta_\ast \right] \phi(s) = 0
\]
\[
    \implies
    \sum_{s} d_{\mu}(s) J_\mu(s) \phi(s) = \sum_{s} d_{\mu}(s) {\left[ \phi^\top(s) \theta_\ast \right]} \phi(s)
\]
This is the projected Bellman equation, we're interested to find.
In matrix form, we have
\begin{align*}
    \text{LHS}
     & \to
    \underbrace{\Phi^\top}_{d \times S} \overbrace{D_\mu}^{S \times S} \underbrace{J_\mu}_{S \times 1}
    \\
    \text{RHS}
     & \to
    \sum_{s} d_\mu(s) \phi(s) \phi^\top(s) \theta = \underbrace{\Phi^\top}_{d \times S} \overbrace{D_\mu}^{S \times S} \underbrace{\Phi}_{S \times d} \overbrace{\theta}^{d \times 1}
\end{align*}
\begin{align*}
    \implies
    \Phi^\top D_\mu J_\mu
     & =
    \Phi^\top D_\mu \Phi \theta_\ast
    \\
    \implies
    \theta_\ast
     & =
    {(\Phi^\top D_\mu \Phi)}^{-1} \Phi^\top D_\mu J_\mu
    \\
    \implies
    \Phi \theta_\ast
     & =
    \Phi {(\Phi^\top D_\mu \Phi)}^{-1} \Phi^\top D_\mu J_\mu
\end{align*}

This such \( \Phi \theta_\ast \) is closest approximation to \( J_\mu \) in the column space of \( \Phi \), \(\operatorname{col}(\Phi) \).

Now, we can appropriately define the project Bellman operator as
\[
    \Pi J = \Phi {(\Phi^\top D_\mu \Phi)}^{-1} \Phi^\top D_\mu J
\]

Starting with the Bellman equation, and applying the projected Bellman operator \( \Pi \) on both sides, and using \( J = \Phi \theta_\ast \), we have
\begin{align*}
    T_{\mu} J
     & =
    r_{\mu} + \gamma P_{\mu} J
    \\
    \implies
    \Pi T_{\mu} J
     & =
    \Pi \left(r_{\mu} + \gamma P_{\mu} J\right)
    =
    \Phi{\left(\Phi^{\top} D_{\mu} \Phi\right)}^{-1} \Phi^{\top} D_{\mu} J
    \\
    \implies
    \Pi T_{\mu} \Phi \theta_\ast
     & =
    \Phi{\left(\Phi^{\top} D_{\mu} \Phi\right)}^{-1} \Phi^{\top} D_{\mu} \Phi \theta_\ast
    \\ & =
    \Phi \cancel{{\left(\Phi^{\top} D_{\mu} \Phi\right)}^{-1}} \cancel{\left(\Phi^{\top} D_{\mu} \Phi\right)} \theta_\ast
    =
    \Phi \theta_\ast
    \\
    \therefore
    \Pi T_{\mu} \Phi \theta_\ast
     & =
    \Phi \theta_\ast
\end{align*}
as required, showing that \( \theta_\ast \) is the fixed point of the projected Bellman operator.

Starting with \( A \theta_\ast = b \), and substituting the definitions of \( A \) and \( b \), we have
\begin{align*}
    \implies
    \Phi^{\top} D_{\mu}\left(\mathbb{I}-\gamma P_{\mu}\right) \Phi \theta_\ast
     & =
    \Phi^{\top} D_{\mu} r_{\mu}
    \\
    \implies
    \Phi^{\top} D_{\mu} \Phi \theta_\ast - \gamma \Phi^{\top} D_{\mu} P_{\mu} \Phi \theta_\ast
     & =
    \Phi^{\top} D_{\mu} r_{\mu}
    \\
    \implies
    \Phi^{\top} D_{\mu} \Phi \theta_\ast
     & =
    \Phi^{\top} D_{\mu} r_{\mu} + \gamma \Phi^{\top} D_{\mu} P_{\mu} \Phi \theta_\ast
    \\ & =
    \Phi^{\top} D_{\mu} \left( r_{\mu} + \gamma P_{\mu} \Phi \theta_\ast \right)
    =
    \Phi^{\top} D_{\mu} T_{\mu} \Phi \theta_\ast
\end{align*}

\begin{align*}
    \Pi T_{\mu} \Phi \theta_\ast
     & =
    \Pi \left( r_{\mu} + \gamma P_{\mu} \Phi \theta_\ast \right)
    =
    \Phi{\left(\Phi^{\top} D_{\mu} \Phi\right)}^{-1} \Phi^{\top} D_{\mu} \left( r_{\mu} + \gamma P_{\mu} \Phi \theta_\ast \right)
    \\ & =
    \Phi{\left(\Phi^{\top} D_{\mu} \Phi\right)}^{-1} \Phi^{\top} D_{\mu} r_{\mu} + \gamma \Phi{\left(\Phi^{\top} D_{\mu} \Phi\right)}^{-1} \Phi^{\top} D_{\mu} P_{\mu} \Phi \theta_\ast
\end{align*}
